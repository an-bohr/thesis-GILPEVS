{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "944a23a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing segmentation masks:   0%|                     | 0/10 [00:00<?, ?it/s]Bounding box: x=(71, 216), y=(95, 198), z=(53, 101)\n",
      "Cropped volume shape: (146, 104, 49)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  10%|█▎           | 1/10 [00:00<00:03,  2.35it/s]Bounding box: x=(91, 214), y=(97, 208), z=(60, 119)\n",
      "Cropped volume shape: (124, 112, 60)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  20%|██▌          | 2/10 [00:00<00:02,  2.92it/s]Bounding box: x=(107, 284), y=(74, 244), z=(71, 139)\n",
      "Cropped volume shape: (178, 171, 69)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  30%|███▉         | 3/10 [00:01<00:02,  2.37it/s]Bounding box: x=(65, 223), y=(50, 203), z=(21, 83)\n",
      "Cropped volume shape: (159, 154, 63)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  40%|█████▏       | 4/10 [00:01<00:02,  2.55it/s]Bounding box: x=(65, 224), y=(97, 201), z=(46, 100)\n",
      "Cropped volume shape: (160, 105, 55)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  50%|██████▌      | 5/10 [00:01<00:01,  2.79it/s]Bounding box: x=(50, 191), y=(62, 175), z=(72, 152)\n",
      "Cropped volume shape: (142, 114, 81)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  60%|███████▊     | 6/10 [00:02<00:01,  2.79it/s]Bounding box: x=(61, 198), y=(99, 207), z=(65, 127)\n",
      "Cropped volume shape: (138, 109, 63)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  70%|█████████    | 7/10 [00:02<00:01,  2.76it/s]Bounding box: x=(76, 241), y=(88, 232), z=(65, 126)\n",
      "Cropped volume shape: (166, 145, 62)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  80%|██████████▍  | 8/10 [00:03<00:00,  2.66it/s]Bounding box: x=(79, 240), y=(110, 235), z=(41, 95)\n",
      "Cropped volume shape: (162, 126, 55)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  90%|███████████▋ | 9/10 [00:03<00:00,  2.75it/s]Bounding box: x=(58, 198), y=(62, 187), z=(62, 117)\n",
      "Cropped volume shape: (141, 126, 56)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks: 100%|████████████| 10/10 [00:03<00:00,  2.75it/s]\n",
      "Mapping file saved to /home/lucasp/thesis/npz_output/mapping.json\n"
     ]
    }
   ],
   "source": [
    "# Prepare_sdf_samples:\n",
    "!python geometric_model/prepare_sdf_samples.py --input_folder /home/lucasp/thesis/TopCoW/gt_train --output_folder /home/lucasp/thesis/npz_output --num_points 15000 --latent_dim 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "464a3108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing segmentation masks:   0%|                     | 0/10 [00:00<?, ?it/s]Bounding box: x=(71, 216), y=(95, 198), z=(53, 101)\n",
      "Cropped volume shape: (146, 104, 49)\n",
      "Shape: Negative voxels: 6720 (0.90%), Positive voxels: 737296 (99.10%)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  10%|█▎           | 1/10 [00:00<00:04,  2.11it/s]Bounding box: x=(91, 214), y=(97, 208), z=(60, 119)\n",
      "Cropped volume shape: (124, 112, 60)\n",
      "Shape: Negative voxels: 11319 (1.36%), Positive voxels: 821961 (98.64%)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  20%|██▌          | 2/10 [00:00<00:03,  2.65it/s]Bounding box: x=(107, 284), y=(74, 244), z=(71, 139)\n",
      "Cropped volume shape: (178, 171, 69)\n",
      "Shape: Negative voxels: 18507 (0.88%), Positive voxels: 2081715 (99.12%)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  30%|███▉         | 3/10 [00:01<00:03,  2.17it/s]Bounding box: x=(65, 223), y=(50, 203), z=(21, 83)\n",
      "Cropped volume shape: (159, 154, 63)\n",
      "Shape: Negative voxels: 13164 (0.85%), Positive voxels: 1529454 (99.15%)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  40%|█████▏       | 4/10 [00:01<00:02,  2.33it/s]Bounding box: x=(65, 224), y=(97, 201), z=(46, 100)\n",
      "Cropped volume shape: (160, 105, 55)\n",
      "Shape: Negative voxels: 13224 (1.43%), Positive voxels: 910776 (98.57%)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  50%|██████▌      | 5/10 [00:02<00:01,  2.60it/s]Bounding box: x=(50, 191), y=(62, 175), z=(72, 152)\n",
      "Cropped volume shape: (142, 114, 81)\n",
      "Shape: Negative voxels: 11408 (0.87%), Positive voxels: 1299820 (99.13%)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  60%|███████▊     | 6/10 [00:02<00:01,  2.49it/s]Bounding box: x=(61, 198), y=(99, 207), z=(65, 127)\n",
      "Cropped volume shape: (138, 109, 63)\n",
      "Shape: Negative voxels: 6796 (0.72%), Positive voxels: 940850 (99.28%)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  70%|█████████    | 7/10 [00:02<00:01,  2.69it/s]Bounding box: x=(76, 241), y=(88, 232), z=(65, 126)\n",
      "Cropped volume shape: (166, 145, 62)\n",
      "Shape: Negative voxels: 11619 (0.78%), Positive voxels: 1480721 (99.22%)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  80%|██████████▍  | 8/10 [00:03<00:00,  2.61it/s]Bounding box: x=(79, 240), y=(110, 235), z=(41, 95)\n",
      "Cropped volume shape: (162, 126, 55)\n",
      "Shape: Negative voxels: 19656 (1.75%), Positive voxels: 1103004 (98.25%)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks:  90%|███████████▋ | 9/10 [00:03<00:00,  2.69it/s]Bounding box: x=(58, 198), y=(62, 187), z=(62, 117)\n",
      "Cropped volume shape: (141, 126, 56)\n",
      "Shape: Negative voxels: 9479 (0.95%), Positive voxels: 985417 (99.05%)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Processing segmentation masks: 100%|████████████| 10/10 [00:03<00:00,  2.61it/s]\n",
      "Mapping file saved to /home/lucasp/thesis/npz_output_test/mapping.json\n",
      "Initial latent codes saved to /home/lucasp/thesis/npz_output_test/latent_codes.npz\n"
     ]
    }
   ],
   "source": [
    "# testing with init_latent\n",
    "!python geometric_model/prepare_sdf_samples.py --input_folder /home/lucasp/thesis/TopCoW/gt_train --output_folder /home/lucasp/thesis/npz_output_test --init_latents\n",
    "# NOTE: Didnt change anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87906b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def load_npz_samples(npz_file):\n",
    "    \"\"\"\n",
    "    Loads the .npz file produced by your preparation script.\n",
    "    Expects the file to contain:\n",
    "       - \"pos\": numpy array of shape [N, 3] with the voxel coordinates\n",
    "       - \"neg\": numpy array of shape [N] with the corresponding SDF values.\n",
    "    \"\"\"\n",
    "    data = np.load(npz_file)\n",
    "    return data['pos'], data['neg']\n",
    "\n",
    "def plot_full_sdf_scatter(coords, sdf_values):\n",
    "    \"\"\"\n",
    "    Creates an interactive 3D scatter plot of all sample points,\n",
    "    color-coded by their SDF value.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    sc = ax.scatter(coords[:, 0], coords[:, 1], coords[:, 2], c=sdf_values,\n",
    "                    cmap='coolwarm', marker='o', s=5, alpha=0.8)\n",
    "    plt.colorbar(sc, ax=ax, label='SDF Value')\n",
    "    ax.set_title(\"3D Scatter Plot of SDF Samples\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "    \n",
    "    plt.show()  # displays the figure interactively\n",
    "\n",
    "def plot_surface_scatter(coords, sdf_values, threshold=1.0):\n",
    "    \"\"\"\n",
    "    Creates an interactive 3D scatter plot showing only points where \n",
    "    |SDF| < threshold (i.e. points near the boundary).\n",
    "    \"\"\"\n",
    "    mask = np.abs(sdf_values) < threshold\n",
    "    surface_coords = coords[mask]\n",
    "    surface_sdf = sdf_values[mask]\n",
    "    \n",
    "    if surface_coords.size == 0:\n",
    "        print(\"No points found near the surface with the given threshold.\")\n",
    "        return\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    sc = ax.scatter(surface_coords[:, 0], surface_coords[:, 1], surface_coords[:, 2],\n",
    "                    c=surface_sdf, cmap='coolwarm', marker='o', s=10)\n",
    "    plt.colorbar(sc, ax=ax, label=f'SDF Value (|SDF| < {threshold})')\n",
    "    ax.set_title(f\"3D Scatter Plot of Surface Samples (|SDF| < {threshold})\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def check_near_surface_points(coords, sdf_values, lower_bound=-0.1, upper_bound=0.1):\n",
    "    \"\"\"\n",
    "    Checks for points with SDF values in the range [lower_bound, upper_bound].\n",
    "    If found, it prints how many such points exist and displays one example point.\n",
    "    \"\"\"\n",
    "    # Create a mask for SDF values in the specified interval.\n",
    "    mask = (sdf_values >= lower_bound) & (sdf_values <= upper_bound)\n",
    "    count = np.sum(mask)\n",
    "    print(f\"Number of points with SDF in the range [{lower_bound}, {upper_bound}]: {count}\")\n",
    "    if count > 0:\n",
    "        idx = np.where(mask)[0][0]\n",
    "        print(f\"Example point:\\n  Coordinates: {coords[idx]}\\n  SDF Value: {sdf_values[idx]}\")\n",
    "    else:\n",
    "        print(\"No points found in the specified SDF range.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    npz_path = \"/home/lucasp/thesis/npz_output/topcow_ct_001.npz\"\n",
    "    \n",
    "    coords, sdf_values = load_npz_samples(npz_path)\n",
    "    \n",
    "    plot_full_sdf_scatter(coords, sdf_values)\n",
    "    plot_surface_scatter(coords, sdf_values, threshold=0.105)\n",
    "    \n",
    "    # check_near_surface_points(coords, sdf_values, lower_bound=-.11, upper_bound=.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9133f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 shapes in dataset.\n",
      "/home/lucasp/miniconda3/envs/nnunet/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "[Epoch 1/2500] Loss: 0.411413 | Time: 0.39s | Mean latent norm: 0.999365\n",
      "New best model saved at epoch 1 with loss 0.411413\n",
      "[Epoch 2/2500] Loss: 0.386310 | Time: 0.19s | Mean latent norm: 0.999404\n",
      "New best model saved at epoch 2 with loss 0.386310\n",
      "[Epoch 3/2500] Loss: 0.386970 | Time: 0.19s | Mean latent norm: 0.999337\n",
      "[Epoch 4/2500] Loss: 0.386068 | Time: 0.19s | Mean latent norm: 0.999229\n",
      "New best model saved at epoch 4 with loss 0.386068\n",
      "[Epoch 5/2500] Loss: 0.384780 | Time: 0.19s | Mean latent norm: 0.999122\n",
      "New best model saved at epoch 5 with loss 0.384780\n",
      "[Epoch 6/2500] Loss: 0.383934 | Time: 0.19s | Mean latent norm: 0.999040\n",
      "New best model saved at epoch 6 with loss 0.383934\n",
      "[Epoch 7/2500] Loss: 0.382438 | Time: 0.19s | Mean latent norm: 0.998900\n",
      "New best model saved at epoch 7 with loss 0.382438\n",
      "[Epoch 8/2500] Loss: 0.378116 | Time: 0.19s | Mean latent norm: 0.998626\n",
      "New best model saved at epoch 8 with loss 0.378116\n",
      "[Epoch 9/2500] Loss: 0.360755 | Time: 0.19s | Mean latent norm: 0.998245\n",
      "New best model saved at epoch 9 with loss 0.360755\n",
      "[Epoch 10/2500] Loss: 0.321071 | Time: 0.19s | Mean latent norm: 0.997974\n",
      "New best model saved at epoch 10 with loss 0.321071\n",
      "[Epoch 11/2500] Loss: 0.305378 | Time: 0.19s | Mean latent norm: 0.997829\n",
      "New best model saved at epoch 11 with loss 0.305378\n",
      "[Epoch 12/2500] Loss: 0.285960 | Time: 0.19s | Mean latent norm: 0.997778\n",
      "New best model saved at epoch 12 with loss 0.285960\n",
      "[Epoch 13/2500] Loss: 0.279469 | Time: 0.19s | Mean latent norm: 0.997701\n",
      "New best model saved at epoch 13 with loss 0.279469\n",
      "[Epoch 14/2500] Loss: 0.274722 | Time: 0.19s | Mean latent norm: 0.997682\n",
      "New best model saved at epoch 14 with loss 0.274722\n",
      "[Epoch 15/2500] Loss: 0.274400 | Time: 0.19s | Mean latent norm: 0.997632\n",
      "New best model saved at epoch 15 with loss 0.274400\n",
      "[Epoch 16/2500] Loss: 0.269336 | Time: 0.19s | Mean latent norm: 0.997564\n",
      "New best model saved at epoch 16 with loss 0.269336\n",
      "[Epoch 17/2500] Loss: 0.262677 | Time: 0.19s | Mean latent norm: 0.997522\n",
      "New best model saved at epoch 17 with loss 0.262677\n",
      "[Epoch 18/2500] Loss: 0.260263 | Time: 0.19s | Mean latent norm: 0.997511\n",
      "New best model saved at epoch 18 with loss 0.260263\n",
      "[Epoch 19/2500] Loss: 0.256078 | Time: 0.19s | Mean latent norm: 0.997507\n",
      "New best model saved at epoch 19 with loss 0.256078\n",
      "[Epoch 20/2500] Loss: 0.253927 | Time: 0.19s | Mean latent norm: 0.997506\n",
      "New best model saved at epoch 20 with loss 0.253927\n",
      "[Epoch 21/2500] Loss: 0.252969 | Time: 0.19s | Mean latent norm: 0.997528\n",
      "New best model saved at epoch 21 with loss 0.252969\n",
      "[Epoch 22/2500] Loss: 0.252288 | Time: 0.19s | Mean latent norm: 0.997506\n",
      "New best model saved at epoch 22 with loss 0.252288\n",
      "[Epoch 23/2500] Loss: 0.255302 | Time: 0.20s | Mean latent norm: 0.997502\n",
      "[Epoch 24/2500] Loss: 0.247508 | Time: 0.20s | Mean latent norm: 0.997477\n",
      "New best model saved at epoch 24 with loss 0.247508\n",
      "[Epoch 25/2500] Loss: 0.247120 | Time: 0.19s | Mean latent norm: 0.997458\n",
      "New best model saved at epoch 25 with loss 0.247120\n",
      "[Epoch 26/2500] Loss: 0.244391 | Time: 0.19s | Mean latent norm: 0.997461\n",
      "New best model saved at epoch 26 with loss 0.244391\n",
      "[Epoch 27/2500] Loss: 0.246024 | Time: 0.19s | Mean latent norm: 0.997430\n",
      "[Epoch 28/2500] Loss: 0.242650 | Time: 0.19s | Mean latent norm: 0.997421\n",
      "New best model saved at epoch 28 with loss 0.242650\n",
      "[Epoch 29/2500] Loss: 0.242260 | Time: 0.19s | Mean latent norm: 0.997422\n",
      "New best model saved at epoch 29 with loss 0.242260\n",
      "[Epoch 30/2500] Loss: 0.238309 | Time: 0.20s | Mean latent norm: 0.997397\n",
      "New best model saved at epoch 30 with loss 0.238309\n",
      "[Epoch 31/2500] Loss: 0.239553 | Time: 0.19s | Mean latent norm: 0.997387\n",
      "[Epoch 32/2500] Loss: 0.233440 | Time: 0.19s | Mean latent norm: 0.997377\n",
      "New best model saved at epoch 32 with loss 0.233440\n",
      "[Epoch 33/2500] Loss: 0.234274 | Time: 0.20s | Mean latent norm: 0.997364\n",
      "[Epoch 34/2500] Loss: 0.233514 | Time: 0.20s | Mean latent norm: 0.997385\n",
      "[Epoch 35/2500] Loss: 0.242445 | Time: 0.19s | Mean latent norm: 0.997367\n",
      "[Epoch 36/2500] Loss: 0.237191 | Time: 0.20s | Mean latent norm: 0.997327\n",
      "[Epoch 37/2500] Loss: 0.240015 | Time: 0.19s | Mean latent norm: 0.997286\n",
      "[Epoch 38/2500] Loss: 0.230319 | Time: 0.20s | Mean latent norm: 0.997248\n",
      "New best model saved at epoch 38 with loss 0.230319\n",
      "[Epoch 39/2500] Loss: 0.227721 | Time: 0.20s | Mean latent norm: 0.997234\n",
      "New best model saved at epoch 39 with loss 0.227721\n",
      "[Epoch 40/2500] Loss: 0.226614 | Time: 0.20s | Mean latent norm: 0.997228\n",
      "New best model saved at epoch 40 with loss 0.226614\n",
      "[Epoch 41/2500] Loss: 0.229384 | Time: 0.19s | Mean latent norm: 0.997241\n",
      "[Epoch 42/2500] Loss: 0.226372 | Time: 0.19s | Mean latent norm: 0.997237\n",
      "New best model saved at epoch 42 with loss 0.226372\n",
      "[Epoch 43/2500] Loss: 0.226266 | Time: 0.20s | Mean latent norm: 0.997244\n",
      "New best model saved at epoch 43 with loss 0.226266\n",
      "[Epoch 44/2500] Loss: 0.223984 | Time: 0.19s | Mean latent norm: 0.997229\n",
      "New best model saved at epoch 44 with loss 0.223984\n",
      "[Epoch 45/2500] Loss: 0.221314 | Time: 0.19s | Mean latent norm: 0.997222\n",
      "New best model saved at epoch 45 with loss 0.221314\n",
      "[Epoch 46/2500] Loss: 0.222105 | Time: 0.17s | Mean latent norm: 0.997244\n",
      "[Epoch 47/2500] Loss: 0.223798 | Time: 0.17s | Mean latent norm: 0.997210\n",
      "[Epoch 48/2500] Loss: 0.221112 | Time: 0.17s | Mean latent norm: 0.997194\n",
      "New best model saved at epoch 48 with loss 0.221112\n",
      "[Epoch 49/2500] Loss: 0.226730 | Time: 0.17s | Mean latent norm: 0.997185\n",
      "[Epoch 50/2500] Loss: 0.224191 | Time: 0.18s | Mean latent norm: 0.997145\n",
      "[Epoch 51/2500] Loss: 0.232020 | Time: 0.17s | Mean latent norm: 0.997131\n",
      "[Epoch 52/2500] Loss: 0.223142 | Time: 0.17s | Mean latent norm: 0.997066\n",
      "[Epoch 53/2500] Loss: 0.224657 | Time: 0.17s | Mean latent norm: 0.997060\n",
      "[Epoch 54/2500] Loss: 0.226190 | Time: 0.17s | Mean latent norm: 0.996984\n",
      "[Epoch 55/2500] Loss: 0.222450 | Time: 0.17s | Mean latent norm: 0.996976\n",
      "[Epoch 56/2500] Loss: 0.226562 | Time: 0.17s | Mean latent norm: 0.996924\n",
      "[Epoch 57/2500] Loss: 0.220788 | Time: 0.17s | Mean latent norm: 0.996923\n",
      "New best model saved at epoch 57 with loss 0.220788\n",
      "[Epoch 58/2500] Loss: 0.226339 | Time: 0.18s | Mean latent norm: 0.996858\n",
      "[Epoch 59/2500] Loss: 0.219746 | Time: 0.17s | Mean latent norm: 0.996837\n",
      "New best model saved at epoch 59 with loss 0.219746\n",
      "[Epoch 60/2500] Loss: 0.215782 | Time: 0.17s | Mean latent norm: 0.996809\n",
      "New best model saved at epoch 60 with loss 0.215782\n",
      "[Epoch 61/2500] Loss: 0.215706 | Time: 0.17s | Mean latent norm: 0.996818\n",
      "New best model saved at epoch 61 with loss 0.215706\n",
      "[Epoch 62/2500] Loss: 0.213054 | Time: 0.17s | Mean latent norm: 0.996779\n",
      "New best model saved at epoch 62 with loss 0.213054\n",
      "[Epoch 63/2500] Loss: 0.210693 | Time: 0.18s | Mean latent norm: 0.996790\n",
      "New best model saved at epoch 63 with loss 0.210693\n",
      "[Epoch 64/2500] Loss: 0.211590 | Time: 0.17s | Mean latent norm: 0.996781\n",
      "[Epoch 65/2500] Loss: 0.211560 | Time: 0.18s | Mean latent norm: 0.996815\n",
      "[Epoch 66/2500] Loss: 0.214614 | Time: 0.18s | Mean latent norm: 0.996761\n",
      "[Epoch 67/2500] Loss: 0.210714 | Time: 0.17s | Mean latent norm: 0.996769\n",
      "[Epoch 68/2500] Loss: 0.213240 | Time: 0.17s | Mean latent norm: 0.996719\n",
      "[Epoch 69/2500] Loss: 0.212924 | Time: 0.17s | Mean latent norm: 0.996711\n",
      "[Epoch 70/2500] Loss: 0.207174 | Time: 0.17s | Mean latent norm: 0.996683\n",
      "New best model saved at epoch 70 with loss 0.207174\n",
      "[Epoch 71/2500] Loss: 0.208034 | Time: 0.18s | Mean latent norm: 0.996666\n",
      "[Epoch 72/2500] Loss: 0.204099 | Time: 0.17s | Mean latent norm: 0.996669\n",
      "New best model saved at epoch 72 with loss 0.204099\n",
      "[Epoch 73/2500] Loss: 0.201250 | Time: 0.17s | Mean latent norm: 0.996667\n",
      "New best model saved at epoch 73 with loss 0.201250\n",
      "[Epoch 74/2500] Loss: 0.199199 | Time: 0.17s | Mean latent norm: 0.996681\n",
      "New best model saved at epoch 74 with loss 0.199199\n",
      "[Epoch 75/2500] Loss: 0.198748 | Time: 0.17s | Mean latent norm: 0.996686\n",
      "New best model saved at epoch 75 with loss 0.198748\n",
      "[Epoch 76/2500] Loss: 0.197331 | Time: 0.17s | Mean latent norm: 0.996691\n",
      "New best model saved at epoch 76 with loss 0.197331\n",
      "[Epoch 77/2500] Loss: 0.197188 | Time: 0.18s | Mean latent norm: 0.996704\n",
      "New best model saved at epoch 77 with loss 0.197188\n",
      "[Epoch 78/2500] Loss: 0.197299 | Time: 0.18s | Mean latent norm: 0.996685\n",
      "[Epoch 79/2500] Loss: 0.199434 | Time: 0.17s | Mean latent norm: 0.996681\n",
      "[Epoch 80/2500] Loss: 0.196313 | Time: 0.17s | Mean latent norm: 0.996663\n",
      "New best model saved at epoch 80 with loss 0.196313\n",
      "[Epoch 81/2500] Loss: 0.197996 | Time: 0.17s | Mean latent norm: 0.996647\n",
      "[Epoch 82/2500] Loss: 0.195636 | Time: 0.17s | Mean latent norm: 0.996627\n",
      "New best model saved at epoch 82 with loss 0.195636\n",
      "[Epoch 83/2500] Loss: 0.194156 | Time: 0.17s | Mean latent norm: 0.996619\n",
      "New best model saved at epoch 83 with loss 0.194156\n",
      "[Epoch 84/2500] Loss: 0.191966 | Time: 0.17s | Mean latent norm: 0.996611\n",
      "New best model saved at epoch 84 with loss 0.191966\n",
      "[Epoch 85/2500] Loss: 0.190760 | Time: 0.17s | Mean latent norm: 0.996614\n",
      "New best model saved at epoch 85 with loss 0.190760\n",
      "[Epoch 86/2500] Loss: 0.191745 | Time: 0.17s | Mean latent norm: 0.996589\n",
      "[Epoch 87/2500] Loss: 0.188411 | Time: 0.17s | Mean latent norm: 0.996575\n",
      "New best model saved at epoch 87 with loss 0.188411\n",
      "[Epoch 88/2500] Loss: 0.187856 | Time: 0.17s | Mean latent norm: 0.996554\n",
      "New best model saved at epoch 88 with loss 0.187856\n",
      "[Epoch 89/2500] Loss: 0.186291 | Time: 0.18s | Mean latent norm: 0.996536\n",
      "New best model saved at epoch 89 with loss 0.186291\n",
      "[Epoch 90/2500] Loss: 0.184530 | Time: 0.17s | Mean latent norm: 0.996521\n",
      "New best model saved at epoch 90 with loss 0.184530\n",
      "[Epoch 91/2500] Loss: 0.183697 | Time: 0.17s | Mean latent norm: 0.996516\n",
      "New best model saved at epoch 91 with loss 0.183697\n",
      "[Epoch 92/2500] Loss: 0.184550 | Time: 0.17s | Mean latent norm: 0.996515\n",
      "[Epoch 93/2500] Loss: 0.188017 | Time: 0.17s | Mean latent norm: 0.996487\n",
      "[Epoch 94/2500] Loss: 0.191203 | Time: 0.17s | Mean latent norm: 0.996442\n",
      "[Epoch 95/2500] Loss: 0.189951 | Time: 0.18s | Mean latent norm: 0.996389\n",
      "[Epoch 96/2500] Loss: 0.188517 | Time: 0.19s | Mean latent norm: 0.996336\n",
      "[Epoch 97/2500] Loss: 0.194434 | Time: 0.21s | Mean latent norm: 0.996283\n",
      "[Epoch 98/2500] Loss: 0.189124 | Time: 0.19s | Mean latent norm: 0.996231\n",
      "[Epoch 99/2500] Loss: 0.186382 | Time: 0.18s | Mean latent norm: 0.996173\n",
      "[Epoch 100/2500] Loss: 0.186532 | Time: 0.19s | Mean latent norm: 0.996134\n",
      "[Epoch 101/2500] Loss: 0.190954 | Time: 0.19s | Mean latent norm: 0.996100\n",
      "[Epoch 102/2500] Loss: 0.189388 | Time: 0.19s | Mean latent norm: 0.996046\n",
      "[Epoch 103/2500] Loss: 0.196136 | Time: 0.20s | Mean latent norm: 0.996043\n",
      "[Epoch 104/2500] Loss: 0.219010 | Time: 0.19s | Mean latent norm: 0.995926\n",
      "[Epoch 105/2500] Loss: 0.229270 | Time: 0.19s | Mean latent norm: 0.995857\n",
      "[Epoch 106/2500] Loss: 0.196991 | Time: 0.19s | Mean latent norm: 0.995778\n",
      "[Epoch 107/2500] Loss: 0.190693 | Time: 0.19s | Mean latent norm: 0.995722\n",
      "[Epoch 108/2500] Loss: 0.183079 | Time: 0.19s | Mean latent norm: 0.995707\n",
      "New best model saved at epoch 108 with loss 0.183079\n",
      "[Epoch 109/2500] Loss: 0.178454 | Time: 0.19s | Mean latent norm: 0.995707\n",
      "New best model saved at epoch 109 with loss 0.178454\n",
      "[Epoch 110/2500] Loss: 0.175718 | Time: 0.19s | Mean latent norm: 0.995713\n",
      "New best model saved at epoch 110 with loss 0.175718\n",
      "[Epoch 111/2500] Loss: 0.172145 | Time: 0.19s | Mean latent norm: 0.995722\n",
      "New best model saved at epoch 111 with loss 0.172145\n",
      "[Epoch 112/2500] Loss: 0.170712 | Time: 0.19s | Mean latent norm: 0.995729\n",
      "New best model saved at epoch 112 with loss 0.170712\n",
      "[Epoch 113/2500] Loss: 0.170845 | Time: 0.19s | Mean latent norm: 0.995725\n",
      "[Epoch 114/2500] Loss: 0.168951 | Time: 0.19s | Mean latent norm: 0.995725\n",
      "New best model saved at epoch 114 with loss 0.168951\n",
      "[Epoch 115/2500] Loss: 0.166976 | Time: 0.19s | Mean latent norm: 0.995731\n",
      "New best model saved at epoch 115 with loss 0.166976\n",
      "[Epoch 116/2500] Loss: 0.166774 | Time: 0.19s | Mean latent norm: 0.995727\n",
      "New best model saved at epoch 116 with loss 0.166774\n",
      "[Epoch 117/2500] Loss: 0.166217 | Time: 0.19s | Mean latent norm: 0.995728\n",
      "New best model saved at epoch 117 with loss 0.166217\n",
      "[Epoch 118/2500] Loss: 0.166231 | Time: 0.19s | Mean latent norm: 0.995730\n",
      "[Epoch 119/2500] Loss: 0.168011 | Time: 0.19s | Mean latent norm: 0.995715\n",
      "[Epoch 120/2500] Loss: 0.171053 | Time: 0.19s | Mean latent norm: 0.995688\n",
      "[Epoch 121/2500] Loss: 0.173469 | Time: 0.19s | Mean latent norm: 0.995680\n",
      "[Epoch 122/2500] Loss: 0.179442 | Time: 0.19s | Mean latent norm: 0.995613\n",
      "[Epoch 123/2500] Loss: 0.177287 | Time: 0.19s | Mean latent norm: 0.995570\n",
      "[Epoch 124/2500] Loss: 0.178759 | Time: 0.19s | Mean latent norm: 0.995500\n",
      "[Epoch 125/2500] Loss: 0.175678 | Time: 0.19s | Mean latent norm: 0.995452\n",
      "[Epoch 126/2500] Loss: 0.181291 | Time: 0.19s | Mean latent norm: 0.995381\n",
      "[Epoch 127/2500] Loss: 0.187203 | Time: 0.19s | Mean latent norm: 0.995317\n",
      "[Epoch 128/2500] Loss: 0.175741 | Time: 0.19s | Mean latent norm: 0.995259\n",
      "[Epoch 129/2500] Loss: 0.172100 | Time: 0.19s | Mean latent norm: 0.995230\n",
      "[Epoch 130/2500] Loss: 0.165993 | Time: 0.20s | Mean latent norm: 0.995214\n",
      "New best model saved at epoch 130 with loss 0.165993\n",
      "[Epoch 131/2500] Loss: 0.163707 | Time: 0.20s | Mean latent norm: 0.995204\n",
      "New best model saved at epoch 131 with loss 0.163707\n",
      "[Epoch 132/2500] Loss: 0.159528 | Time: 0.19s | Mean latent norm: 0.995215\n",
      "New best model saved at epoch 132 with loss 0.159528\n",
      "[Epoch 133/2500] Loss: 0.160188 | Time: 0.19s | Mean latent norm: 0.995213\n",
      "[Epoch 134/2500] Loss: 0.156846 | Time: 0.19s | Mean latent norm: 0.995213\n",
      "New best model saved at epoch 134 with loss 0.156846\n",
      "[Epoch 135/2500] Loss: 0.155898 | Time: 0.19s | Mean latent norm: 0.995219\n",
      "New best model saved at epoch 135 with loss 0.155898\n",
      "[Epoch 136/2500] Loss: 0.154825 | Time: 0.19s | Mean latent norm: 0.995226\n",
      "New best model saved at epoch 136 with loss 0.154825\n",
      "[Epoch 137/2500] Loss: 0.154711 | Time: 0.19s | Mean latent norm: 0.995231\n",
      "New best model saved at epoch 137 with loss 0.154711\n",
      "[Epoch 138/2500] Loss: 0.153884 | Time: 0.19s | Mean latent norm: 0.995235\n",
      "New best model saved at epoch 138 with loss 0.153884\n",
      "[Epoch 139/2500] Loss: 0.153438 | Time: 0.19s | Mean latent norm: 0.995233\n",
      "New best model saved at epoch 139 with loss 0.153438\n",
      "[Epoch 140/2500] Loss: 0.152183 | Time: 0.19s | Mean latent norm: 0.995237\n",
      "New best model saved at epoch 140 with loss 0.152183\n",
      "[Epoch 141/2500] Loss: 0.151709 | Time: 0.19s | Mean latent norm: 0.995240\n",
      "New best model saved at epoch 141 with loss 0.151709\n",
      "[Epoch 142/2500] Loss: 0.153837 | Time: 0.19s | Mean latent norm: 0.995238\n",
      "[Epoch 143/2500] Loss: 0.155533 | Time: 0.19s | Mean latent norm: 0.995221\n",
      "[Epoch 144/2500] Loss: 0.158947 | Time: 0.19s | Mean latent norm: 0.995218\n",
      "[Epoch 145/2500] Loss: 0.161811 | Time: 0.19s | Mean latent norm: 0.995182\n",
      "[Epoch 146/2500] Loss: 0.160991 | Time: 0.19s | Mean latent norm: 0.995145\n",
      "[Epoch 147/2500] Loss: 0.155608 | Time: 0.19s | Mean latent norm: 0.995129\n",
      "[Epoch 148/2500] Loss: 0.152163 | Time: 0.19s | Mean latent norm: 0.995104\n",
      "[Epoch 149/2500] Loss: 0.162658 | Time: 0.19s | Mean latent norm: 0.995099\n",
      "[Epoch 150/2500] Loss: 0.160939 | Time: 0.19s | Mean latent norm: 0.995081\n",
      "[Epoch 151/2500] Loss: 0.168692 | Time: 0.19s | Mean latent norm: 0.995033\n",
      "[Epoch 152/2500] Loss: 0.174920 | Time: 0.19s | Mean latent norm: 0.994986\n",
      "[Epoch 153/2500] Loss: 0.175373 | Time: 0.19s | Mean latent norm: 0.994885\n",
      "[Epoch 154/2500] Loss: 0.164163 | Time: 0.19s | Mean latent norm: 0.994862\n",
      "[Epoch 155/2500] Loss: 0.158937 | Time: 0.19s | Mean latent norm: 0.994842\n",
      "[Epoch 156/2500] Loss: 0.151125 | Time: 0.19s | Mean latent norm: 0.994836\n",
      "New best model saved at epoch 156 with loss 0.151125\n",
      "[Epoch 157/2500] Loss: 0.150295 | Time: 0.19s | Mean latent norm: 0.994821\n",
      "New best model saved at epoch 157 with loss 0.150295\n",
      "[Epoch 158/2500] Loss: 0.155305 | Time: 0.19s | Mean latent norm: 0.994821\n",
      "[Epoch 159/2500] Loss: 0.154380 | Time: 0.19s | Mean latent norm: 0.994795\n",
      "[Epoch 160/2500] Loss: 0.157352 | Time: 0.18s | Mean latent norm: 0.994786\n",
      "[Epoch 161/2500] Loss: 0.153626 | Time: 0.18s | Mean latent norm: 0.994754\n",
      "[Epoch 162/2500] Loss: 0.150641 | Time: 0.18s | Mean latent norm: 0.994736\n",
      "[Epoch 163/2500] Loss: 0.149390 | Time: 0.18s | Mean latent norm: 0.994727\n",
      "New best model saved at epoch 163 with loss 0.149390\n",
      "[Epoch 164/2500] Loss: 0.147580 | Time: 0.18s | Mean latent norm: 0.994704\n",
      "New best model saved at epoch 164 with loss 0.147580\n",
      "[Epoch 165/2500] Loss: 0.147140 | Time: 0.18s | Mean latent norm: 0.994699\n",
      "New best model saved at epoch 165 with loss 0.147140\n",
      "[Epoch 166/2500] Loss: 0.145738 | Time: 0.18s | Mean latent norm: 0.994699\n",
      "New best model saved at epoch 166 with loss 0.145738\n",
      "[Epoch 167/2500] Loss: 0.143564 | Time: 0.18s | Mean latent norm: 0.994698\n",
      "New best model saved at epoch 167 with loss 0.143564\n",
      "[Epoch 168/2500] Loss: 0.142985 | Time: 0.18s | Mean latent norm: 0.994703\n",
      "New best model saved at epoch 168 with loss 0.142985\n",
      "[Epoch 169/2500] Loss: 0.143576 | Time: 0.18s | Mean latent norm: 0.994697\n",
      "[Epoch 170/2500] Loss: 0.142419 | Time: 0.19s | Mean latent norm: 0.994701\n",
      "New best model saved at epoch 170 with loss 0.142419\n",
      "[Epoch 171/2500] Loss: 0.143329 | Time: 0.19s | Mean latent norm: 0.994701\n",
      "[Epoch 172/2500] Loss: 0.144767 | Time: 0.19s | Mean latent norm: 0.994698\n",
      "[Epoch 173/2500] Loss: 0.143790 | Time: 0.19s | Mean latent norm: 0.994693\n",
      "[Epoch 174/2500] Loss: 0.143803 | Time: 0.19s | Mean latent norm: 0.994685\n",
      "[Epoch 175/2500] Loss: 0.144952 | Time: 0.19s | Mean latent norm: 0.994669\n",
      "[Epoch 176/2500] Loss: 0.149424 | Time: 0.19s | Mean latent norm: 0.994656\n",
      "[Epoch 177/2500] Loss: 0.147070 | Time: 0.19s | Mean latent norm: 0.994626\n",
      "[Epoch 178/2500] Loss: 0.147512 | Time: 0.19s | Mean latent norm: 0.994599\n",
      "[Epoch 179/2500] Loss: 0.142326 | Time: 0.19s | Mean latent norm: 0.994579\n",
      "New best model saved at epoch 179 with loss 0.142326\n",
      "[Epoch 180/2500] Loss: 0.140436 | Time: 0.19s | Mean latent norm: 0.994575\n",
      "New best model saved at epoch 180 with loss 0.140436\n",
      "[Epoch 181/2500] Loss: 0.139348 | Time: 0.19s | Mean latent norm: 0.994570\n",
      "New best model saved at epoch 181 with loss 0.139348\n",
      "[Epoch 182/2500] Loss: 0.138943 | Time: 0.19s | Mean latent norm: 0.994573\n",
      "New best model saved at epoch 182 with loss 0.138943\n",
      "[Epoch 183/2500] Loss: 0.140078 | Time: 0.19s | Mean latent norm: 0.994561\n",
      "[Epoch 184/2500] Loss: 0.140265 | Time: 0.19s | Mean latent norm: 0.994559\n",
      "[Epoch 185/2500] Loss: 0.143498 | Time: 0.19s | Mean latent norm: 0.994543\n",
      "[Epoch 186/2500] Loss: 0.146407 | Time: 0.19s | Mean latent norm: 0.994531\n",
      "[Epoch 187/2500] Loss: 0.141526 | Time: 0.20s | Mean latent norm: 0.994522\n",
      "[Epoch 188/2500] Loss: 0.142064 | Time: 0.19s | Mean latent norm: 0.994495\n",
      "[Epoch 189/2500] Loss: 0.138983 | Time: 0.19s | Mean latent norm: 0.994478\n",
      "[Epoch 190/2500] Loss: 0.137919 | Time: 0.19s | Mean latent norm: 0.994472\n",
      "New best model saved at epoch 190 with loss 0.137919\n",
      "[Epoch 191/2500] Loss: 0.135910 | Time: 0.19s | Mean latent norm: 0.994471\n",
      "New best model saved at epoch 191 with loss 0.135910\n",
      "[Epoch 192/2500] Loss: 0.135779 | Time: 0.18s | Mean latent norm: 0.994470\n",
      "New best model saved at epoch 192 with loss 0.135779\n",
      "[Epoch 193/2500] Loss: 0.136842 | Time: 0.18s | Mean latent norm: 0.994463\n",
      "[Epoch 194/2500] Loss: 0.136669 | Time: 0.18s | Mean latent norm: 0.994455\n",
      "[Epoch 195/2500] Loss: 0.135948 | Time: 0.18s | Mean latent norm: 0.994449\n",
      "[Epoch 196/2500] Loss: 0.136575 | Time: 0.18s | Mean latent norm: 0.994434\n",
      "[Epoch 197/2500] Loss: 0.136628 | Time: 0.18s | Mean latent norm: 0.994424\n",
      "[Epoch 198/2500] Loss: 0.142247 | Time: 0.18s | Mean latent norm: 0.994392\n",
      "[Epoch 199/2500] Loss: 0.138399 | Time: 0.18s | Mean latent norm: 0.994358\n",
      "[Epoch 200/2500] Loss: 0.139796 | Time: 0.18s | Mean latent norm: 0.994343\n",
      "[Epoch 201/2500] Loss: 0.137246 | Time: 0.19s | Mean latent norm: 0.994320\n",
      "[Epoch 202/2500] Loss: 0.134069 | Time: 0.19s | Mean latent norm: 0.994318\n",
      "New best model saved at epoch 202 with loss 0.134069\n",
      "[Epoch 203/2500] Loss: 0.140092 | Time: 0.20s | Mean latent norm: 0.994285\n",
      "[Epoch 204/2500] Loss: 0.137199 | Time: 0.19s | Mean latent norm: 0.994257\n",
      "[Epoch 205/2500] Loss: 0.138190 | Time: 0.19s | Mean latent norm: 0.994242\n",
      "[Epoch 206/2500] Loss: 0.135612 | Time: 0.19s | Mean latent norm: 0.994229\n",
      "[Epoch 207/2500] Loss: 0.134489 | Time: 0.19s | Mean latent norm: 0.994217\n",
      "[Epoch 208/2500] Loss: 0.132339 | Time: 0.19s | Mean latent norm: 0.994215\n",
      "New best model saved at epoch 208 with loss 0.132339\n",
      "[Epoch 209/2500] Loss: 0.131036 | Time: 0.19s | Mean latent norm: 0.994206\n",
      "New best model saved at epoch 209 with loss 0.131036\n",
      "[Epoch 210/2500] Loss: 0.133968 | Time: 0.19s | Mean latent norm: 0.994206\n",
      "[Epoch 211/2500] Loss: 0.132507 | Time: 0.19s | Mean latent norm: 0.994209\n",
      "[Epoch 212/2500] Loss: 0.131713 | Time: 0.19s | Mean latent norm: 0.994200\n",
      "[Epoch 213/2500] Loss: 0.131098 | Time: 0.19s | Mean latent norm: 0.994190\n",
      "[Epoch 214/2500] Loss: 0.131558 | Time: 0.19s | Mean latent norm: 0.994188\n",
      "[Epoch 215/2500] Loss: 0.137595 | Time: 0.19s | Mean latent norm: 0.994168\n",
      "[Epoch 216/2500] Loss: 0.139216 | Time: 0.19s | Mean latent norm: 0.994141\n",
      "[Epoch 217/2500] Loss: 0.135817 | Time: 0.18s | Mean latent norm: 0.994124\n",
      "[Epoch 218/2500] Loss: 0.132297 | Time: 0.19s | Mean latent norm: 0.994113\n",
      "[Epoch 219/2500] Loss: 0.128597 | Time: 0.18s | Mean latent norm: 0.994099\n",
      "New best model saved at epoch 219 with loss 0.128597\n",
      "[Epoch 220/2500] Loss: 0.129753 | Time: 0.18s | Mean latent norm: 0.994089\n",
      "[Epoch 221/2500] Loss: 0.130500 | Time: 0.18s | Mean latent norm: 0.994085\n",
      "[Epoch 222/2500] Loss: 0.131499 | Time: 0.18s | Mean latent norm: 0.994074\n",
      "[Epoch 223/2500] Loss: 0.132699 | Time: 0.18s | Mean latent norm: 0.994054\n",
      "[Epoch 224/2500] Loss: 0.130125 | Time: 0.18s | Mean latent norm: 0.994037\n",
      "[Epoch 225/2500] Loss: 0.131269 | Time: 0.18s | Mean latent norm: 0.994026\n",
      "[Epoch 226/2500] Loss: 0.131976 | Time: 0.18s | Mean latent norm: 0.993999\n",
      "[Epoch 227/2500] Loss: 0.133908 | Time: 0.18s | Mean latent norm: 0.993989\n",
      "[Epoch 228/2500] Loss: 0.138443 | Time: 0.18s | Mean latent norm: 0.993979\n",
      "[Epoch 229/2500] Loss: 0.136603 | Time: 0.19s | Mean latent norm: 0.993924\n",
      "[Epoch 230/2500] Loss: 0.133817 | Time: 0.19s | Mean latent norm: 0.993899\n",
      "[Epoch 231/2500] Loss: 0.133039 | Time: 0.19s | Mean latent norm: 0.993874\n",
      "[Epoch 232/2500] Loss: 0.127736 | Time: 0.19s | Mean latent norm: 0.993866\n",
      "New best model saved at epoch 232 with loss 0.127736\n",
      "[Epoch 233/2500] Loss: 0.126519 | Time: 0.19s | Mean latent norm: 0.993860\n",
      "New best model saved at epoch 233 with loss 0.126519\n",
      "[Epoch 234/2500] Loss: 0.125205 | Time: 0.19s | Mean latent norm: 0.993862\n",
      "New best model saved at epoch 234 with loss 0.125205\n",
      "[Epoch 235/2500] Loss: 0.125003 | Time: 0.19s | Mean latent norm: 0.993867\n",
      "New best model saved at epoch 235 with loss 0.125003\n",
      "[Epoch 236/2500] Loss: 0.124433 | Time: 0.19s | Mean latent norm: 0.993866\n",
      "New best model saved at epoch 236 with loss 0.124433\n",
      "[Epoch 237/2500] Loss: 0.124856 | Time: 0.19s | Mean latent norm: 0.993856\n",
      "[Epoch 238/2500] Loss: 0.122560 | Time: 0.19s | Mean latent norm: 0.993845\n",
      "New best model saved at epoch 238 with loss 0.122560\n",
      "[Epoch 239/2500] Loss: 0.124265 | Time: 0.19s | Mean latent norm: 0.993852\n",
      "[Epoch 240/2500] Loss: 0.122814 | Time: 0.19s | Mean latent norm: 0.993848\n",
      "[Epoch 241/2500] Loss: 0.122721 | Time: 0.19s | Mean latent norm: 0.993845\n",
      "[Epoch 242/2500] Loss: 0.123965 | Time: 0.19s | Mean latent norm: 0.993835\n",
      "[Epoch 243/2500] Loss: 0.124094 | Time: 0.19s | Mean latent norm: 0.993823\n",
      "[Epoch 244/2500] Loss: 0.125700 | Time: 0.19s | Mean latent norm: 0.993821\n",
      "[Epoch 245/2500] Loss: 0.124445 | Time: 0.19s | Mean latent norm: 0.993804\n",
      "[Epoch 246/2500] Loss: 0.122553 | Time: 0.19s | Mean latent norm: 0.993790\n",
      "New best model saved at epoch 246 with loss 0.122553\n",
      "[Epoch 247/2500] Loss: 0.123430 | Time: 0.19s | Mean latent norm: 0.993783\n",
      "[Epoch 248/2500] Loss: 0.123437 | Time: 0.19s | Mean latent norm: 0.993770\n",
      "[Epoch 249/2500] Loss: 0.122523 | Time: 0.19s | Mean latent norm: 0.993755\n",
      "New best model saved at epoch 249 with loss 0.122523\n",
      "[Epoch 250/2500] Loss: 0.128986 | Time: 0.19s | Mean latent norm: 0.993746\n",
      "[Epoch 251/2500] Loss: 0.129896 | Time: 0.19s | Mean latent norm: 0.993723\n",
      "[Epoch 252/2500] Loss: 0.125280 | Time: 0.19s | Mean latent norm: 0.993698\n",
      "[Epoch 253/2500] Loss: 0.124872 | Time: 0.19s | Mean latent norm: 0.993686\n",
      "[Epoch 254/2500] Loss: 0.128538 | Time: 0.19s | Mean latent norm: 0.993653\n",
      "[Epoch 255/2500] Loss: 0.130795 | Time: 0.19s | Mean latent norm: 0.993622\n",
      "[Epoch 256/2500] Loss: 0.124975 | Time: 0.19s | Mean latent norm: 0.993583\n",
      "[Epoch 257/2500] Loss: 0.121624 | Time: 0.19s | Mean latent norm: 0.993576\n",
      "New best model saved at epoch 257 with loss 0.121624\n",
      "[Epoch 258/2500] Loss: 0.120219 | Time: 0.19s | Mean latent norm: 0.993571\n",
      "New best model saved at epoch 258 with loss 0.120219\n",
      "[Epoch 259/2500] Loss: 0.119113 | Time: 0.19s | Mean latent norm: 0.993559\n",
      "New best model saved at epoch 259 with loss 0.119113\n",
      "[Epoch 260/2500] Loss: 0.118106 | Time: 0.19s | Mean latent norm: 0.993566\n",
      "New best model saved at epoch 260 with loss 0.118106\n",
      "[Epoch 261/2500] Loss: 0.116815 | Time: 0.21s | Mean latent norm: 0.993575\n",
      "New best model saved at epoch 261 with loss 0.116815\n",
      "[Epoch 262/2500] Loss: 0.116505 | Time: 0.20s | Mean latent norm: 0.993578\n",
      "New best model saved at epoch 262 with loss 0.116505\n",
      "[Epoch 263/2500] Loss: 0.116873 | Time: 0.20s | Mean latent norm: 0.993580\n",
      "[Epoch 264/2500] Loss: 0.116454 | Time: 0.20s | Mean latent norm: 0.993578\n",
      "New best model saved at epoch 264 with loss 0.116454\n",
      "[Epoch 265/2500] Loss: 0.116454 | Time: 0.18s | Mean latent norm: 0.993584\n",
      "New best model saved at epoch 265 with loss 0.116454\n",
      "[Epoch 266/2500] Loss: 0.118031 | Time: 0.18s | Mean latent norm: 0.993582\n",
      "[Epoch 267/2500] Loss: 0.117080 | Time: 0.18s | Mean latent norm: 0.993580\n",
      "[Epoch 268/2500] Loss: 0.120931 | Time: 0.20s | Mean latent norm: 0.993548\n",
      "[Epoch 269/2500] Loss: 0.126342 | Time: 0.23s | Mean latent norm: 0.993531\n",
      "[Epoch 270/2500] Loss: 0.121051 | Time: 0.22s | Mean latent norm: 0.993522\n",
      "[Epoch 271/2500] Loss: 0.119760 | Time: 0.22s | Mean latent norm: 0.993517\n",
      "[Epoch 272/2500] Loss: 0.121620 | Time: 0.18s | Mean latent norm: 0.993487\n",
      "[Epoch 273/2500] Loss: 0.122861 | Time: 0.18s | Mean latent norm: 0.993457\n",
      "[Epoch 274/2500] Loss: 0.134368 | Time: 0.18s | Mean latent norm: 0.993415\n",
      "[Epoch 275/2500] Loss: 0.124815 | Time: 0.18s | Mean latent norm: 0.993367\n",
      "[Epoch 276/2500] Loss: 0.134405 | Time: 0.18s | Mean latent norm: 0.993349\n",
      "[Epoch 277/2500] Loss: 0.137042 | Time: 0.18s | Mean latent norm: 0.993289\n",
      "[Epoch 278/2500] Loss: 0.140339 | Time: 0.18s | Mean latent norm: 0.993255\n",
      "[Epoch 279/2500] Loss: 0.136082 | Time: 0.18s | Mean latent norm: 0.993161\n",
      "[Epoch 280/2500] Loss: 0.123982 | Time: 0.18s | Mean latent norm: 0.993108\n",
      "[Epoch 281/2500] Loss: 0.121600 | Time: 0.18s | Mean latent norm: 0.993069\n",
      "[Epoch 282/2500] Loss: 0.120384 | Time: 0.20s | Mean latent norm: 0.993046\n",
      "[Epoch 283/2500] Loss: 0.117332 | Time: 0.18s | Mean latent norm: 0.993038\n",
      "[Epoch 284/2500] Loss: 0.115602 | Time: 0.20s | Mean latent norm: 0.993039\n",
      "New best model saved at epoch 284 with loss 0.115602\n",
      "[Epoch 285/2500] Loss: 0.115082 | Time: 0.20s | Mean latent norm: 0.993048\n",
      "New best model saved at epoch 285 with loss 0.115082\n",
      "[Epoch 286/2500] Loss: 0.113618 | Time: 0.18s | Mean latent norm: 0.993049\n",
      "New best model saved at epoch 286 with loss 0.113618\n",
      "[Epoch 287/2500] Loss: 0.114083 | Time: 0.20s | Mean latent norm: 0.993046\n",
      "[Epoch 288/2500] Loss: 0.113613 | Time: 0.19s | Mean latent norm: 0.993050\n",
      "New best model saved at epoch 288 with loss 0.113613\n",
      "[Epoch 289/2500] Loss: 0.113480 | Time: 0.20s | Mean latent norm: 0.993053\n",
      "New best model saved at epoch 289 with loss 0.113480\n",
      "[Epoch 290/2500] Loss: 0.111970 | Time: 0.19s | Mean latent norm: 0.993056\n",
      "New best model saved at epoch 290 with loss 0.111970\n",
      "[Epoch 291/2500] Loss: 0.111681 | Time: 0.18s | Mean latent norm: 0.993053\n",
      "New best model saved at epoch 291 with loss 0.111681\n",
      "[Epoch 292/2500] Loss: 0.111964 | Time: 0.18s | Mean latent norm: 0.993054\n",
      "[Epoch 293/2500] Loss: 0.111272 | Time: 0.18s | Mean latent norm: 0.993057\n",
      "New best model saved at epoch 293 with loss 0.111272\n",
      "[Epoch 294/2500] Loss: 0.112314 | Time: 0.18s | Mean latent norm: 0.993061\n",
      "[Epoch 295/2500] Loss: 0.112264 | Time: 0.18s | Mean latent norm: 0.993058\n",
      "[Epoch 296/2500] Loss: 0.113937 | Time: 0.18s | Mean latent norm: 0.993050\n",
      "[Epoch 297/2500] Loss: 0.114977 | Time: 0.19s | Mean latent norm: 0.993037\n",
      "[Epoch 298/2500] Loss: 0.114358 | Time: 0.19s | Mean latent norm: 0.993016\n",
      "[Epoch 299/2500] Loss: 0.116320 | Time: 0.19s | Mean latent norm: 0.992999\n",
      "[Epoch 300/2500] Loss: 0.112753 | Time: 0.20s | Mean latent norm: 0.992988\n",
      "[Epoch 301/2500] Loss: 0.111547 | Time: 0.23s | Mean latent norm: 0.992983\n",
      "[Epoch 302/2500] Loss: 0.112508 | Time: 0.19s | Mean latent norm: 0.992973\n",
      "[Epoch 303/2500] Loss: 0.112489 | Time: 0.18s | Mean latent norm: 0.992963\n",
      "[Epoch 304/2500] Loss: 0.112842 | Time: 0.19s | Mean latent norm: 0.992955\n",
      "[Epoch 305/2500] Loss: 0.111075 | Time: 0.19s | Mean latent norm: 0.992946\n",
      "New best model saved at epoch 305 with loss 0.111075\n",
      "[Epoch 306/2500] Loss: 0.111444 | Time: 0.18s | Mean latent norm: 0.992929\n",
      "[Epoch 307/2500] Loss: 0.112520 | Time: 0.18s | Mean latent norm: 0.992931\n",
      "[Epoch 308/2500] Loss: 0.117394 | Time: 0.18s | Mean latent norm: 0.992922\n",
      "[Epoch 309/2500] Loss: 0.112332 | Time: 0.18s | Mean latent norm: 0.992903\n",
      "[Epoch 310/2500] Loss: 0.109131 | Time: 0.18s | Mean latent norm: 0.992893\n",
      "New best model saved at epoch 310 with loss 0.109131\n",
      "[Epoch 311/2500] Loss: 0.109530 | Time: 0.18s | Mean latent norm: 0.992891\n",
      "[Epoch 312/2500] Loss: 0.109843 | Time: 0.18s | Mean latent norm: 0.992889\n",
      "[Epoch 313/2500] Loss: 0.107154 | Time: 0.18s | Mean latent norm: 0.992885\n",
      "New best model saved at epoch 313 with loss 0.107154\n",
      "[Epoch 314/2500] Loss: 0.107790 | Time: 0.20s | Mean latent norm: 0.992879\n",
      "[Epoch 315/2500] Loss: 0.108453 | Time: 0.23s | Mean latent norm: 0.992881\n",
      "[Epoch 316/2500] Loss: 0.109210 | Time: 0.18s | Mean latent norm: 0.992871\n",
      "[Epoch 317/2500] Loss: 0.111425 | Time: 0.18s | Mean latent norm: 0.992869\n",
      "[Epoch 318/2500] Loss: 0.114601 | Time: 0.18s | Mean latent norm: 0.992834\n",
      "[Epoch 319/2500] Loss: 0.115446 | Time: 0.18s | Mean latent norm: 0.992801\n",
      "[Epoch 320/2500] Loss: 0.129669 | Time: 0.18s | Mean latent norm: 0.992782\n",
      "[Epoch 321/2500] Loss: 0.122894 | Time: 0.18s | Mean latent norm: 0.992719\n",
      "[Epoch 322/2500] Loss: 0.114284 | Time: 0.19s | Mean latent norm: 0.992687\n",
      "[Epoch 323/2500] Loss: 0.113058 | Time: 0.21s | Mean latent norm: 0.992670\n",
      "[Epoch 324/2500] Loss: 0.108060 | Time: 0.18s | Mean latent norm: 0.992660\n",
      "[Epoch 325/2500] Loss: 0.107920 | Time: 0.18s | Mean latent norm: 0.992669\n",
      "[Epoch 326/2500] Loss: 0.106206 | Time: 0.19s | Mean latent norm: 0.992670\n",
      "New best model saved at epoch 326 with loss 0.106206\n",
      "[Epoch 327/2500] Loss: 0.104816 | Time: 0.20s | Mean latent norm: 0.992673\n",
      "New best model saved at epoch 327 with loss 0.104816\n",
      "[Epoch 328/2500] Loss: 0.105515 | Time: 0.19s | Mean latent norm: 0.992676\n",
      "[Epoch 329/2500] Loss: 0.105609 | Time: 0.19s | Mean latent norm: 0.992677\n",
      "[Epoch 330/2500] Loss: 0.104557 | Time: 0.19s | Mean latent norm: 0.992679\n",
      "New best model saved at epoch 330 with loss 0.104557\n",
      "[Epoch 331/2500] Loss: 0.104885 | Time: 0.19s | Mean latent norm: 0.992680\n",
      "[Epoch 332/2500] Loss: 0.105470 | Time: 0.19s | Mean latent norm: 0.992676\n",
      "[Epoch 333/2500] Loss: 0.104620 | Time: 0.19s | Mean latent norm: 0.992672\n",
      "[Epoch 334/2500] Loss: 0.104501 | Time: 0.19s | Mean latent norm: 0.992673\n",
      "New best model saved at epoch 334 with loss 0.104501\n",
      "[Epoch 335/2500] Loss: 0.104523 | Time: 0.19s | Mean latent norm: 0.992673\n",
      "[Epoch 336/2500] Loss: 0.104094 | Time: 0.19s | Mean latent norm: 0.992665\n",
      "New best model saved at epoch 336 with loss 0.104094\n",
      "[Epoch 337/2500] Loss: 0.103148 | Time: 0.19s | Mean latent norm: 0.992656\n",
      "New best model saved at epoch 337 with loss 0.103148\n",
      "[Epoch 338/2500] Loss: 0.102904 | Time: 0.19s | Mean latent norm: 0.992660\n",
      "New best model saved at epoch 338 with loss 0.102904\n",
      "[Epoch 339/2500] Loss: 0.103112 | Time: 0.19s | Mean latent norm: 0.992659\n",
      "[Epoch 340/2500] Loss: 0.103191 | Time: 0.19s | Mean latent norm: 0.992657\n",
      "[Epoch 341/2500] Loss: 0.103737 | Time: 0.19s | Mean latent norm: 0.992662\n",
      "[Epoch 342/2500] Loss: 0.105729 | Time: 0.19s | Mean latent norm: 0.992646\n",
      "[Epoch 343/2500] Loss: 0.106179 | Time: 0.19s | Mean latent norm: 0.992631\n",
      "[Epoch 344/2500] Loss: 0.111057 | Time: 0.19s | Mean latent norm: 0.992624\n",
      "[Epoch 345/2500] Loss: 0.120969 | Time: 0.19s | Mean latent norm: 0.992584\n",
      "[Epoch 346/2500] Loss: 0.116816 | Time: 0.19s | Mean latent norm: 0.992537\n",
      "[Epoch 347/2500] Loss: 0.122736 | Time: 0.19s | Mean latent norm: 0.992471\n",
      "[Epoch 348/2500] Loss: 0.117781 | Time: 0.19s | Mean latent norm: 0.992423\n",
      "[Epoch 349/2500] Loss: 0.119727 | Time: 0.19s | Mean latent norm: 0.992405\n",
      "[Epoch 350/2500] Loss: 0.116422 | Time: 0.19s | Mean latent norm: 0.992352\n",
      "[Epoch 351/2500] Loss: 0.114529 | Time: 0.19s | Mean latent norm: 0.992335\n",
      "[Epoch 352/2500] Loss: 0.113936 | Time: 0.19s | Mean latent norm: 0.992305\n",
      "[Epoch 353/2500] Loss: 0.123217 | Time: 0.19s | Mean latent norm: 0.992280\n",
      "[Epoch 354/2500] Loss: 0.118197 | Time: 0.20s | Mean latent norm: 0.992235\n",
      "[Epoch 355/2500] Loss: 0.117096 | Time: 0.19s | Mean latent norm: 0.992215\n",
      "[Epoch 356/2500] Loss: 0.118423 | Time: 0.19s | Mean latent norm: 0.992163\n",
      "[Epoch 357/2500] Loss: 0.113305 | Time: 0.19s | Mean latent norm: 0.992156\n",
      "[Epoch 358/2500] Loss: 0.108607 | Time: 0.19s | Mean latent norm: 0.992156\n",
      "[Epoch 359/2500] Loss: 0.106785 | Time: 0.19s | Mean latent norm: 0.992146\n",
      "[Epoch 360/2500] Loss: 0.104654 | Time: 0.19s | Mean latent norm: 0.992138\n",
      "[Epoch 361/2500] Loss: 0.103278 | Time: 0.19s | Mean latent norm: 0.992135\n",
      "[Epoch 362/2500] Loss: 0.101906 | Time: 0.19s | Mean latent norm: 0.992136\n",
      "New best model saved at epoch 362 with loss 0.101906\n",
      "[Epoch 363/2500] Loss: 0.100813 | Time: 0.19s | Mean latent norm: 0.992137\n",
      "New best model saved at epoch 363 with loss 0.100813\n",
      "[Epoch 364/2500] Loss: 0.100400 | Time: 0.19s | Mean latent norm: 0.992139\n",
      "New best model saved at epoch 364 with loss 0.100400\n",
      "[Epoch 365/2500] Loss: 0.099828 | Time: 0.19s | Mean latent norm: 0.992142\n",
      "New best model saved at epoch 365 with loss 0.099828\n",
      "[Epoch 366/2500] Loss: 0.099550 | Time: 0.19s | Mean latent norm: 0.992150\n",
      "New best model saved at epoch 366 with loss 0.099550\n",
      "[Epoch 367/2500] Loss: 0.099695 | Time: 0.19s | Mean latent norm: 0.992149\n",
      "[Epoch 368/2500] Loss: 0.098994 | Time: 0.19s | Mean latent norm: 0.992154\n",
      "New best model saved at epoch 368 with loss 0.098994\n",
      "[Epoch 369/2500] Loss: 0.099043 | Time: 0.19s | Mean latent norm: 0.992158\n",
      "[Epoch 370/2500] Loss: 0.098548 | Time: 0.19s | Mean latent norm: 0.992162\n",
      "New best model saved at epoch 370 with loss 0.098548\n",
      "[Epoch 371/2500] Loss: 0.098612 | Time: 0.19s | Mean latent norm: 0.992165\n",
      "[Epoch 372/2500] Loss: 0.098565 | Time: 0.19s | Mean latent norm: 0.992166\n",
      "[Epoch 373/2500] Loss: 0.098154 | Time: 0.19s | Mean latent norm: 0.992164\n",
      "New best model saved at epoch 373 with loss 0.098154\n",
      "[Epoch 374/2500] Loss: 0.098433 | Time: 0.19s | Mean latent norm: 0.992166\n",
      "[Epoch 375/2500] Loss: 0.098633 | Time: 0.19s | Mean latent norm: 0.992171\n",
      "[Epoch 376/2500] Loss: 0.098457 | Time: 0.19s | Mean latent norm: 0.992168\n",
      "[Epoch 377/2500] Loss: 0.098729 | Time: 0.19s | Mean latent norm: 0.992164\n",
      "[Epoch 378/2500] Loss: 0.098061 | Time: 0.19s | Mean latent norm: 0.992168\n",
      "New best model saved at epoch 378 with loss 0.098061\n",
      "[Epoch 379/2500] Loss: 0.098216 | Time: 0.19s | Mean latent norm: 0.992170\n",
      "[Epoch 380/2500] Loss: 0.097125 | Time: 0.19s | Mean latent norm: 0.992168\n",
      "New best model saved at epoch 380 with loss 0.097125\n",
      "[Epoch 381/2500] Loss: 0.098055 | Time: 0.20s | Mean latent norm: 0.992170\n",
      "[Epoch 382/2500] Loss: 0.098072 | Time: 0.23s | Mean latent norm: 0.992167\n",
      "[Epoch 383/2500] Loss: 0.099523 | Time: 0.23s | Mean latent norm: 0.992159\n",
      "[Epoch 384/2500] Loss: 0.101257 | Time: 0.20s | Mean latent norm: 0.992153\n",
      "[Epoch 385/2500] Loss: 0.099891 | Time: 0.20s | Mean latent norm: 0.992146\n",
      "[Epoch 386/2500] Loss: 0.098651 | Time: 0.20s | Mean latent norm: 0.992142\n",
      "[Epoch 387/2500] Loss: 0.098584 | Time: 0.21s | Mean latent norm: 0.992136\n",
      "[Epoch 388/2500] Loss: 0.097032 | Time: 0.19s | Mean latent norm: 0.992133\n",
      "New best model saved at epoch 388 with loss 0.097032\n",
      "[Epoch 389/2500] Loss: 0.097664 | Time: 0.20s | Mean latent norm: 0.992136\n",
      "[Epoch 390/2500] Loss: 0.096872 | Time: 0.21s | Mean latent norm: 0.992133\n",
      "New best model saved at epoch 390 with loss 0.096872\n",
      "[Epoch 391/2500] Loss: 0.097084 | Time: 0.20s | Mean latent norm: 0.992135\n",
      "[Epoch 392/2500] Loss: 0.100826 | Time: 0.20s | Mean latent norm: 0.992117\n",
      "[Epoch 393/2500] Loss: 0.101195 | Time: 0.20s | Mean latent norm: 0.992101\n",
      "[Epoch 394/2500] Loss: 0.099879 | Time: 0.20s | Mean latent norm: 0.992089\n",
      "[Epoch 395/2500] Loss: 0.100851 | Time: 0.20s | Mean latent norm: 0.992070\n",
      "[Epoch 396/2500] Loss: 0.102199 | Time: 0.19s | Mean latent norm: 0.992055\n",
      "[Epoch 397/2500] Loss: 0.100998 | Time: 0.18s | Mean latent norm: 0.992041\n",
      "[Epoch 398/2500] Loss: 0.098383 | Time: 0.18s | Mean latent norm: 0.992023\n",
      "[Epoch 399/2500] Loss: 0.097175 | Time: 0.19s | Mean latent norm: 0.992016\n",
      "[Epoch 400/2500] Loss: 0.097021 | Time: 0.20s | Mean latent norm: 0.992017\n",
      "[Epoch 401/2500] Loss: 0.098341 | Time: 0.19s | Mean latent norm: 0.992009\n",
      "[Epoch 402/2500] Loss: 0.100505 | Time: 0.19s | Mean latent norm: 0.991995\n",
      "[Epoch 403/2500] Loss: 0.100235 | Time: 0.18s | Mean latent norm: 0.991986\n",
      "[Epoch 404/2500] Loss: 0.099316 | Time: 0.18s | Mean latent norm: 0.991970\n",
      "[Epoch 405/2500] Loss: 0.097409 | Time: 0.19s | Mean latent norm: 0.991954\n",
      "[Epoch 406/2500] Loss: 0.096888 | Time: 0.18s | Mean latent norm: 0.991954\n",
      "[Epoch 407/2500] Loss: 0.096248 | Time: 0.18s | Mean latent norm: 0.991952\n",
      "New best model saved at epoch 407 with loss 0.096248\n",
      "[Epoch 408/2500] Loss: 0.095705 | Time: 0.18s | Mean latent norm: 0.991953\n",
      "New best model saved at epoch 408 with loss 0.095705\n",
      "[Epoch 409/2500] Loss: 0.095347 | Time: 0.18s | Mean latent norm: 0.991949\n",
      "New best model saved at epoch 409 with loss 0.095347\n",
      "[Epoch 410/2500] Loss: 0.095178 | Time: 0.18s | Mean latent norm: 0.991953\n",
      "New best model saved at epoch 410 with loss 0.095178\n",
      "[Epoch 411/2500] Loss: 0.095196 | Time: 0.19s | Mean latent norm: 0.991956\n",
      "[Epoch 412/2500] Loss: 0.095252 | Time: 0.22s | Mean latent norm: 0.991955\n",
      "[Epoch 413/2500] Loss: 0.094868 | Time: 0.23s | Mean latent norm: 0.991948\n",
      "New best model saved at epoch 413 with loss 0.094868\n",
      "[Epoch 414/2500] Loss: 0.094841 | Time: 0.19s | Mean latent norm: 0.991947\n",
      "New best model saved at epoch 414 with loss 0.094841\n",
      "[Epoch 415/2500] Loss: 0.094966 | Time: 0.18s | Mean latent norm: 0.991943\n",
      "[Epoch 416/2500] Loss: 0.094778 | Time: 0.18s | Mean latent norm: 0.991936\n",
      "New best model saved at epoch 416 with loss 0.094778\n",
      "[Epoch 417/2500] Loss: 0.096355 | Time: 0.18s | Mean latent norm: 0.991930\n",
      "[Epoch 418/2500] Loss: 0.095503 | Time: 0.18s | Mean latent norm: 0.991925\n",
      "[Epoch 419/2500] Loss: 0.096629 | Time: 0.18s | Mean latent norm: 0.991918\n",
      "[Epoch 420/2500] Loss: 0.096834 | Time: 0.18s | Mean latent norm: 0.991912\n",
      "[Epoch 421/2500] Loss: 0.100092 | Time: 0.19s | Mean latent norm: 0.991895\n",
      "[Epoch 422/2500] Loss: 0.097178 | Time: 0.21s | Mean latent norm: 0.991868\n",
      "[Epoch 423/2500] Loss: 0.097095 | Time: 0.20s | Mean latent norm: 0.991862\n",
      "[Epoch 424/2500] Loss: 0.097671 | Time: 0.19s | Mean latent norm: 0.991854\n",
      "[Epoch 425/2500] Loss: 0.102371 | Time: 0.19s | Mean latent norm: 0.991823\n",
      "[Epoch 426/2500] Loss: 0.102586 | Time: 0.19s | Mean latent norm: 0.991800\n",
      "[Epoch 427/2500] Loss: 0.101951 | Time: 0.21s | Mean latent norm: 0.991762\n",
      "[Epoch 428/2500] Loss: 0.098016 | Time: 0.21s | Mean latent norm: 0.991746\n",
      "[Epoch 429/2500] Loss: 0.098189 | Time: 0.19s | Mean latent norm: 0.991738\n",
      "[Epoch 430/2500] Loss: 0.100382 | Time: 0.18s | Mean latent norm: 0.991717\n",
      "[Epoch 431/2500] Loss: 0.098230 | Time: 0.18s | Mean latent norm: 0.991696\n",
      "[Epoch 432/2500] Loss: 0.097298 | Time: 0.18s | Mean latent norm: 0.991682\n",
      "[Epoch 433/2500] Loss: 0.096420 | Time: 0.18s | Mean latent norm: 0.991661\n",
      "[Epoch 434/2500] Loss: 0.094554 | Time: 0.18s | Mean latent norm: 0.991655\n",
      "New best model saved at epoch 434 with loss 0.094554\n",
      "[Epoch 435/2500] Loss: 0.094049 | Time: 0.18s | Mean latent norm: 0.991644\n",
      "New best model saved at epoch 435 with loss 0.094049\n",
      "[Epoch 436/2500] Loss: 0.095026 | Time: 0.17s | Mean latent norm: 0.991640\n",
      "[Epoch 437/2500] Loss: 0.094756 | Time: 0.18s | Mean latent norm: 0.991642\n",
      "[Epoch 438/2500] Loss: 0.098674 | Time: 0.18s | Mean latent norm: 0.991633\n",
      "[Epoch 439/2500] Loss: 0.097488 | Time: 0.18s | Mean latent norm: 0.991609\n",
      "[Epoch 440/2500] Loss: 0.102649 | Time: 0.18s | Mean latent norm: 0.991574\n",
      "[Epoch 441/2500] Loss: 0.111921 | Time: 0.18s | Mean latent norm: 0.991539\n",
      "[Epoch 442/2500] Loss: 0.113061 | Time: 0.17s | Mean latent norm: 0.991500\n",
      "[Epoch 443/2500] Loss: 0.113366 | Time: 0.17s | Mean latent norm: 0.991421\n",
      "[Epoch 444/2500] Loss: 0.107053 | Time: 0.21s | Mean latent norm: 0.991384\n",
      "[Epoch 445/2500] Loss: 0.115552 | Time: 0.18s | Mean latent norm: 0.991322\n",
      "[Epoch 446/2500] Loss: 0.115468 | Time: 0.18s | Mean latent norm: 0.991294\n",
      "[Epoch 447/2500] Loss: 0.113486 | Time: 0.19s | Mean latent norm: 0.991225\n",
      "[Epoch 448/2500] Loss: 0.111963 | Time: 0.18s | Mean latent norm: 0.991188\n",
      "[Epoch 449/2500] Loss: 0.111502 | Time: 0.18s | Mean latent norm: 0.991151\n",
      "[Epoch 450/2500] Loss: 0.111605 | Time: 0.18s | Mean latent norm: 0.991142\n",
      "[Epoch 451/2500] Loss: 0.108786 | Time: 0.17s | Mean latent norm: 0.991104\n",
      "[Epoch 452/2500] Loss: 0.107087 | Time: 0.19s | Mean latent norm: 0.991072\n",
      "[Epoch 453/2500] Loss: 0.103025 | Time: 0.18s | Mean latent norm: 0.991058\n",
      "[Epoch 454/2500] Loss: 0.104909 | Time: 0.18s | Mean latent norm: 0.991040\n",
      "[Epoch 455/2500] Loss: 0.101786 | Time: 0.17s | Mean latent norm: 0.991012\n",
      "[Epoch 456/2500] Loss: 0.099119 | Time: 0.18s | Mean latent norm: 0.990999\n",
      "[Epoch 457/2500] Loss: 0.097080 | Time: 0.18s | Mean latent norm: 0.990997\n",
      "[Epoch 458/2500] Loss: 0.093986 | Time: 0.18s | Mean latent norm: 0.990995\n",
      "New best model saved at epoch 458 with loss 0.093986\n",
      "[Epoch 459/2500] Loss: 0.093134 | Time: 0.18s | Mean latent norm: 0.990990\n",
      "New best model saved at epoch 459 with loss 0.093134\n",
      "[Epoch 460/2500] Loss: 0.092158 | Time: 0.18s | Mean latent norm: 0.990992\n",
      "New best model saved at epoch 460 with loss 0.092158\n",
      "[Epoch 461/2500] Loss: 0.092048 | Time: 0.18s | Mean latent norm: 0.991002\n",
      "New best model saved at epoch 461 with loss 0.092048\n",
      "[Epoch 462/2500] Loss: 0.092267 | Time: 0.18s | Mean latent norm: 0.991002\n",
      "[Epoch 463/2500] Loss: 0.092032 | Time: 0.18s | Mean latent norm: 0.991006\n",
      "New best model saved at epoch 463 with loss 0.092032\n",
      "[Epoch 464/2500] Loss: 0.092082 | Time: 0.18s | Mean latent norm: 0.991011\n",
      "[Epoch 465/2500] Loss: 0.091864 | Time: 0.18s | Mean latent norm: 0.991008\n",
      "New best model saved at epoch 465 with loss 0.091864\n",
      "[Epoch 466/2500] Loss: 0.091934 | Time: 0.18s | Mean latent norm: 0.991006\n",
      "[Epoch 467/2500] Loss: 0.091997 | Time: 0.17s | Mean latent norm: 0.991008\n",
      "[Epoch 468/2500] Loss: 0.093547 | Time: 0.17s | Mean latent norm: 0.991009\n",
      "[Epoch 469/2500] Loss: 0.095203 | Time: 0.20s | Mean latent norm: 0.991002\n",
      "[Epoch 470/2500] Loss: 0.093505 | Time: 0.17s | Mean latent norm: 0.990996\n",
      "[Epoch 471/2500] Loss: 0.095273 | Time: 0.17s | Mean latent norm: 0.990981\n",
      "[Epoch 472/2500] Loss: 0.095591 | Time: 0.19s | Mean latent norm: 0.990978\n",
      "[Epoch 473/2500] Loss: 0.091674 | Time: 0.18s | Mean latent norm: 0.990971\n",
      "New best model saved at epoch 473 with loss 0.091674\n",
      "[Epoch 474/2500] Loss: 0.091364 | Time: 0.18s | Mean latent norm: 0.990973\n",
      "New best model saved at epoch 474 with loss 0.091364\n",
      "[Epoch 475/2500] Loss: 0.091262 | Time: 0.18s | Mean latent norm: 0.990973\n",
      "New best model saved at epoch 475 with loss 0.091262\n",
      "[Epoch 476/2500] Loss: 0.091050 | Time: 0.18s | Mean latent norm: 0.990968\n",
      "New best model saved at epoch 476 with loss 0.091050\n",
      "[Epoch 477/2500] Loss: 0.091134 | Time: 0.19s | Mean latent norm: 0.990971\n",
      "[Epoch 478/2500] Loss: 0.094029 | Time: 0.18s | Mean latent norm: 0.990965\n",
      "[Epoch 479/2500] Loss: 0.092939 | Time: 0.17s | Mean latent norm: 0.990954\n",
      "[Epoch 480/2500] Loss: 0.094570 | Time: 0.17s | Mean latent norm: 0.990951\n",
      "[Epoch 481/2500] Loss: 0.098088 | Time: 0.19s | Mean latent norm: 0.990930\n",
      "[Epoch 482/2500] Loss: 0.097448 | Time: 0.18s | Mean latent norm: 0.990929\n",
      "[Epoch 483/2500] Loss: 0.110687 | Time: 0.17s | Mean latent norm: 0.990876\n",
      "[Epoch 484/2500] Loss: 0.097292 | Time: 0.20s | Mean latent norm: 0.990833\n",
      "[Epoch 485/2500] Loss: 0.096683 | Time: 0.17s | Mean latent norm: 0.990829\n",
      "[Epoch 486/2500] Loss: 0.092997 | Time: 0.18s | Mean latent norm: 0.990822\n",
      "[Epoch 487/2500] Loss: 0.091780 | Time: 0.17s | Mean latent norm: 0.990821\n",
      "[Epoch 488/2500] Loss: 0.091676 | Time: 0.17s | Mean latent norm: 0.990826\n",
      "[Epoch 489/2500] Loss: 0.092161 | Time: 0.19s | Mean latent norm: 0.990820\n",
      "[Epoch 490/2500] Loss: 0.091965 | Time: 0.18s | Mean latent norm: 0.990804\n",
      "[Epoch 491/2500] Loss: 0.090245 | Time: 0.17s | Mean latent norm: 0.990798\n",
      "New best model saved at epoch 491 with loss 0.090245\n",
      "[Epoch 492/2500] Loss: 0.089823 | Time: 0.18s | Mean latent norm: 0.990812\n",
      "New best model saved at epoch 492 with loss 0.089823\n",
      "[Epoch 493/2500] Loss: 0.090193 | Time: 0.18s | Mean latent norm: 0.990799\n",
      "[Epoch 494/2500] Loss: 0.090132 | Time: 0.18s | Mean latent norm: 0.990794\n",
      "[Epoch 495/2500] Loss: 0.089475 | Time: 0.18s | Mean latent norm: 0.990795\n",
      "New best model saved at epoch 495 with loss 0.089475\n",
      "[Epoch 496/2500] Loss: 0.089795 | Time: 0.18s | Mean latent norm: 0.990798\n",
      "[Epoch 497/2500] Loss: 0.090066 | Time: 0.18s | Mean latent norm: 0.990801\n",
      "[Epoch 498/2500] Loss: 0.089508 | Time: 0.18s | Mean latent norm: 0.990800\n",
      "[Epoch 499/2500] Loss: 0.089621 | Time: 0.18s | Mean latent norm: 0.990797\n",
      "[Epoch 500/2500] Loss: 0.089510 | Time: 0.18s | Mean latent norm: 0.990802\n",
      "[Epoch 501/2500] Loss: 0.089483 | Time: 0.18s | Mean latent norm: 0.990801\n",
      "[Epoch 502/2500] Loss: 0.089179 | Time: 0.18s | Mean latent norm: 0.990800\n",
      "New best model saved at epoch 502 with loss 0.089179\n",
      "[Epoch 503/2500] Loss: 0.088825 | Time: 0.18s | Mean latent norm: 0.990806\n",
      "New best model saved at epoch 503 with loss 0.088825\n",
      "[Epoch 504/2500] Loss: 0.089137 | Time: 0.18s | Mean latent norm: 0.990805\n",
      "[Epoch 505/2500] Loss: 0.088792 | Time: 0.17s | Mean latent norm: 0.990796\n",
      "New best model saved at epoch 505 with loss 0.088792\n",
      "[Epoch 506/2500] Loss: 0.088735 | Time: 0.18s | Mean latent norm: 0.990797\n",
      "New best model saved at epoch 506 with loss 0.088735\n",
      "[Epoch 507/2500] Loss: 0.089165 | Time: 0.18s | Mean latent norm: 0.990789\n",
      "[Epoch 508/2500] Loss: 0.088829 | Time: 0.18s | Mean latent norm: 0.990797\n",
      "[Epoch 509/2500] Loss: 0.088691 | Time: 0.18s | Mean latent norm: 0.990800\n",
      "New best model saved at epoch 509 with loss 0.088691\n",
      "[Epoch 510/2500] Loss: 0.088576 | Time: 0.18s | Mean latent norm: 0.990799\n",
      "New best model saved at epoch 510 with loss 0.088576\n",
      "[Epoch 511/2500] Loss: 0.088320 | Time: 0.18s | Mean latent norm: 0.990797\n",
      "New best model saved at epoch 511 with loss 0.088320\n",
      "[Epoch 512/2500] Loss: 0.088199 | Time: 0.18s | Mean latent norm: 0.990801\n",
      "New best model saved at epoch 512 with loss 0.088199\n",
      "[Epoch 513/2500] Loss: 0.088324 | Time: 0.18s | Mean latent norm: 0.990794\n",
      "[Epoch 514/2500] Loss: 0.088260 | Time: 0.18s | Mean latent norm: 0.990793\n",
      "[Epoch 515/2500] Loss: 0.088211 | Time: 0.17s | Mean latent norm: 0.990797\n",
      "[Epoch 516/2500] Loss: 0.089111 | Time: 0.17s | Mean latent norm: 0.990790\n",
      "[Epoch 517/2500] Loss: 0.090217 | Time: 0.18s | Mean latent norm: 0.990785\n",
      "[Epoch 518/2500] Loss: 0.090612 | Time: 0.18s | Mean latent norm: 0.990771\n",
      "[Epoch 519/2500] Loss: 0.090250 | Time: 0.18s | Mean latent norm: 0.990756\n",
      "[Epoch 520/2500] Loss: 0.090265 | Time: 0.18s | Mean latent norm: 0.990743\n",
      "[Epoch 521/2500] Loss: 0.091286 | Time: 0.17s | Mean latent norm: 0.990731\n",
      "[Epoch 522/2500] Loss: 0.094458 | Time: 0.17s | Mean latent norm: 0.990726\n",
      "[Epoch 523/2500] Loss: 0.094816 | Time: 0.19s | Mean latent norm: 0.990693\n",
      "[Epoch 524/2500] Loss: 0.093430 | Time: 0.18s | Mean latent norm: 0.990684\n",
      "[Epoch 525/2500] Loss: 0.105118 | Time: 0.17s | Mean latent norm: 0.990640\n",
      "[Epoch 526/2500] Loss: 0.098926 | Time: 0.19s | Mean latent norm: 0.990606\n",
      "[Epoch 527/2500] Loss: 0.100931 | Time: 0.17s | Mean latent norm: 0.990548\n",
      "[Epoch 528/2500] Loss: 0.099108 | Time: 0.18s | Mean latent norm: 0.990513\n",
      "[Epoch 529/2500] Loss: 0.097829 | Time: 0.19s | Mean latent norm: 0.990483\n",
      "[Epoch 530/2500] Loss: 0.097066 | Time: 0.18s | Mean latent norm: 0.990449\n",
      "[Epoch 531/2500] Loss: 0.092797 | Time: 0.18s | Mean latent norm: 0.990420\n",
      "[Epoch 532/2500] Loss: 0.092842 | Time: 0.18s | Mean latent norm: 0.990414\n",
      "[Epoch 533/2500] Loss: 0.093641 | Time: 0.17s | Mean latent norm: 0.990418\n",
      "[Epoch 534/2500] Loss: 0.093873 | Time: 0.19s | Mean latent norm: 0.990400\n",
      "[Epoch 535/2500] Loss: 0.093525 | Time: 0.18s | Mean latent norm: 0.990373\n",
      "[Epoch 536/2500] Loss: 0.091846 | Time: 0.18s | Mean latent norm: 0.990362\n",
      "[Epoch 537/2500] Loss: 0.089979 | Time: 0.19s | Mean latent norm: 0.990362\n",
      "[Epoch 538/2500] Loss: 0.090881 | Time: 0.18s | Mean latent norm: 0.990353\n",
      "[Epoch 539/2500] Loss: 0.089970 | Time: 0.18s | Mean latent norm: 0.990341\n",
      "[Epoch 540/2500] Loss: 0.088441 | Time: 0.19s | Mean latent norm: 0.990339\n",
      "[Epoch 541/2500] Loss: 0.088495 | Time: 0.18s | Mean latent norm: 0.990341\n",
      "[Epoch 542/2500] Loss: 0.088692 | Time: 0.18s | Mean latent norm: 0.990334\n",
      "[Epoch 543/2500] Loss: 0.088235 | Time: 0.18s | Mean latent norm: 0.990335\n",
      "[Epoch 544/2500] Loss: 0.088269 | Time: 0.17s | Mean latent norm: 0.990334\n",
      "[Epoch 545/2500] Loss: 0.088059 | Time: 0.19s | Mean latent norm: 0.990332\n",
      "New best model saved at epoch 545 with loss 0.088059\n",
      "[Epoch 546/2500] Loss: 0.087341 | Time: 0.18s | Mean latent norm: 0.990329\n",
      "New best model saved at epoch 546 with loss 0.087341\n",
      "[Epoch 547/2500] Loss: 0.087101 | Time: 0.18s | Mean latent norm: 0.990327\n",
      "New best model saved at epoch 547 with loss 0.087101\n",
      "[Epoch 548/2500] Loss: 0.087112 | Time: 0.19s | Mean latent norm: 0.990325\n",
      "[Epoch 549/2500] Loss: 0.086786 | Time: 0.18s | Mean latent norm: 0.990325\n",
      "New best model saved at epoch 549 with loss 0.086786\n",
      "[Epoch 550/2500] Loss: 0.086793 | Time: 0.18s | Mean latent norm: 0.990325\n",
      "[Epoch 551/2500] Loss: 0.087066 | Time: 0.18s | Mean latent norm: 0.990327\n",
      "[Epoch 552/2500] Loss: 0.086482 | Time: 0.18s | Mean latent norm: 0.990324\n",
      "New best model saved at epoch 552 with loss 0.086482\n",
      "[Epoch 553/2500] Loss: 0.086948 | Time: 0.19s | Mean latent norm: 0.990327\n",
      "[Epoch 554/2500] Loss: 0.087115 | Time: 0.17s | Mean latent norm: 0.990330\n",
      "[Epoch 555/2500] Loss: 0.089069 | Time: 0.17s | Mean latent norm: 0.990324\n",
      "[Epoch 556/2500] Loss: 0.087816 | Time: 0.19s | Mean latent norm: 0.990313\n",
      "[Epoch 557/2500] Loss: 0.089114 | Time: 0.18s | Mean latent norm: 0.990301\n",
      "[Epoch 558/2500] Loss: 0.090589 | Time: 0.18s | Mean latent norm: 0.990279\n",
      "[Epoch 559/2500] Loss: 0.090406 | Time: 0.19s | Mean latent norm: 0.990265\n",
      "[Epoch 560/2500] Loss: 0.089031 | Time: 0.17s | Mean latent norm: 0.990259\n",
      "[Epoch 561/2500] Loss: 0.089515 | Time: 0.19s | Mean latent norm: 0.990246\n",
      "[Epoch 562/2500] Loss: 0.088584 | Time: 0.19s | Mean latent norm: 0.990230\n",
      "[Epoch 563/2500] Loss: 0.087178 | Time: 0.21s | Mean latent norm: 0.990221\n",
      "[Epoch 564/2500] Loss: 0.086753 | Time: 0.17s | Mean latent norm: 0.990213\n",
      "[Epoch 565/2500] Loss: 0.087696 | Time: 0.17s | Mean latent norm: 0.990216\n",
      "[Epoch 566/2500] Loss: 0.087814 | Time: 0.18s | Mean latent norm: 0.990205\n",
      "[Epoch 567/2500] Loss: 0.087121 | Time: 0.17s | Mean latent norm: 0.990204\n",
      "[Epoch 568/2500] Loss: 0.087835 | Time: 0.17s | Mean latent norm: 0.990192\n",
      "[Epoch 569/2500] Loss: 0.088947 | Time: 0.18s | Mean latent norm: 0.990188\n",
      "[Epoch 570/2500] Loss: 0.089717 | Time: 0.18s | Mean latent norm: 0.990171\n",
      "[Epoch 571/2500] Loss: 0.088812 | Time: 0.18s | Mean latent norm: 0.990147\n",
      "[Epoch 572/2500] Loss: 0.088032 | Time: 0.17s | Mean latent norm: 0.990139\n",
      "[Epoch 573/2500] Loss: 0.086180 | Time: 0.18s | Mean latent norm: 0.990144\n",
      "New best model saved at epoch 573 with loss 0.086180\n",
      "[Epoch 574/2500] Loss: 0.085925 | Time: 0.21s | Mean latent norm: 0.990140\n",
      "New best model saved at epoch 574 with loss 0.085925\n",
      "[Epoch 575/2500] Loss: 0.085671 | Time: 0.18s | Mean latent norm: 0.990142\n",
      "New best model saved at epoch 575 with loss 0.085671\n",
      "[Epoch 576/2500] Loss: 0.085742 | Time: 0.19s | Mean latent norm: 0.990139\n",
      "[Epoch 577/2500] Loss: 0.085924 | Time: 0.18s | Mean latent norm: 0.990136\n",
      "[Epoch 578/2500] Loss: 0.085834 | Time: 0.18s | Mean latent norm: 0.990137\n",
      "[Epoch 579/2500] Loss: 0.086100 | Time: 0.19s | Mean latent norm: 0.990135\n",
      "[Epoch 580/2500] Loss: 0.085886 | Time: 0.17s | Mean latent norm: 0.990128\n",
      "[Epoch 581/2500] Loss: 0.086551 | Time: 0.18s | Mean latent norm: 0.990129\n",
      "[Epoch 582/2500] Loss: 0.086714 | Time: 0.19s | Mean latent norm: 0.990122\n",
      "[Epoch 583/2500] Loss: 0.086628 | Time: 0.18s | Mean latent norm: 0.990118\n",
      "[Epoch 584/2500] Loss: 0.086341 | Time: 0.17s | Mean latent norm: 0.990108\n",
      "[Epoch 585/2500] Loss: 0.087042 | Time: 0.19s | Mean latent norm: 0.990096\n",
      "[Epoch 586/2500] Loss: 0.087835 | Time: 0.18s | Mean latent norm: 0.990099\n",
      "[Epoch 587/2500] Loss: 0.087781 | Time: 0.18s | Mean latent norm: 0.990082\n",
      "[Epoch 588/2500] Loss: 0.087952 | Time: 0.19s | Mean latent norm: 0.990066\n",
      "[Epoch 589/2500] Loss: 0.088888 | Time: 0.18s | Mean latent norm: 0.990051\n",
      "[Epoch 590/2500] Loss: 0.091754 | Time: 0.19s | Mean latent norm: 0.990032\n",
      "[Epoch 591/2500] Loss: 0.093483 | Time: 0.18s | Mean latent norm: 0.989988\n",
      "[Epoch 592/2500] Loss: 0.094987 | Time: 0.18s | Mean latent norm: 0.989973\n",
      "[Epoch 593/2500] Loss: 0.094787 | Time: 0.18s | Mean latent norm: 0.989936\n",
      "[Epoch 594/2500] Loss: 0.097108 | Time: 0.18s | Mean latent norm: 0.989902\n",
      "[Epoch 595/2500] Loss: 0.096772 | Time: 0.17s | Mean latent norm: 0.989868\n",
      "[Epoch 596/2500] Loss: 0.092768 | Time: 0.19s | Mean latent norm: 0.989841\n",
      "[Epoch 597/2500] Loss: 0.091408 | Time: 0.18s | Mean latent norm: 0.989824\n",
      "[Epoch 598/2500] Loss: 0.090211 | Time: 0.17s | Mean latent norm: 0.989798\n",
      "[Epoch 599/2500] Loss: 0.088288 | Time: 0.19s | Mean latent norm: 0.989788\n",
      "[Epoch 600/2500] Loss: 0.090430 | Time: 0.17s | Mean latent norm: 0.989787\n",
      "[Epoch 601/2500] Loss: 0.087916 | Time: 0.18s | Mean latent norm: 0.989769\n",
      "[Epoch 602/2500] Loss: 0.087899 | Time: 0.19s | Mean latent norm: 0.989762\n",
      "[Epoch 603/2500] Loss: 0.087747 | Time: 0.17s | Mean latent norm: 0.989753\n",
      "[Epoch 604/2500] Loss: 0.086879 | Time: 0.18s | Mean latent norm: 0.989739\n",
      "[Epoch 605/2500] Loss: 0.085978 | Time: 0.18s | Mean latent norm: 0.989729\n",
      "[Epoch 606/2500] Loss: 0.085632 | Time: 0.17s | Mean latent norm: 0.989724\n",
      "New best model saved at epoch 606 with loss 0.085632\n",
      "[Epoch 607/2500] Loss: 0.086559 | Time: 0.19s | Mean latent norm: 0.989725\n",
      "[Epoch 608/2500] Loss: 0.086757 | Time: 0.18s | Mean latent norm: 0.989720\n",
      "[Epoch 609/2500] Loss: 0.087632 | Time: 0.18s | Mean latent norm: 0.989718\n",
      "[Epoch 610/2500] Loss: 0.091027 | Time: 0.19s | Mean latent norm: 0.989704\n",
      "[Epoch 611/2500] Loss: 0.097232 | Time: 0.17s | Mean latent norm: 0.989666\n",
      "[Epoch 612/2500] Loss: 0.096229 | Time: 0.18s | Mean latent norm: 0.989613\n",
      "[Epoch 613/2500] Loss: 0.096199 | Time: 0.19s | Mean latent norm: 0.989586\n",
      "[Epoch 614/2500] Loss: 0.090469 | Time: 0.17s | Mean latent norm: 0.989558\n",
      "[Epoch 615/2500] Loss: 0.090161 | Time: 0.18s | Mean latent norm: 0.989538\n",
      "[Epoch 616/2500] Loss: 0.086671 | Time: 0.18s | Mean latent norm: 0.989535\n",
      "[Epoch 617/2500] Loss: 0.086915 | Time: 0.17s | Mean latent norm: 0.989532\n",
      "[Epoch 618/2500] Loss: 0.086033 | Time: 0.18s | Mean latent norm: 0.989530\n",
      "[Epoch 619/2500] Loss: 0.086903 | Time: 0.17s | Mean latent norm: 0.989513\n",
      "[Epoch 620/2500] Loss: 0.085903 | Time: 0.18s | Mean latent norm: 0.989510\n",
      "[Epoch 621/2500] Loss: 0.089777 | Time: 0.18s | Mean latent norm: 0.989497\n",
      "[Epoch 622/2500] Loss: 0.092911 | Time: 0.17s | Mean latent norm: 0.989483\n",
      "[Epoch 623/2500] Loss: 0.092618 | Time: 0.17s | Mean latent norm: 0.989462\n",
      "[Epoch 624/2500] Loss: 0.088141 | Time: 0.19s | Mean latent norm: 0.989425\n",
      "[Epoch 625/2500] Loss: 0.087975 | Time: 0.18s | Mean latent norm: 0.989410\n",
      "[Epoch 626/2500] Loss: 0.085403 | Time: 0.17s | Mean latent norm: 0.989407\n",
      "New best model saved at epoch 626 with loss 0.085403\n",
      "[Epoch 627/2500] Loss: 0.084859 | Time: 0.18s | Mean latent norm: 0.989411\n",
      "New best model saved at epoch 627 with loss 0.084859\n",
      "[Epoch 628/2500] Loss: 0.084795 | Time: 0.19s | Mean latent norm: 0.989405\n",
      "New best model saved at epoch 628 with loss 0.084795\n",
      "[Epoch 629/2500] Loss: 0.084559 | Time: 0.18s | Mean latent norm: 0.989402\n",
      "New best model saved at epoch 629 with loss 0.084559\n",
      "[Epoch 630/2500] Loss: 0.084638 | Time: 0.19s | Mean latent norm: 0.989405\n",
      "[Epoch 631/2500] Loss: 0.084355 | Time: 0.18s | Mean latent norm: 0.989405\n",
      "New best model saved at epoch 631 with loss 0.084355\n",
      "[Epoch 632/2500] Loss: 0.083927 | Time: 0.17s | Mean latent norm: 0.989402\n",
      "New best model saved at epoch 632 with loss 0.083927\n",
      "[Epoch 633/2500] Loss: 0.083638 | Time: 0.19s | Mean latent norm: 0.989405\n",
      "New best model saved at epoch 633 with loss 0.083638\n",
      "[Epoch 634/2500] Loss: 0.083708 | Time: 0.18s | Mean latent norm: 0.989407\n",
      "[Epoch 635/2500] Loss: 0.083741 | Time: 0.18s | Mean latent norm: 0.989406\n",
      "[Epoch 636/2500] Loss: 0.084036 | Time: 0.21s | Mean latent norm: 0.989399\n",
      "[Epoch 637/2500] Loss: 0.083775 | Time: 0.18s | Mean latent norm: 0.989392\n",
      "[Epoch 638/2500] Loss: 0.084241 | Time: 0.19s | Mean latent norm: 0.989396\n",
      "[Epoch 639/2500] Loss: 0.084335 | Time: 0.18s | Mean latent norm: 0.989397\n",
      "[Epoch 640/2500] Loss: 0.083638 | Time: 0.18s | Mean latent norm: 0.989397\n",
      "[Epoch 641/2500] Loss: 0.083926 | Time: 0.19s | Mean latent norm: 0.989396\n",
      "[Epoch 642/2500] Loss: 0.084243 | Time: 0.17s | Mean latent norm: 0.989394\n",
      "[Epoch 643/2500] Loss: 0.084235 | Time: 0.18s | Mean latent norm: 0.989384\n",
      "[Epoch 644/2500] Loss: 0.084233 | Time: 0.19s | Mean latent norm: 0.989383\n",
      "[Epoch 645/2500] Loss: 0.085826 | Time: 0.18s | Mean latent norm: 0.989367\n",
      "[Epoch 646/2500] Loss: 0.084646 | Time: 0.20s | Mean latent norm: 0.989370\n",
      "[Epoch 647/2500] Loss: 0.086290 | Time: 0.18s | Mean latent norm: 0.989352\n",
      "[Epoch 648/2500] Loss: 0.084845 | Time: 0.19s | Mean latent norm: 0.989335\n",
      "[Epoch 649/2500] Loss: 0.085716 | Time: 0.22s | Mean latent norm: 0.989322\n",
      "[Epoch 650/2500] Loss: 0.084663 | Time: 0.20s | Mean latent norm: 0.989321\n",
      "[Epoch 651/2500] Loss: 0.086251 | Time: 0.21s | Mean latent norm: 0.989303\n",
      "[Epoch 652/2500] Loss: 0.085364 | Time: 0.19s | Mean latent norm: 0.989297\n",
      "[Epoch 653/2500] Loss: 0.086016 | Time: 0.18s | Mean latent norm: 0.989277\n",
      "[Epoch 654/2500] Loss: 0.087973 | Time: 0.23s | Mean latent norm: 0.989264\n",
      "[Epoch 655/2500] Loss: 0.089929 | Time: 0.18s | Mean latent norm: 0.989251\n",
      "[Epoch 656/2500] Loss: 0.089877 | Time: 0.17s | Mean latent norm: 0.989211\n",
      "[Epoch 657/2500] Loss: 0.088120 | Time: 0.18s | Mean latent norm: 0.989198\n",
      "[Epoch 658/2500] Loss: 0.089577 | Time: 0.18s | Mean latent norm: 0.989185\n",
      "[Epoch 659/2500] Loss: 0.087128 | Time: 0.17s | Mean latent norm: 0.989155\n",
      "[Epoch 660/2500] Loss: 0.086735 | Time: 0.17s | Mean latent norm: 0.989133\n",
      "[Epoch 661/2500] Loss: 0.084957 | Time: 0.18s | Mean latent norm: 0.989133\n",
      "[Epoch 662/2500] Loss: 0.084983 | Time: 0.18s | Mean latent norm: 0.989140\n",
      "[Epoch 663/2500] Loss: 0.085933 | Time: 0.18s | Mean latent norm: 0.989108\n",
      "[Epoch 664/2500] Loss: 0.084470 | Time: 0.17s | Mean latent norm: 0.989098\n",
      "[Epoch 665/2500] Loss: 0.084387 | Time: 0.18s | Mean latent norm: 0.989093\n",
      "[Epoch 666/2500] Loss: 0.084030 | Time: 0.18s | Mean latent norm: 0.989091\n",
      "[Epoch 667/2500] Loss: 0.085196 | Time: 0.17s | Mean latent norm: 0.989079\n",
      "[Epoch 668/2500] Loss: 0.085156 | Time: 0.18s | Mean latent norm: 0.989066\n",
      "[Epoch 669/2500] Loss: 0.086003 | Time: 0.17s | Mean latent norm: 0.989048\n",
      "[Epoch 670/2500] Loss: 0.085518 | Time: 0.18s | Mean latent norm: 0.989049\n",
      "[Epoch 671/2500] Loss: 0.085690 | Time: 0.18s | Mean latent norm: 0.989039\n",
      "[Epoch 672/2500] Loss: 0.084885 | Time: 0.18s | Mean latent norm: 0.989023\n",
      "[Epoch 673/2500] Loss: 0.084134 | Time: 0.17s | Mean latent norm: 0.989012\n",
      "[Epoch 674/2500] Loss: 0.086778 | Time: 0.18s | Mean latent norm: 0.989000\n",
      "[Epoch 675/2500] Loss: 0.085831 | Time: 0.18s | Mean latent norm: 0.988979\n",
      "[Epoch 676/2500] Loss: 0.088959 | Time: 0.18s | Mean latent norm: 0.988976\n",
      "[Epoch 677/2500] Loss: 0.088049 | Time: 0.18s | Mean latent norm: 0.988952\n",
      "[Epoch 678/2500] Loss: 0.095860 | Time: 0.18s | Mean latent norm: 0.988931\n",
      "[Epoch 679/2500] Loss: 0.092845 | Time: 0.18s | Mean latent norm: 0.988892\n",
      "[Epoch 680/2500] Loss: 0.088433 | Time: 0.18s | Mean latent norm: 0.988858\n",
      "[Epoch 681/2500] Loss: 0.088975 | Time: 0.18s | Mean latent norm: 0.988820\n",
      "[Epoch 682/2500] Loss: 0.086219 | Time: 0.17s | Mean latent norm: 0.988801\n",
      "[Epoch 683/2500] Loss: 0.087856 | Time: 0.18s | Mean latent norm: 0.988785\n",
      "[Epoch 684/2500] Loss: 0.087332 | Time: 0.17s | Mean latent norm: 0.988773\n",
      "[Epoch 685/2500] Loss: 0.086865 | Time: 0.17s | Mean latent norm: 0.988758\n",
      "[Epoch 686/2500] Loss: 0.085340 | Time: 0.17s | Mean latent norm: 0.988731\n",
      "[Epoch 687/2500] Loss: 0.085281 | Time: 0.18s | Mean latent norm: 0.988731\n",
      "[Epoch 688/2500] Loss: 0.084096 | Time: 0.19s | Mean latent norm: 0.988725\n",
      "[Epoch 689/2500] Loss: 0.084107 | Time: 0.19s | Mean latent norm: 0.988716\n",
      "[Epoch 690/2500] Loss: 0.083588 | Time: 0.19s | Mean latent norm: 0.988707\n",
      "New best model saved at epoch 690 with loss 0.083588\n",
      "[Epoch 691/2500] Loss: 0.083573 | Time: 0.19s | Mean latent norm: 0.988701\n",
      "New best model saved at epoch 691 with loss 0.083573\n",
      "[Epoch 692/2500] Loss: 0.084008 | Time: 0.18s | Mean latent norm: 0.988697\n",
      "[Epoch 693/2500] Loss: 0.083282 | Time: 0.17s | Mean latent norm: 0.988691\n",
      "New best model saved at epoch 693 with loss 0.083282\n",
      "[Epoch 694/2500] Loss: 0.084355 | Time: 0.17s | Mean latent norm: 0.988682\n",
      "[Epoch 695/2500] Loss: 0.083435 | Time: 0.17s | Mean latent norm: 0.988671\n",
      "[Epoch 696/2500] Loss: 0.083556 | Time: 0.17s | Mean latent norm: 0.988668\n",
      "[Epoch 697/2500] Loss: 0.082662 | Time: 0.17s | Mean latent norm: 0.988664\n",
      "New best model saved at epoch 697 with loss 0.082662\n",
      "[Epoch 698/2500] Loss: 0.083755 | Time: 0.18s | Mean latent norm: 0.988657\n",
      "[Epoch 699/2500] Loss: 0.083038 | Time: 0.18s | Mean latent norm: 0.988653\n",
      "[Epoch 700/2500] Loss: 0.083189 | Time: 0.18s | Mean latent norm: 0.988645\n",
      "[Epoch 701/2500] Loss: 0.082323 | Time: 0.18s | Mean latent norm: 0.988633\n",
      "New best model saved at epoch 701 with loss 0.082323\n",
      "[Epoch 702/2500] Loss: 0.081560 | Time: 0.18s | Mean latent norm: 0.988632\n",
      "New best model saved at epoch 702 with loss 0.081560\n",
      "[Epoch 703/2500] Loss: 0.081684 | Time: 0.18s | Mean latent norm: 0.988632\n",
      "[Epoch 704/2500] Loss: 0.082271 | Time: 0.19s | Mean latent norm: 0.988624\n",
      "[Epoch 705/2500] Loss: 0.082124 | Time: 0.19s | Mean latent norm: 0.988631\n",
      "[Epoch 706/2500] Loss: 0.082634 | Time: 0.19s | Mean latent norm: 0.988615\n",
      "[Epoch 707/2500] Loss: 0.082587 | Time: 0.19s | Mean latent norm: 0.988617\n",
      "[Epoch 708/2500] Loss: 0.082109 | Time: 0.18s | Mean latent norm: 0.988613\n",
      "[Epoch 709/2500] Loss: 0.082247 | Time: 0.18s | Mean latent norm: 0.988608\n",
      "[Epoch 710/2500] Loss: 0.081855 | Time: 0.18s | Mean latent norm: 0.988607\n",
      "[Epoch 711/2500] Loss: 0.081676 | Time: 0.18s | Mean latent norm: 0.988606\n",
      "[Epoch 712/2500] Loss: 0.082188 | Time: 0.17s | Mean latent norm: 0.988603\n",
      "[Epoch 713/2500] Loss: 0.082424 | Time: 0.17s | Mean latent norm: 0.988598\n",
      "[Epoch 714/2500] Loss: 0.082717 | Time: 0.18s | Mean latent norm: 0.988590\n",
      "[Epoch 715/2500] Loss: 0.082533 | Time: 0.18s | Mean latent norm: 0.988589\n",
      "[Epoch 716/2500] Loss: 0.082574 | Time: 0.17s | Mean latent norm: 0.988578\n",
      "[Epoch 717/2500] Loss: 0.083148 | Time: 0.18s | Mean latent norm: 0.988567\n",
      "[Epoch 718/2500] Loss: 0.082529 | Time: 0.17s | Mean latent norm: 0.988553\n",
      "[Epoch 719/2500] Loss: 0.084049 | Time: 0.17s | Mean latent norm: 0.988551\n",
      "[Epoch 720/2500] Loss: 0.083311 | Time: 0.17s | Mean latent norm: 0.988539\n",
      "[Epoch 721/2500] Loss: 0.084524 | Time: 0.17s | Mean latent norm: 0.988519\n",
      "[Epoch 722/2500] Loss: 0.084781 | Time: 0.18s | Mean latent norm: 0.988494\n",
      "[Epoch 723/2500] Loss: 0.089517 | Time: 0.18s | Mean latent norm: 0.988481\n",
      "[Epoch 724/2500] Loss: 0.087409 | Time: 0.17s | Mean latent norm: 0.988454\n",
      "[Epoch 725/2500] Loss: 0.088642 | Time: 0.18s | Mean latent norm: 0.988433\n",
      "[Epoch 726/2500] Loss: 0.087803 | Time: 0.17s | Mean latent norm: 0.988393\n",
      "[Epoch 727/2500] Loss: 0.091534 | Time: 0.17s | Mean latent norm: 0.988355\n",
      "[Epoch 728/2500] Loss: 0.088957 | Time: 0.17s | Mean latent norm: 0.988327\n",
      "[Epoch 729/2500] Loss: 0.085727 | Time: 0.18s | Mean latent norm: 0.988310\n",
      "[Epoch 730/2500] Loss: 0.083950 | Time: 0.17s | Mean latent norm: 0.988304\n",
      "[Epoch 731/2500] Loss: 0.084606 | Time: 0.17s | Mean latent norm: 0.988282\n",
      "[Epoch 732/2500] Loss: 0.083120 | Time: 0.18s | Mean latent norm: 0.988265\n",
      "[Epoch 733/2500] Loss: 0.082224 | Time: 0.18s | Mean latent norm: 0.988262\n",
      "[Epoch 734/2500] Loss: 0.081748 | Time: 0.17s | Mean latent norm: 0.988255\n",
      "[Epoch 735/2500] Loss: 0.081481 | Time: 0.17s | Mean latent norm: 0.988249\n",
      "New best model saved at epoch 735 with loss 0.081481\n",
      "[Epoch 736/2500] Loss: 0.081658 | Time: 0.18s | Mean latent norm: 0.988250\n",
      "[Epoch 737/2500] Loss: 0.081677 | Time: 0.17s | Mean latent norm: 0.988248\n",
      "[Epoch 738/2500] Loss: 0.081760 | Time: 0.17s | Mean latent norm: 0.988237\n",
      "[Epoch 739/2500] Loss: 0.081608 | Time: 0.17s | Mean latent norm: 0.988226\n",
      "[Epoch 740/2500] Loss: 0.081857 | Time: 0.17s | Mean latent norm: 0.988220\n",
      "[Epoch 741/2500] Loss: 0.082077 | Time: 0.17s | Mean latent norm: 0.988221\n",
      "[Epoch 742/2500] Loss: 0.083340 | Time: 0.18s | Mean latent norm: 0.988210\n",
      "[Epoch 743/2500] Loss: 0.081564 | Time: 0.18s | Mean latent norm: 0.988198\n",
      "[Epoch 744/2500] Loss: 0.082309 | Time: 0.17s | Mean latent norm: 0.988179\n",
      "[Epoch 745/2500] Loss: 0.082655 | Time: 0.17s | Mean latent norm: 0.988183\n",
      "[Epoch 746/2500] Loss: 0.083317 | Time: 0.18s | Mean latent norm: 0.988167\n",
      "[Epoch 747/2500] Loss: 0.083188 | Time: 0.18s | Mean latent norm: 0.988151\n",
      "[Epoch 748/2500] Loss: 0.083004 | Time: 0.18s | Mean latent norm: 0.988136\n",
      "[Epoch 749/2500] Loss: 0.086672 | Time: 0.17s | Mean latent norm: 0.988106\n",
      "[Epoch 750/2500] Loss: 0.088321 | Time: 0.17s | Mean latent norm: 0.988062\n",
      "[Epoch 751/2500] Loss: 0.087925 | Time: 0.17s | Mean latent norm: 0.988039\n",
      "[Epoch 752/2500] Loss: 0.089403 | Time: 0.17s | Mean latent norm: 0.988000\n",
      "[Epoch 753/2500] Loss: 0.088035 | Time: 0.18s | Mean latent norm: 0.987942\n",
      "[Epoch 754/2500] Loss: 0.084877 | Time: 0.17s | Mean latent norm: 0.987917\n",
      "[Epoch 755/2500] Loss: 0.083332 | Time: 0.17s | Mean latent norm: 0.987895\n",
      "[Epoch 756/2500] Loss: 0.083662 | Time: 0.18s | Mean latent norm: 0.987898\n",
      "[Epoch 757/2500] Loss: 0.089158 | Time: 0.18s | Mean latent norm: 0.987877\n",
      "[Epoch 758/2500] Loss: 0.088682 | Time: 0.18s | Mean latent norm: 0.987853\n",
      "[Epoch 759/2500] Loss: 0.087599 | Time: 0.17s | Mean latent norm: 0.987816\n",
      "[Epoch 760/2500] Loss: 0.087076 | Time: 0.18s | Mean latent norm: 0.987790\n",
      "[Epoch 761/2500] Loss: 0.091590 | Time: 0.17s | Mean latent norm: 0.987763\n",
      "[Epoch 762/2500] Loss: 0.093693 | Time: 0.17s | Mean latent norm: 0.987743\n",
      "[Epoch 763/2500] Loss: 0.093623 | Time: 0.17s | Mean latent norm: 0.987714\n",
      "[Epoch 764/2500] Loss: 0.089831 | Time: 0.18s | Mean latent norm: 0.987676\n",
      "[Epoch 765/2500] Loss: 0.087085 | Time: 0.17s | Mean latent norm: 0.987653\n",
      "[Epoch 766/2500] Loss: 0.083355 | Time: 0.17s | Mean latent norm: 0.987626\n",
      "[Epoch 767/2500] Loss: 0.081893 | Time: 0.17s | Mean latent norm: 0.987613\n",
      "[Epoch 768/2500] Loss: 0.080807 | Time: 0.18s | Mean latent norm: 0.987613\n",
      "New best model saved at epoch 768 with loss 0.080807\n",
      "[Epoch 769/2500] Loss: 0.081598 | Time: 0.18s | Mean latent norm: 0.987624\n",
      "[Epoch 770/2500] Loss: 0.081635 | Time: 0.17s | Mean latent norm: 0.987617\n",
      "[Epoch 771/2500] Loss: 0.080635 | Time: 0.18s | Mean latent norm: 0.987602\n",
      "New best model saved at epoch 771 with loss 0.080635\n",
      "[Epoch 772/2500] Loss: 0.080390 | Time: 0.18s | Mean latent norm: 0.987591\n",
      "New best model saved at epoch 772 with loss 0.080390\n",
      "[Epoch 773/2500] Loss: 0.080187 | Time: 0.18s | Mean latent norm: 0.987589\n",
      "New best model saved at epoch 773 with loss 0.080187\n",
      "[Epoch 774/2500] Loss: 0.080358 | Time: 0.18s | Mean latent norm: 0.987588\n",
      "[Epoch 775/2500] Loss: 0.079813 | Time: 0.18s | Mean latent norm: 0.987588\n",
      "New best model saved at epoch 775 with loss 0.079813\n",
      "[Epoch 776/2500] Loss: 0.080340 | Time: 0.18s | Mean latent norm: 0.987583\n",
      "[Epoch 777/2500] Loss: 0.088643 | Time: 0.17s | Mean latent norm: 0.987558\n",
      "[Epoch 778/2500] Loss: 0.088735 | Time: 0.17s | Mean latent norm: 0.987537\n",
      "[Epoch 779/2500] Loss: 0.096150 | Time: 0.17s | Mean latent norm: 0.987519\n",
      "[Epoch 780/2500] Loss: 0.093309 | Time: 0.18s | Mean latent norm: 0.987467\n",
      "[Epoch 781/2500] Loss: 0.089179 | Time: 0.17s | Mean latent norm: 0.987444\n",
      "[Epoch 782/2500] Loss: 0.086380 | Time: 0.18s | Mean latent norm: 0.987415\n",
      "[Epoch 783/2500] Loss: 0.084260 | Time: 0.17s | Mean latent norm: 0.987399\n",
      "[Epoch 784/2500] Loss: 0.082312 | Time: 0.18s | Mean latent norm: 0.987391\n",
      "[Epoch 785/2500] Loss: 0.080396 | Time: 0.18s | Mean latent norm: 0.987393\n",
      "[Epoch 786/2500] Loss: 0.079633 | Time: 0.17s | Mean latent norm: 0.987402\n",
      "New best model saved at epoch 786 with loss 0.079633\n",
      "[Epoch 787/2500] Loss: 0.079548 | Time: 0.18s | Mean latent norm: 0.987393\n",
      "New best model saved at epoch 787 with loss 0.079548\n",
      "[Epoch 788/2500] Loss: 0.079116 | Time: 0.17s | Mean latent norm: 0.987387\n",
      "New best model saved at epoch 788 with loss 0.079116\n",
      "[Epoch 789/2500] Loss: 0.078693 | Time: 0.18s | Mean latent norm: 0.987387\n",
      "New best model saved at epoch 789 with loss 0.078693\n",
      "[Epoch 790/2500] Loss: 0.078829 | Time: 0.18s | Mean latent norm: 0.987388\n",
      "[Epoch 791/2500] Loss: 0.078539 | Time: 0.18s | Mean latent norm: 0.987385\n",
      "New best model saved at epoch 791 with loss 0.078539\n",
      "[Epoch 792/2500] Loss: 0.078654 | Time: 0.18s | Mean latent norm: 0.987382\n",
      "[Epoch 793/2500] Loss: 0.079665 | Time: 0.18s | Mean latent norm: 0.987376\n",
      "[Epoch 794/2500] Loss: 0.080242 | Time: 0.18s | Mean latent norm: 0.987362\n",
      "[Epoch 795/2500] Loss: 0.079604 | Time: 0.17s | Mean latent norm: 0.987352\n",
      "[Epoch 796/2500] Loss: 0.078814 | Time: 0.18s | Mean latent norm: 0.987352\n",
      "[Epoch 797/2500] Loss: 0.079149 | Time: 0.18s | Mean latent norm: 0.987346\n",
      "[Epoch 798/2500] Loss: 0.079543 | Time: 0.18s | Mean latent norm: 0.987342\n",
      "[Epoch 799/2500] Loss: 0.079157 | Time: 0.18s | Mean latent norm: 0.987334\n",
      "[Epoch 800/2500] Loss: 0.079511 | Time: 0.18s | Mean latent norm: 0.987327\n",
      "[Epoch 801/2500] Loss: 0.080314 | Time: 0.19s | Mean latent norm: 0.987322\n",
      "[Epoch 802/2500] Loss: 0.079968 | Time: 0.19s | Mean latent norm: 0.987311\n",
      "[Epoch 803/2500] Loss: 0.083328 | Time: 0.18s | Mean latent norm: 0.987305\n",
      "[Epoch 804/2500] Loss: 0.082676 | Time: 0.18s | Mean latent norm: 0.987283\n",
      "[Epoch 805/2500] Loss: 0.083823 | Time: 0.18s | Mean latent norm: 0.987268\n",
      "[Epoch 806/2500] Loss: 0.085355 | Time: 0.17s | Mean latent norm: 0.987235\n",
      "[Epoch 807/2500] Loss: 0.088087 | Time: 0.17s | Mean latent norm: 0.987233\n",
      "[Epoch 808/2500] Loss: 0.085410 | Time: 0.17s | Mean latent norm: 0.987217\n",
      "[Epoch 809/2500] Loss: 0.091458 | Time: 0.18s | Mean latent norm: 0.987175\n",
      "[Epoch 810/2500] Loss: 0.089051 | Time: 0.18s | Mean latent norm: 0.987140\n",
      "[Epoch 811/2500] Loss: 0.083883 | Time: 0.17s | Mean latent norm: 0.987109\n",
      "[Epoch 812/2500] Loss: 0.080970 | Time: 0.17s | Mean latent norm: 0.987086\n",
      "[Epoch 813/2500] Loss: 0.082201 | Time: 0.18s | Mean latent norm: 0.987070\n",
      "[Epoch 814/2500] Loss: 0.079760 | Time: 0.18s | Mean latent norm: 0.987064\n",
      "[Epoch 815/2500] Loss: 0.080491 | Time: 0.18s | Mean latent norm: 0.987069\n",
      "[Epoch 816/2500] Loss: 0.080503 | Time: 0.18s | Mean latent norm: 0.987050\n",
      "[Epoch 817/2500] Loss: 0.080794 | Time: 0.19s | Mean latent norm: 0.987041\n",
      "[Epoch 818/2500] Loss: 0.080534 | Time: 0.18s | Mean latent norm: 0.987020\n",
      "[Epoch 819/2500] Loss: 0.080149 | Time: 0.19s | Mean latent norm: 0.987024\n",
      "[Epoch 820/2500] Loss: 0.079906 | Time: 0.18s | Mean latent norm: 0.987014\n",
      "[Epoch 821/2500] Loss: 0.082268 | Time: 0.18s | Mean latent norm: 0.986986\n",
      "[Epoch 822/2500] Loss: 0.082683 | Time: 0.18s | Mean latent norm: 0.986968\n",
      "[Epoch 823/2500] Loss: 0.083965 | Time: 0.17s | Mean latent norm: 0.986959\n",
      "[Epoch 824/2500] Loss: 0.087494 | Time: 0.18s | Mean latent norm: 0.986951\n",
      "[Epoch 825/2500] Loss: 0.087130 | Time: 0.17s | Mean latent norm: 0.986931\n",
      "[Epoch 826/2500] Loss: 0.089163 | Time: 0.18s | Mean latent norm: 0.986887\n",
      "[Epoch 827/2500] Loss: 0.095094 | Time: 0.17s | Mean latent norm: 0.986855\n",
      "[Epoch 828/2500] Loss: 0.090623 | Time: 0.17s | Mean latent norm: 0.986824\n",
      "[Epoch 829/2500] Loss: 0.086311 | Time: 0.18s | Mean latent norm: 0.986794\n",
      "[Epoch 830/2500] Loss: 0.080683 | Time: 0.18s | Mean latent norm: 0.986784\n",
      "[Epoch 831/2500] Loss: 0.078595 | Time: 0.19s | Mean latent norm: 0.986769\n",
      "[Epoch 832/2500] Loss: 0.077782 | Time: 0.19s | Mean latent norm: 0.986776\n",
      "New best model saved at epoch 832 with loss 0.077782\n",
      "[Epoch 833/2500] Loss: 0.077658 | Time: 0.19s | Mean latent norm: 0.986776\n",
      "New best model saved at epoch 833 with loss 0.077658\n",
      "[Epoch 834/2500] Loss: 0.077052 | Time: 0.19s | Mean latent norm: 0.986779\n",
      "New best model saved at epoch 834 with loss 0.077052\n",
      "[Epoch 835/2500] Loss: 0.077558 | Time: 0.18s | Mean latent norm: 0.986781\n",
      "[Epoch 836/2500] Loss: 0.077329 | Time: 0.18s | Mean latent norm: 0.986785\n",
      "[Epoch 837/2500] Loss: 0.077197 | Time: 0.17s | Mean latent norm: 0.986785\n",
      "[Epoch 838/2500] Loss: 0.077085 | Time: 0.17s | Mean latent norm: 0.986771\n",
      "[Epoch 839/2500] Loss: 0.076982 | Time: 0.18s | Mean latent norm: 0.986767\n",
      "New best model saved at epoch 839 with loss 0.076982\n",
      "[Epoch 840/2500] Loss: 0.077191 | Time: 0.18s | Mean latent norm: 0.986772\n",
      "[Epoch 841/2500] Loss: 0.077214 | Time: 0.17s | Mean latent norm: 0.986774\n",
      "[Epoch 842/2500] Loss: 0.077236 | Time: 0.18s | Mean latent norm: 0.986761\n",
      "[Epoch 843/2500] Loss: 0.079442 | Time: 0.18s | Mean latent norm: 0.986761\n",
      "[Epoch 844/2500] Loss: 0.079285 | Time: 0.17s | Mean latent norm: 0.986751\n",
      "[Epoch 845/2500] Loss: 0.078947 | Time: 0.18s | Mean latent norm: 0.986726\n",
      "[Epoch 846/2500] Loss: 0.079012 | Time: 0.17s | Mean latent norm: 0.986725\n",
      "[Epoch 847/2500] Loss: 0.078219 | Time: 0.18s | Mean latent norm: 0.986721\n",
      "[Epoch 848/2500] Loss: 0.077973 | Time: 0.18s | Mean latent norm: 0.986708\n",
      "[Epoch 849/2500] Loss: 0.076527 | Time: 0.17s | Mean latent norm: 0.986710\n",
      "New best model saved at epoch 849 with loss 0.076527\n",
      "[Epoch 850/2500] Loss: 0.077542 | Time: 0.18s | Mean latent norm: 0.986703\n",
      "[Epoch 851/2500] Loss: 0.077388 | Time: 0.17s | Mean latent norm: 0.986691\n",
      "[Epoch 852/2500] Loss: 0.077701 | Time: 0.18s | Mean latent norm: 0.986688\n",
      "[Epoch 853/2500] Loss: 0.076930 | Time: 0.18s | Mean latent norm: 0.986687\n",
      "[Epoch 854/2500] Loss: 0.076695 | Time: 0.19s | Mean latent norm: 0.986685\n",
      "[Epoch 855/2500] Loss: 0.077245 | Time: 0.19s | Mean latent norm: 0.986676\n",
      "[Epoch 856/2500] Loss: 0.076859 | Time: 0.17s | Mean latent norm: 0.986677\n",
      "[Epoch 857/2500] Loss: 0.077373 | Time: 0.18s | Mean latent norm: 0.986673\n",
      "[Epoch 858/2500] Loss: 0.077019 | Time: 0.18s | Mean latent norm: 0.986664\n",
      "[Epoch 859/2500] Loss: 0.076589 | Time: 0.18s | Mean latent norm: 0.986664\n",
      "[Epoch 860/2500] Loss: 0.076859 | Time: 0.17s | Mean latent norm: 0.986656\n",
      "[Epoch 861/2500] Loss: 0.077153 | Time: 0.17s | Mean latent norm: 0.986652\n",
      "[Epoch 862/2500] Loss: 0.077981 | Time: 0.18s | Mean latent norm: 0.986634\n",
      "[Epoch 863/2500] Loss: 0.077728 | Time: 0.18s | Mean latent norm: 0.986628\n",
      "[Epoch 864/2500] Loss: 0.078268 | Time: 0.17s | Mean latent norm: 0.986630\n",
      "[Epoch 865/2500] Loss: 0.077690 | Time: 0.18s | Mean latent norm: 0.986614\n",
      "[Epoch 866/2500] Loss: 0.080914 | Time: 0.17s | Mean latent norm: 0.986594\n",
      "[Epoch 867/2500] Loss: 0.082258 | Time: 0.18s | Mean latent norm: 0.986582\n",
      "[Epoch 868/2500] Loss: 0.083023 | Time: 0.18s | Mean latent norm: 0.986554\n",
      "[Epoch 869/2500] Loss: 0.083342 | Time: 0.17s | Mean latent norm: 0.986524\n",
      "[Epoch 870/2500] Loss: 0.079641 | Time: 0.18s | Mean latent norm: 0.986496\n",
      "[Epoch 871/2500] Loss: 0.078312 | Time: 0.18s | Mean latent norm: 0.986492\n",
      "[Epoch 872/2500] Loss: 0.077460 | Time: 0.17s | Mean latent norm: 0.986468\n",
      "[Epoch 873/2500] Loss: 0.078085 | Time: 0.18s | Mean latent norm: 0.986464\n",
      "[Epoch 874/2500] Loss: 0.078606 | Time: 0.18s | Mean latent norm: 0.986448\n",
      "[Epoch 875/2500] Loss: 0.079588 | Time: 0.17s | Mean latent norm: 0.986446\n",
      "[Epoch 876/2500] Loss: 0.081004 | Time: 0.17s | Mean latent norm: 0.986423\n",
      "[Epoch 877/2500] Loss: 0.082032 | Time: 0.17s | Mean latent norm: 0.986403\n",
      "[Epoch 878/2500] Loss: 0.078957 | Time: 0.17s | Mean latent norm: 0.986391\n",
      "[Epoch 879/2500] Loss: 0.077070 | Time: 0.17s | Mean latent norm: 0.986379\n",
      "[Epoch 880/2500] Loss: 0.078123 | Time: 0.18s | Mean latent norm: 0.986363\n",
      "[Epoch 881/2500] Loss: 0.077275 | Time: 0.18s | Mean latent norm: 0.986363\n",
      "[Epoch 882/2500] Loss: 0.077424 | Time: 0.18s | Mean latent norm: 0.986354\n",
      "[Epoch 883/2500] Loss: 0.079351 | Time: 0.17s | Mean latent norm: 0.986336\n",
      "[Epoch 884/2500] Loss: 0.079567 | Time: 0.18s | Mean latent norm: 0.986332\n",
      "[Epoch 885/2500] Loss: 0.081454 | Time: 0.18s | Mean latent norm: 0.986308\n",
      "[Epoch 886/2500] Loss: 0.078300 | Time: 0.18s | Mean latent norm: 0.986287\n",
      "[Epoch 887/2500] Loss: 0.079624 | Time: 0.17s | Mean latent norm: 0.986255\n",
      "[Epoch 888/2500] Loss: 0.080767 | Time: 0.18s | Mean latent norm: 0.986247\n",
      "[Epoch 889/2500] Loss: 0.081118 | Time: 0.18s | Mean latent norm: 0.986209\n",
      "[Epoch 890/2500] Loss: 0.079004 | Time: 0.17s | Mean latent norm: 0.986194\n",
      "[Epoch 891/2500] Loss: 0.080241 | Time: 0.17s | Mean latent norm: 0.986182\n",
      "[Epoch 892/2500] Loss: 0.080181 | Time: 0.18s | Mean latent norm: 0.986171\n",
      "[Epoch 893/2500] Loss: 0.078073 | Time: 0.17s | Mean latent norm: 0.986159\n",
      "[Epoch 894/2500] Loss: 0.083793 | Time: 0.18s | Mean latent norm: 0.986164\n",
      "[Epoch 895/2500] Loss: 0.083649 | Time: 0.18s | Mean latent norm: 0.986140\n",
      "[Epoch 896/2500] Loss: 0.086124 | Time: 0.17s | Mean latent norm: 0.986099\n",
      "[Epoch 897/2500] Loss: 0.085005 | Time: 0.18s | Mean latent norm: 0.986067\n",
      "[Epoch 898/2500] Loss: 0.080459 | Time: 0.18s | Mean latent norm: 0.986048\n",
      "[Epoch 899/2500] Loss: 0.086088 | Time: 0.17s | Mean latent norm: 0.986031\n",
      "[Epoch 900/2500] Loss: 0.080397 | Time: 0.18s | Mean latent norm: 0.986005\n",
      "[Epoch 901/2500] Loss: 0.078649 | Time: 0.18s | Mean latent norm: 0.985994\n",
      "[Epoch 902/2500] Loss: 0.078810 | Time: 0.18s | Mean latent norm: 0.985985\n",
      "[Epoch 903/2500] Loss: 0.078683 | Time: 0.18s | Mean latent norm: 0.985969\n",
      "[Epoch 904/2500] Loss: 0.078245 | Time: 0.18s | Mean latent norm: 0.985961\n",
      "[Epoch 905/2500] Loss: 0.079044 | Time: 0.18s | Mean latent norm: 0.985957\n",
      "[Epoch 906/2500] Loss: 0.076535 | Time: 0.18s | Mean latent norm: 0.985950\n",
      "[Epoch 907/2500] Loss: 0.076870 | Time: 0.18s | Mean latent norm: 0.985943\n",
      "[Epoch 908/2500] Loss: 0.075836 | Time: 0.19s | Mean latent norm: 0.985933\n",
      "New best model saved at epoch 908 with loss 0.075836\n",
      "[Epoch 909/2500] Loss: 0.078164 | Time: 0.18s | Mean latent norm: 0.985925\n",
      "[Epoch 910/2500] Loss: 0.076680 | Time: 0.18s | Mean latent norm: 0.985925\n",
      "[Epoch 911/2500] Loss: 0.076252 | Time: 0.19s | Mean latent norm: 0.985921\n",
      "[Epoch 912/2500] Loss: 0.076809 | Time: 0.18s | Mean latent norm: 0.985906\n",
      "[Epoch 913/2500] Loss: 0.076824 | Time: 0.18s | Mean latent norm: 0.985896\n",
      "[Epoch 914/2500] Loss: 0.075589 | Time: 0.17s | Mean latent norm: 0.985886\n",
      "New best model saved at epoch 914 with loss 0.075589\n",
      "[Epoch 915/2500] Loss: 0.075183 | Time: 0.18s | Mean latent norm: 0.985891\n",
      "New best model saved at epoch 915 with loss 0.075183\n",
      "[Epoch 916/2500] Loss: 0.075128 | Time: 0.17s | Mean latent norm: 0.985890\n",
      "New best model saved at epoch 916 with loss 0.075128\n",
      "[Epoch 917/2500] Loss: 0.076296 | Time: 0.17s | Mean latent norm: 0.985884\n",
      "[Epoch 918/2500] Loss: 0.076277 | Time: 0.17s | Mean latent norm: 0.985878\n",
      "[Epoch 919/2500] Loss: 0.076978 | Time: 0.17s | Mean latent norm: 0.985872\n",
      "[Epoch 920/2500] Loss: 0.078935 | Time: 0.17s | Mean latent norm: 0.985847\n",
      "[Epoch 921/2500] Loss: 0.077443 | Time: 0.18s | Mean latent norm: 0.985838\n",
      "[Epoch 922/2500] Loss: 0.077490 | Time: 0.18s | Mean latent norm: 0.985839\n",
      "[Epoch 923/2500] Loss: 0.076914 | Time: 0.17s | Mean latent norm: 0.985818\n",
      "[Epoch 924/2500] Loss: 0.078061 | Time: 0.18s | Mean latent norm: 0.985798\n",
      "[Epoch 925/2500] Loss: 0.076555 | Time: 0.17s | Mean latent norm: 0.985793\n",
      "[Epoch 926/2500] Loss: 0.078467 | Time: 0.17s | Mean latent norm: 0.985777\n",
      "[Epoch 927/2500] Loss: 0.078290 | Time: 0.17s | Mean latent norm: 0.985741\n",
      "[Epoch 928/2500] Loss: 0.076763 | Time: 0.17s | Mean latent norm: 0.985723\n",
      "[Epoch 929/2500] Loss: 0.078227 | Time: 0.18s | Mean latent norm: 0.985708\n",
      "[Epoch 930/2500] Loss: 0.076715 | Time: 0.18s | Mean latent norm: 0.985688\n",
      "[Epoch 931/2500] Loss: 0.076222 | Time: 0.18s | Mean latent norm: 0.985670\n",
      "[Epoch 932/2500] Loss: 0.074912 | Time: 0.18s | Mean latent norm: 0.985664\n",
      "New best model saved at epoch 932 with loss 0.074912\n",
      "[Epoch 933/2500] Loss: 0.075205 | Time: 0.18s | Mean latent norm: 0.985663\n",
      "[Epoch 934/2500] Loss: 0.076119 | Time: 0.17s | Mean latent norm: 0.985656\n",
      "[Epoch 935/2500] Loss: 0.076449 | Time: 0.18s | Mean latent norm: 0.985651\n",
      "[Epoch 936/2500] Loss: 0.076858 | Time: 0.18s | Mean latent norm: 0.985642\n",
      "[Epoch 937/2500] Loss: 0.077363 | Time: 0.18s | Mean latent norm: 0.985629\n",
      "[Epoch 938/2500] Loss: 0.079050 | Time: 0.19s | Mean latent norm: 0.985622\n",
      "[Epoch 939/2500] Loss: 0.075907 | Time: 0.18s | Mean latent norm: 0.985601\n",
      "[Epoch 940/2500] Loss: 0.076065 | Time: 0.19s | Mean latent norm: 0.985588\n",
      "[Epoch 941/2500] Loss: 0.077619 | Time: 0.18s | Mean latent norm: 0.985581\n",
      "[Epoch 942/2500] Loss: 0.076453 | Time: 0.18s | Mean latent norm: 0.985570\n",
      "[Epoch 943/2500] Loss: 0.076839 | Time: 0.18s | Mean latent norm: 0.985568\n",
      "[Epoch 944/2500] Loss: 0.078074 | Time: 0.17s | Mean latent norm: 0.985562\n",
      "[Epoch 945/2500] Loss: 0.077283 | Time: 0.17s | Mean latent norm: 0.985554\n",
      "[Epoch 946/2500] Loss: 0.075623 | Time: 0.18s | Mean latent norm: 0.985532\n",
      "[Epoch 947/2500] Loss: 0.078235 | Time: 0.18s | Mean latent norm: 0.985516\n",
      "[Epoch 948/2500] Loss: 0.075599 | Time: 0.17s | Mean latent norm: 0.985508\n",
      "[Epoch 949/2500] Loss: 0.076701 | Time: 0.18s | Mean latent norm: 0.985492\n",
      "[Epoch 950/2500] Loss: 0.076558 | Time: 0.18s | Mean latent norm: 0.985464\n",
      "[Epoch 951/2500] Loss: 0.076058 | Time: 0.17s | Mean latent norm: 0.985462\n",
      "[Epoch 952/2500] Loss: 0.078162 | Time: 0.17s | Mean latent norm: 0.985438\n",
      "[Epoch 953/2500] Loss: 0.075717 | Time: 0.18s | Mean latent norm: 0.985413\n",
      "[Epoch 954/2500] Loss: 0.077753 | Time: 0.17s | Mean latent norm: 0.985402\n",
      "[Epoch 955/2500] Loss: 0.075449 | Time: 0.17s | Mean latent norm: 0.985381\n",
      "[Epoch 956/2500] Loss: 0.076514 | Time: 0.18s | Mean latent norm: 0.985388\n",
      "[Epoch 957/2500] Loss: 0.076216 | Time: 0.20s | Mean latent norm: 0.985366\n",
      "[Epoch 958/2500] Loss: 0.077518 | Time: 0.19s | Mean latent norm: 0.985343\n",
      "[Epoch 959/2500] Loss: 0.076984 | Time: 0.18s | Mean latent norm: 0.985339\n",
      "[Epoch 960/2500] Loss: 0.075598 | Time: 0.18s | Mean latent norm: 0.985331\n",
      "[Epoch 961/2500] Loss: 0.075365 | Time: 0.17s | Mean latent norm: 0.985322\n",
      "[Epoch 962/2500] Loss: 0.075698 | Time: 0.17s | Mean latent norm: 0.985316\n",
      "[Epoch 963/2500] Loss: 0.076357 | Time: 0.17s | Mean latent norm: 0.985295\n",
      "[Epoch 964/2500] Loss: 0.075773 | Time: 0.17s | Mean latent norm: 0.985272\n",
      "[Epoch 965/2500] Loss: 0.076610 | Time: 0.18s | Mean latent norm: 0.985260\n",
      "[Epoch 966/2500] Loss: 0.074618 | Time: 0.18s | Mean latent norm: 0.985262\n",
      "New best model saved at epoch 966 with loss 0.074618\n",
      "[Epoch 967/2500] Loss: 0.073999 | Time: 0.18s | Mean latent norm: 0.985248\n",
      "New best model saved at epoch 967 with loss 0.073999\n",
      "[Epoch 968/2500] Loss: 0.074718 | Time: 0.18s | Mean latent norm: 0.985242\n",
      "[Epoch 969/2500] Loss: 0.075888 | Time: 0.18s | Mean latent norm: 0.985238\n",
      "[Epoch 970/2500] Loss: 0.078130 | Time: 0.17s | Mean latent norm: 0.985232\n",
      "[Epoch 971/2500] Loss: 0.076004 | Time: 0.17s | Mean latent norm: 0.985213\n",
      "[Epoch 972/2500] Loss: 0.078083 | Time: 0.18s | Mean latent norm: 0.985191\n",
      "[Epoch 973/2500] Loss: 0.075429 | Time: 0.18s | Mean latent norm: 0.985174\n",
      "[Epoch 974/2500] Loss: 0.074542 | Time: 0.18s | Mean latent norm: 0.985178\n",
      "[Epoch 975/2500] Loss: 0.074105 | Time: 0.21s | Mean latent norm: 0.985162\n",
      "[Epoch 976/2500] Loss: 0.074868 | Time: 0.22s | Mean latent norm: 0.985154\n",
      "[Epoch 977/2500] Loss: 0.074543 | Time: 0.21s | Mean latent norm: 0.985148\n",
      "[Epoch 978/2500] Loss: 0.073715 | Time: 0.18s | Mean latent norm: 0.985154\n",
      "New best model saved at epoch 978 with loss 0.073715\n",
      "[Epoch 979/2500] Loss: 0.073399 | Time: 0.18s | Mean latent norm: 0.985139\n",
      "New best model saved at epoch 979 with loss 0.073399\n",
      "[Epoch 980/2500] Loss: 0.074504 | Time: 0.18s | Mean latent norm: 0.985135\n",
      "[Epoch 981/2500] Loss: 0.074899 | Time: 0.18s | Mean latent norm: 0.985131\n",
      "[Epoch 982/2500] Loss: 0.073677 | Time: 0.18s | Mean latent norm: 0.985125\n",
      "[Epoch 983/2500] Loss: 0.073203 | Time: 0.18s | Mean latent norm: 0.985119\n",
      "New best model saved at epoch 983 with loss 0.073203\n",
      "[Epoch 984/2500] Loss: 0.073996 | Time: 0.18s | Mean latent norm: 0.985106\n",
      "[Epoch 985/2500] Loss: 0.079650 | Time: 0.17s | Mean latent norm: 0.985096\n",
      "[Epoch 986/2500] Loss: 0.076571 | Time: 0.18s | Mean latent norm: 0.985083\n",
      "[Epoch 987/2500] Loss: 0.080496 | Time: 0.18s | Mean latent norm: 0.985047\n",
      "[Epoch 988/2500] Loss: 0.082245 | Time: 0.17s | Mean latent norm: 0.985021\n",
      "[Epoch 989/2500] Loss: 0.078758 | Time: 0.17s | Mean latent norm: 0.984990\n",
      "[Epoch 990/2500] Loss: 0.078464 | Time: 0.17s | Mean latent norm: 0.984981\n",
      "[Epoch 991/2500] Loss: 0.078542 | Time: 0.17s | Mean latent norm: 0.984963\n",
      "[Epoch 992/2500] Loss: 0.087325 | Time: 0.17s | Mean latent norm: 0.984938\n",
      "[Epoch 993/2500] Loss: 0.085404 | Time: 0.18s | Mean latent norm: 0.984890\n",
      "[Epoch 994/2500] Loss: 0.082743 | Time: 0.17s | Mean latent norm: 0.984850\n",
      "[Epoch 995/2500] Loss: 0.081441 | Time: 0.18s | Mean latent norm: 0.984817\n",
      "[Epoch 996/2500] Loss: 0.077002 | Time: 0.18s | Mean latent norm: 0.984794\n",
      "[Epoch 997/2500] Loss: 0.075146 | Time: 0.17s | Mean latent norm: 0.984777\n",
      "[Epoch 998/2500] Loss: 0.074851 | Time: 0.17s | Mean latent norm: 0.984774\n",
      "[Epoch 999/2500] Loss: 0.077690 | Time: 0.18s | Mean latent norm: 0.984763\n",
      "[Epoch 1000/2500] Loss: 0.075096 | Time: 0.18s | Mean latent norm: 0.984758\n",
      "[Epoch 1001/2500] Loss: 0.077095 | Time: 0.18s | Mean latent norm: 0.984746\n",
      "[Epoch 1002/2500] Loss: 0.075087 | Time: 0.18s | Mean latent norm: 0.984730\n",
      "[Epoch 1003/2500] Loss: 0.074665 | Time: 0.18s | Mean latent norm: 0.984716\n",
      "[Epoch 1004/2500] Loss: 0.074496 | Time: 0.18s | Mean latent norm: 0.984706\n",
      "[Epoch 1005/2500] Loss: 0.074733 | Time: 0.18s | Mean latent norm: 0.984709\n",
      "[Epoch 1006/2500] Loss: 0.074658 | Time: 0.18s | Mean latent norm: 0.984694\n",
      "[Epoch 1007/2500] Loss: 0.075484 | Time: 0.18s | Mean latent norm: 0.984690\n",
      "[Epoch 1008/2500] Loss: 0.075197 | Time: 0.18s | Mean latent norm: 0.984685\n",
      "[Epoch 1009/2500] Loss: 0.077998 | Time: 0.18s | Mean latent norm: 0.984677\n",
      "[Epoch 1010/2500] Loss: 0.076326 | Time: 0.17s | Mean latent norm: 0.984653\n",
      "[Epoch 1011/2500] Loss: 0.077296 | Time: 0.18s | Mean latent norm: 0.984636\n",
      "[Epoch 1012/2500] Loss: 0.075294 | Time: 0.18s | Mean latent norm: 0.984618\n",
      "[Epoch 1013/2500] Loss: 0.075308 | Time: 0.18s | Mean latent norm: 0.984607\n",
      "[Epoch 1014/2500] Loss: 0.073101 | Time: 0.18s | Mean latent norm: 0.984595\n",
      "New best model saved at epoch 1014 with loss 0.073101\n",
      "[Epoch 1015/2500] Loss: 0.073159 | Time: 0.18s | Mean latent norm: 0.984606\n",
      "[Epoch 1016/2500] Loss: 0.073425 | Time: 0.18s | Mean latent norm: 0.984585\n",
      "[Epoch 1017/2500] Loss: 0.072672 | Time: 0.18s | Mean latent norm: 0.984580\n",
      "New best model saved at epoch 1017 with loss 0.072672\n",
      "[Epoch 1018/2500] Loss: 0.073009 | Time: 0.18s | Mean latent norm: 0.984580\n",
      "[Epoch 1019/2500] Loss: 0.073536 | Time: 0.19s | Mean latent norm: 0.984578\n",
      "[Epoch 1020/2500] Loss: 0.072711 | Time: 0.19s | Mean latent norm: 0.984564\n",
      "[Epoch 1021/2500] Loss: 0.073656 | Time: 0.18s | Mean latent norm: 0.984566\n",
      "[Epoch 1022/2500] Loss: 0.074561 | Time: 0.19s | Mean latent norm: 0.984556\n",
      "[Epoch 1023/2500] Loss: 0.074966 | Time: 0.18s | Mean latent norm: 0.984541\n",
      "[Epoch 1024/2500] Loss: 0.077135 | Time: 0.18s | Mean latent norm: 0.984527\n",
      "[Epoch 1025/2500] Loss: 0.075312 | Time: 0.18s | Mean latent norm: 0.984517\n",
      "[Epoch 1026/2500] Loss: 0.074357 | Time: 0.18s | Mean latent norm: 0.984496\n",
      "[Epoch 1027/2500] Loss: 0.073571 | Time: 0.18s | Mean latent norm: 0.984480\n",
      "[Epoch 1028/2500] Loss: 0.072568 | Time: 0.18s | Mean latent norm: 0.984479\n",
      "New best model saved at epoch 1028 with loss 0.072568\n",
      "[Epoch 1029/2500] Loss: 0.073681 | Time: 0.17s | Mean latent norm: 0.984473\n",
      "[Epoch 1030/2500] Loss: 0.074405 | Time: 0.18s | Mean latent norm: 0.984475\n",
      "[Epoch 1031/2500] Loss: 0.073885 | Time: 0.18s | Mean latent norm: 0.984472\n",
      "[Epoch 1032/2500] Loss: 0.075745 | Time: 0.18s | Mean latent norm: 0.984446\n",
      "[Epoch 1033/2500] Loss: 0.072529 | Time: 0.17s | Mean latent norm: 0.984426\n",
      "New best model saved at epoch 1033 with loss 0.072529\n",
      "[Epoch 1034/2500] Loss: 0.074606 | Time: 0.18s | Mean latent norm: 0.984415\n",
      "[Epoch 1035/2500] Loss: 0.073365 | Time: 0.18s | Mean latent norm: 0.984411\n",
      "[Epoch 1036/2500] Loss: 0.071616 | Time: 0.18s | Mean latent norm: 0.984400\n",
      "New best model saved at epoch 1036 with loss 0.071616\n",
      "[Epoch 1037/2500] Loss: 0.072637 | Time: 0.18s | Mean latent norm: 0.984397\n",
      "[Epoch 1038/2500] Loss: 0.072288 | Time: 0.18s | Mean latent norm: 0.984394\n",
      "[Epoch 1039/2500] Loss: 0.071736 | Time: 0.17s | Mean latent norm: 0.984398\n",
      "[Epoch 1040/2500] Loss: 0.071578 | Time: 0.18s | Mean latent norm: 0.984398\n",
      "New best model saved at epoch 1040 with loss 0.071578\n",
      "[Epoch 1041/2500] Loss: 0.071588 | Time: 0.18s | Mean latent norm: 0.984392\n",
      "[Epoch 1042/2500] Loss: 0.072029 | Time: 0.18s | Mean latent norm: 0.984388\n",
      "[Epoch 1043/2500] Loss: 0.073038 | Time: 0.17s | Mean latent norm: 0.984388\n",
      "[Epoch 1044/2500] Loss: 0.073974 | Time: 0.18s | Mean latent norm: 0.984376\n",
      "[Epoch 1045/2500] Loss: 0.073330 | Time: 0.17s | Mean latent norm: 0.984360\n",
      "[Epoch 1046/2500] Loss: 0.076758 | Time: 0.17s | Mean latent norm: 0.984340\n",
      "[Epoch 1047/2500] Loss: 0.078357 | Time: 0.17s | Mean latent norm: 0.984335\n",
      "[Epoch 1048/2500] Loss: 0.076288 | Time: 0.18s | Mean latent norm: 0.984301\n",
      "[Epoch 1049/2500] Loss: 0.073582 | Time: 0.17s | Mean latent norm: 0.984269\n",
      "[Epoch 1050/2500] Loss: 0.074444 | Time: 0.18s | Mean latent norm: 0.984252\n",
      "[Epoch 1051/2500] Loss: 0.074200 | Time: 0.18s | Mean latent norm: 0.984252\n",
      "[Epoch 1052/2500] Loss: 0.072190 | Time: 0.17s | Mean latent norm: 0.984242\n",
      "[Epoch 1053/2500] Loss: 0.071434 | Time: 0.18s | Mean latent norm: 0.984242\n",
      "New best model saved at epoch 1053 with loss 0.071434\n",
      "[Epoch 1054/2500] Loss: 0.071290 | Time: 0.18s | Mean latent norm: 0.984244\n",
      "New best model saved at epoch 1054 with loss 0.071290\n",
      "[Epoch 1055/2500] Loss: 0.071060 | Time: 0.18s | Mean latent norm: 0.984245\n",
      "New best model saved at epoch 1055 with loss 0.071060\n",
      "[Epoch 1056/2500] Loss: 0.071469 | Time: 0.18s | Mean latent norm: 0.984237\n",
      "[Epoch 1057/2500] Loss: 0.071898 | Time: 0.17s | Mean latent norm: 0.984234\n",
      "[Epoch 1058/2500] Loss: 0.071329 | Time: 0.17s | Mean latent norm: 0.984232\n",
      "[Epoch 1059/2500] Loss: 0.071674 | Time: 0.17s | Mean latent norm: 0.984212\n",
      "[Epoch 1060/2500] Loss: 0.071516 | Time: 0.17s | Mean latent norm: 0.984217\n",
      "[Epoch 1061/2500] Loss: 0.072119 | Time: 0.18s | Mean latent norm: 0.984216\n",
      "[Epoch 1062/2500] Loss: 0.073573 | Time: 0.17s | Mean latent norm: 0.984213\n",
      "[Epoch 1063/2500] Loss: 0.072958 | Time: 0.18s | Mean latent norm: 0.984197\n",
      "[Epoch 1064/2500] Loss: 0.072604 | Time: 0.17s | Mean latent norm: 0.984194\n",
      "[Epoch 1065/2500] Loss: 0.072721 | Time: 0.17s | Mean latent norm: 0.984183\n",
      "[Epoch 1066/2500] Loss: 0.073314 | Time: 0.18s | Mean latent norm: 0.984175\n",
      "[Epoch 1067/2500] Loss: 0.074522 | Time: 0.17s | Mean latent norm: 0.984153\n",
      "[Epoch 1068/2500] Loss: 0.074536 | Time: 0.17s | Mean latent norm: 0.984144\n",
      "[Epoch 1069/2500] Loss: 0.073280 | Time: 0.18s | Mean latent norm: 0.984130\n",
      "[Epoch 1070/2500] Loss: 0.074465 | Time: 0.17s | Mean latent norm: 0.984129\n",
      "[Epoch 1071/2500] Loss: 0.078677 | Time: 0.17s | Mean latent norm: 0.984111\n",
      "[Epoch 1072/2500] Loss: 0.076230 | Time: 0.18s | Mean latent norm: 0.984086\n",
      "[Epoch 1073/2500] Loss: 0.085652 | Time: 0.17s | Mean latent norm: 0.984047\n",
      "[Epoch 1074/2500] Loss: 0.080133 | Time: 0.17s | Mean latent norm: 0.984021\n",
      "[Epoch 1075/2500] Loss: 0.079402 | Time: 0.17s | Mean latent norm: 0.984007\n",
      "[Epoch 1076/2500] Loss: 0.078334 | Time: 0.18s | Mean latent norm: 0.983993\n",
      "[Epoch 1077/2500] Loss: 0.077129 | Time: 0.18s | Mean latent norm: 0.983981\n",
      "[Epoch 1078/2500] Loss: 0.076909 | Time: 0.18s | Mean latent norm: 0.983949\n",
      "[Epoch 1079/2500] Loss: 0.080129 | Time: 0.17s | Mean latent norm: 0.983925\n",
      "[Epoch 1080/2500] Loss: 0.078517 | Time: 0.18s | Mean latent norm: 0.983902\n",
      "[Epoch 1081/2500] Loss: 0.078493 | Time: 0.18s | Mean latent norm: 0.983882\n",
      "[Epoch 1082/2500] Loss: 0.077310 | Time: 0.18s | Mean latent norm: 0.983850\n",
      "[Epoch 1083/2500] Loss: 0.077913 | Time: 0.18s | Mean latent norm: 0.983837\n",
      "[Epoch 1084/2500] Loss: 0.080001 | Time: 0.17s | Mean latent norm: 0.983835\n",
      "[Epoch 1085/2500] Loss: 0.082451 | Time: 0.18s | Mean latent norm: 0.983810\n",
      "[Epoch 1086/2500] Loss: 0.083400 | Time: 0.17s | Mean latent norm: 0.983782\n",
      "[Epoch 1087/2500] Loss: 0.075591 | Time: 0.17s | Mean latent norm: 0.983744\n",
      "[Epoch 1088/2500] Loss: 0.075606 | Time: 0.18s | Mean latent norm: 0.983720\n",
      "[Epoch 1089/2500] Loss: 0.072297 | Time: 0.20s | Mean latent norm: 0.983700\n",
      "[Epoch 1090/2500] Loss: 0.072820 | Time: 0.19s | Mean latent norm: 0.983698\n",
      "[Epoch 1091/2500] Loss: 0.072650 | Time: 0.18s | Mean latent norm: 0.983696\n",
      "[Epoch 1092/2500] Loss: 0.072068 | Time: 0.18s | Mean latent norm: 0.983691\n",
      "[Epoch 1093/2500] Loss: 0.072423 | Time: 0.18s | Mean latent norm: 0.983680\n",
      "[Epoch 1094/2500] Loss: 0.073395 | Time: 0.18s | Mean latent norm: 0.983681\n",
      "[Epoch 1095/2500] Loss: 0.072327 | Time: 0.18s | Mean latent norm: 0.983670\n",
      "[Epoch 1096/2500] Loss: 0.074958 | Time: 0.19s | Mean latent norm: 0.983652\n",
      "[Epoch 1097/2500] Loss: 0.075431 | Time: 0.18s | Mean latent norm: 0.983627\n",
      "[Epoch 1098/2500] Loss: 0.074446 | Time: 0.17s | Mean latent norm: 0.983619\n",
      "[Epoch 1099/2500] Loss: 0.071642 | Time: 0.17s | Mean latent norm: 0.983608\n",
      "[Epoch 1100/2500] Loss: 0.071923 | Time: 0.19s | Mean latent norm: 0.983591\n",
      "[Epoch 1101/2500] Loss: 0.071477 | Time: 0.18s | Mean latent norm: 0.983584\n",
      "[Epoch 1102/2500] Loss: 0.072219 | Time: 0.18s | Mean latent norm: 0.983582\n",
      "[Epoch 1103/2500] Loss: 0.071020 | Time: 0.19s | Mean latent norm: 0.983581\n",
      "New best model saved at epoch 1103 with loss 0.071020\n",
      "[Epoch 1104/2500] Loss: 0.071178 | Time: 0.20s | Mean latent norm: 0.983570\n",
      "[Epoch 1105/2500] Loss: 0.071277 | Time: 0.19s | Mean latent norm: 0.983558\n",
      "[Epoch 1106/2500] Loss: 0.071855 | Time: 0.20s | Mean latent norm: 0.983564\n",
      "[Epoch 1107/2500] Loss: 0.071384 | Time: 0.20s | Mean latent norm: 0.983544\n",
      "[Epoch 1108/2500] Loss: 0.070481 | Time: 0.19s | Mean latent norm: 0.983533\n",
      "New best model saved at epoch 1108 with loss 0.070481\n",
      "[Epoch 1109/2500] Loss: 0.070366 | Time: 0.19s | Mean latent norm: 0.983535\n",
      "New best model saved at epoch 1109 with loss 0.070366\n",
      "[Epoch 1110/2500] Loss: 0.070005 | Time: 0.19s | Mean latent norm: 0.983541\n",
      "New best model saved at epoch 1110 with loss 0.070005\n",
      "[Epoch 1111/2500] Loss: 0.070452 | Time: 0.19s | Mean latent norm: 0.983529\n",
      "[Epoch 1112/2500] Loss: 0.070100 | Time: 0.19s | Mean latent norm: 0.983527\n",
      "[Epoch 1113/2500] Loss: 0.071520 | Time: 0.19s | Mean latent norm: 0.983524\n",
      "[Epoch 1114/2500] Loss: 0.071390 | Time: 0.20s | Mean latent norm: 0.983509\n",
      "[Epoch 1115/2500] Loss: 0.072797 | Time: 0.19s | Mean latent norm: 0.983495\n",
      "[Epoch 1116/2500] Loss: 0.071790 | Time: 0.19s | Mean latent norm: 0.983501\n",
      "[Epoch 1117/2500] Loss: 0.071886 | Time: 0.19s | Mean latent norm: 0.983481\n",
      "[Epoch 1118/2500] Loss: 0.071720 | Time: 0.19s | Mean latent norm: 0.983465\n",
      "[Epoch 1119/2500] Loss: 0.072196 | Time: 0.20s | Mean latent norm: 0.983455\n",
      "[Epoch 1120/2500] Loss: 0.071982 | Time: 0.20s | Mean latent norm: 0.983454\n",
      "[Epoch 1121/2500] Loss: 0.072971 | Time: 0.19s | Mean latent norm: 0.983440\n",
      "[Epoch 1122/2500] Loss: 0.073151 | Time: 0.19s | Mean latent norm: 0.983421\n",
      "[Epoch 1123/2500] Loss: 0.073246 | Time: 0.19s | Mean latent norm: 0.983398\n",
      "[Epoch 1124/2500] Loss: 0.074975 | Time: 0.19s | Mean latent norm: 0.983408\n",
      "[Epoch 1125/2500] Loss: 0.072496 | Time: 0.19s | Mean latent norm: 0.983385\n",
      "[Epoch 1126/2500] Loss: 0.071586 | Time: 0.19s | Mean latent norm: 0.983362\n",
      "[Epoch 1127/2500] Loss: 0.071473 | Time: 0.19s | Mean latent norm: 0.983350\n",
      "[Epoch 1128/2500] Loss: 0.073301 | Time: 0.19s | Mean latent norm: 0.983347\n",
      "[Epoch 1129/2500] Loss: 0.072200 | Time: 0.19s | Mean latent norm: 0.983329\n",
      "[Epoch 1130/2500] Loss: 0.071081 | Time: 0.19s | Mean latent norm: 0.983318\n",
      "[Epoch 1131/2500] Loss: 0.073515 | Time: 0.20s | Mean latent norm: 0.983306\n",
      "[Epoch 1132/2500] Loss: 0.073936 | Time: 0.20s | Mean latent norm: 0.983288\n",
      "[Epoch 1133/2500] Loss: 0.072660 | Time: 0.19s | Mean latent norm: 0.983262\n",
      "[Epoch 1134/2500] Loss: 0.074306 | Time: 0.19s | Mean latent norm: 0.983266\n",
      "[Epoch 1135/2500] Loss: 0.077171 | Time: 0.19s | Mean latent norm: 0.983247\n",
      "[Epoch 1136/2500] Loss: 0.074053 | Time: 0.19s | Mean latent norm: 0.983221\n",
      "[Epoch 1137/2500] Loss: 0.076170 | Time: 0.19s | Mean latent norm: 0.983188\n",
      "[Epoch 1138/2500] Loss: 0.078990 | Time: 0.19s | Mean latent norm: 0.983158\n",
      "[Epoch 1139/2500] Loss: 0.080110 | Time: 0.19s | Mean latent norm: 0.983130\n",
      "[Epoch 1140/2500] Loss: 0.078734 | Time: 0.19s | Mean latent norm: 0.983098\n",
      "[Epoch 1141/2500] Loss: 0.079245 | Time: 0.19s | Mean latent norm: 0.983089\n",
      "[Epoch 1142/2500] Loss: 0.082239 | Time: 0.19s | Mean latent norm: 0.983065\n",
      "[Epoch 1143/2500] Loss: 0.080494 | Time: 0.19s | Mean latent norm: 0.983044\n",
      "[Epoch 1144/2500] Loss: 0.082348 | Time: 0.19s | Mean latent norm: 0.983021\n",
      "[Epoch 1145/2500] Loss: 0.080243 | Time: 0.19s | Mean latent norm: 0.982980\n",
      "[Epoch 1146/2500] Loss: 0.074228 | Time: 0.19s | Mean latent norm: 0.982954\n",
      "[Epoch 1147/2500] Loss: 0.073539 | Time: 0.19s | Mean latent norm: 0.982941\n",
      "[Epoch 1148/2500] Loss: 0.071598 | Time: 0.19s | Mean latent norm: 0.982931\n",
      "[Epoch 1149/2500] Loss: 0.070434 | Time: 0.19s | Mean latent norm: 0.982935\n",
      "[Epoch 1150/2500] Loss: 0.070304 | Time: 0.19s | Mean latent norm: 0.982945\n",
      "[Epoch 1151/2500] Loss: 0.070454 | Time: 0.19s | Mean latent norm: 0.982934\n",
      "[Epoch 1152/2500] Loss: 0.070474 | Time: 0.19s | Mean latent norm: 0.982921\n",
      "[Epoch 1153/2500] Loss: 0.069791 | Time: 0.19s | Mean latent norm: 0.982916\n",
      "New best model saved at epoch 1153 with loss 0.069791\n",
      "[Epoch 1154/2500] Loss: 0.069780 | Time: 0.19s | Mean latent norm: 0.982928\n",
      "New best model saved at epoch 1154 with loss 0.069780\n",
      "[Epoch 1155/2500] Loss: 0.070217 | Time: 0.19s | Mean latent norm: 0.982922\n",
      "[Epoch 1156/2500] Loss: 0.069832 | Time: 0.19s | Mean latent norm: 0.982911\n",
      "[Epoch 1157/2500] Loss: 0.069594 | Time: 0.19s | Mean latent norm: 0.982913\n",
      "New best model saved at epoch 1157 with loss 0.069594\n",
      "[Epoch 1158/2500] Loss: 0.069200 | Time: 0.19s | Mean latent norm: 0.982905\n",
      "New best model saved at epoch 1158 with loss 0.069200\n",
      "[Epoch 1159/2500] Loss: 0.069980 | Time: 0.19s | Mean latent norm: 0.982908\n",
      "[Epoch 1160/2500] Loss: 0.070602 | Time: 0.19s | Mean latent norm: 0.982910\n",
      "[Epoch 1161/2500] Loss: 0.070899 | Time: 0.19s | Mean latent norm: 0.982892\n",
      "[Epoch 1162/2500] Loss: 0.070629 | Time: 0.19s | Mean latent norm: 0.982881\n",
      "[Epoch 1163/2500] Loss: 0.070593 | Time: 0.19s | Mean latent norm: 0.982873\n",
      "[Epoch 1164/2500] Loss: 0.070666 | Time: 0.19s | Mean latent norm: 0.982870\n",
      "[Epoch 1165/2500] Loss: 0.072797 | Time: 0.19s | Mean latent norm: 0.982865\n",
      "[Epoch 1166/2500] Loss: 0.077520 | Time: 0.19s | Mean latent norm: 0.982845\n",
      "[Epoch 1167/2500] Loss: 0.074054 | Time: 0.17s | Mean latent norm: 0.982827\n",
      "[Epoch 1168/2500] Loss: 0.076672 | Time: 0.17s | Mean latent norm: 0.982818\n",
      "[Epoch 1169/2500] Loss: 0.077337 | Time: 0.17s | Mean latent norm: 0.982811\n",
      "[Epoch 1170/2500] Loss: 0.079472 | Time: 0.17s | Mean latent norm: 0.982787\n",
      "[Epoch 1171/2500] Loss: 0.086821 | Time: 0.17s | Mean latent norm: 0.982750\n",
      "[Epoch 1172/2500] Loss: 0.077281 | Time: 0.17s | Mean latent norm: 0.982724\n",
      "[Epoch 1173/2500] Loss: 0.073576 | Time: 0.17s | Mean latent norm: 0.982700\n",
      "[Epoch 1174/2500] Loss: 0.071716 | Time: 0.17s | Mean latent norm: 0.982680\n",
      "[Epoch 1175/2500] Loss: 0.070219 | Time: 0.17s | Mean latent norm: 0.982678\n",
      "[Epoch 1176/2500] Loss: 0.069895 | Time: 0.17s | Mean latent norm: 0.982669\n",
      "[Epoch 1177/2500] Loss: 0.069906 | Time: 0.17s | Mean latent norm: 0.982673\n",
      "[Epoch 1178/2500] Loss: 0.070428 | Time: 0.17s | Mean latent norm: 0.982659\n",
      "[Epoch 1179/2500] Loss: 0.069424 | Time: 0.17s | Mean latent norm: 0.982650\n",
      "[Epoch 1180/2500] Loss: 0.069494 | Time: 0.17s | Mean latent norm: 0.982646\n",
      "[Epoch 1181/2500] Loss: 0.069203 | Time: 0.17s | Mean latent norm: 0.982659\n",
      "[Epoch 1182/2500] Loss: 0.070172 | Time: 0.17s | Mean latent norm: 0.982635\n",
      "[Epoch 1183/2500] Loss: 0.069743 | Time: 0.17s | Mean latent norm: 0.982628\n",
      "[Epoch 1184/2500] Loss: 0.069078 | Time: 0.17s | Mean latent norm: 0.982621\n",
      "New best model saved at epoch 1184 with loss 0.069078\n",
      "[Epoch 1185/2500] Loss: 0.069624 | Time: 0.17s | Mean latent norm: 0.982612\n",
      "[Epoch 1186/2500] Loss: 0.069137 | Time: 0.17s | Mean latent norm: 0.982598\n",
      "[Epoch 1187/2500] Loss: 0.069128 | Time: 0.17s | Mean latent norm: 0.982602\n",
      "[Epoch 1188/2500] Loss: 0.068849 | Time: 0.17s | Mean latent norm: 0.982598\n",
      "New best model saved at epoch 1188 with loss 0.068849\n",
      "[Epoch 1189/2500] Loss: 0.068861 | Time: 0.19s | Mean latent norm: 0.982590\n",
      "[Epoch 1190/2500] Loss: 0.068940 | Time: 0.20s | Mean latent norm: 0.982592\n",
      "[Epoch 1191/2500] Loss: 0.069141 | Time: 0.19s | Mean latent norm: 0.982583\n",
      "[Epoch 1192/2500] Loss: 0.070120 | Time: 0.18s | Mean latent norm: 0.982581\n",
      "[Epoch 1193/2500] Loss: 0.069391 | Time: 0.20s | Mean latent norm: 0.982567\n",
      "[Epoch 1194/2500] Loss: 0.071852 | Time: 0.20s | Mean latent norm: 0.982565\n",
      "[Epoch 1195/2500] Loss: 0.071676 | Time: 0.19s | Mean latent norm: 0.982557\n",
      "[Epoch 1196/2500] Loss: 0.070959 | Time: 0.19s | Mean latent norm: 0.982543\n",
      "[Epoch 1197/2500] Loss: 0.072531 | Time: 0.17s | Mean latent norm: 0.982522\n",
      "[Epoch 1198/2500] Loss: 0.072594 | Time: 0.17s | Mean latent norm: 0.982504\n",
      "[Epoch 1199/2500] Loss: 0.072618 | Time: 0.19s | Mean latent norm: 0.982484\n",
      "[Epoch 1200/2500] Loss: 0.071168 | Time: 0.19s | Mean latent norm: 0.982477\n",
      "[Epoch 1201/2500] Loss: 0.071122 | Time: 0.20s | Mean latent norm: 0.982456\n",
      "[Epoch 1202/2500] Loss: 0.071364 | Time: 0.19s | Mean latent norm: 0.982444\n",
      "[Epoch 1203/2500] Loss: 0.072150 | Time: 0.19s | Mean latent norm: 0.982442\n",
      "[Epoch 1204/2500] Loss: 0.073903 | Time: 0.19s | Mean latent norm: 0.982433\n",
      "[Epoch 1205/2500] Loss: 0.072509 | Time: 0.19s | Mean latent norm: 0.982384\n",
      "[Epoch 1206/2500] Loss: 0.071447 | Time: 0.19s | Mean latent norm: 0.982372\n",
      "[Epoch 1207/2500] Loss: 0.070640 | Time: 0.19s | Mean latent norm: 0.982363\n",
      "[Epoch 1208/2500] Loss: 0.069856 | Time: 0.19s | Mean latent norm: 0.982353\n",
      "[Epoch 1209/2500] Loss: 0.070142 | Time: 0.19s | Mean latent norm: 0.982339\n",
      "[Epoch 1210/2500] Loss: 0.069425 | Time: 0.19s | Mean latent norm: 0.982340\n",
      "[Epoch 1211/2500] Loss: 0.070726 | Time: 0.19s | Mean latent norm: 0.982336\n",
      "[Epoch 1212/2500] Loss: 0.069156 | Time: 0.19s | Mean latent norm: 0.982323\n",
      "[Epoch 1213/2500] Loss: 0.069862 | Time: 0.19s | Mean latent norm: 0.982306\n",
      "[Epoch 1214/2500] Loss: 0.069620 | Time: 0.19s | Mean latent norm: 0.982310\n",
      "[Epoch 1215/2500] Loss: 0.069121 | Time: 0.19s | Mean latent norm: 0.982303\n",
      "[Epoch 1216/2500] Loss: 0.072309 | Time: 0.19s | Mean latent norm: 0.982290\n",
      "[Epoch 1217/2500] Loss: 0.073252 | Time: 0.19s | Mean latent norm: 0.982273\n",
      "[Epoch 1218/2500] Loss: 0.072701 | Time: 0.19s | Mean latent norm: 0.982261\n",
      "[Epoch 1219/2500] Loss: 0.070666 | Time: 0.19s | Mean latent norm: 0.982246\n",
      "[Epoch 1220/2500] Loss: 0.071383 | Time: 0.19s | Mean latent norm: 0.982237\n",
      "[Epoch 1221/2500] Loss: 0.069085 | Time: 0.19s | Mean latent norm: 0.982236\n",
      "[Epoch 1222/2500] Loss: 0.071520 | Time: 0.19s | Mean latent norm: 0.982205\n",
      "[Epoch 1223/2500] Loss: 0.070290 | Time: 0.19s | Mean latent norm: 0.982193\n",
      "[Epoch 1224/2500] Loss: 0.072177 | Time: 0.19s | Mean latent norm: 0.982187\n",
      "[Epoch 1225/2500] Loss: 0.070842 | Time: 0.20s | Mean latent norm: 0.982167\n",
      "[Epoch 1226/2500] Loss: 0.069093 | Time: 0.20s | Mean latent norm: 0.982154\n",
      "[Epoch 1227/2500] Loss: 0.069639 | Time: 0.19s | Mean latent norm: 0.982147\n",
      "[Epoch 1228/2500] Loss: 0.071242 | Time: 0.19s | Mean latent norm: 0.982141\n",
      "[Epoch 1229/2500] Loss: 0.075097 | Time: 0.19s | Mean latent norm: 0.982125\n",
      "[Epoch 1230/2500] Loss: 0.073363 | Time: 0.19s | Mean latent norm: 0.982110\n",
      "[Epoch 1231/2500] Loss: 0.074117 | Time: 0.19s | Mean latent norm: 0.982083\n",
      "[Epoch 1232/2500] Loss: 0.072612 | Time: 0.19s | Mean latent norm: 0.982062\n",
      "[Epoch 1233/2500] Loss: 0.071716 | Time: 0.19s | Mean latent norm: 0.982053\n",
      "[Epoch 1234/2500] Loss: 0.071871 | Time: 0.19s | Mean latent norm: 0.982046\n",
      "[Epoch 1235/2500] Loss: 0.071750 | Time: 0.19s | Mean latent norm: 0.982038\n",
      "[Epoch 1236/2500] Loss: 0.070808 | Time: 0.19s | Mean latent norm: 0.982011\n",
      "[Epoch 1237/2500] Loss: 0.070540 | Time: 0.19s | Mean latent norm: 0.982014\n",
      "[Epoch 1238/2500] Loss: 0.072351 | Time: 0.19s | Mean latent norm: 0.982017\n",
      "[Epoch 1239/2500] Loss: 0.076592 | Time: 0.19s | Mean latent norm: 0.982005\n",
      "[Epoch 1240/2500] Loss: 0.071121 | Time: 0.19s | Mean latent norm: 0.981961\n",
      "[Epoch 1241/2500] Loss: 0.073291 | Time: 0.19s | Mean latent norm: 0.981927\n",
      "[Epoch 1242/2500] Loss: 0.074611 | Time: 0.19s | Mean latent norm: 0.981919\n",
      "[Epoch 1243/2500] Loss: 0.069902 | Time: 0.19s | Mean latent norm: 0.981914\n",
      "[Epoch 1244/2500] Loss: 0.069713 | Time: 0.19s | Mean latent norm: 0.981892\n",
      "[Epoch 1245/2500] Loss: 0.069095 | Time: 0.19s | Mean latent norm: 0.981877\n",
      "[Epoch 1246/2500] Loss: 0.070301 | Time: 0.19s | Mean latent norm: 0.981883\n",
      "[Epoch 1247/2500] Loss: 0.069357 | Time: 0.19s | Mean latent norm: 0.981868\n",
      "[Epoch 1248/2500] Loss: 0.072338 | Time: 0.19s | Mean latent norm: 0.981858\n",
      "[Epoch 1249/2500] Loss: 0.071000 | Time: 0.19s | Mean latent norm: 0.981824\n",
      "[Epoch 1250/2500] Loss: 0.069892 | Time: 0.19s | Mean latent norm: 0.981806\n",
      "[Epoch 1251/2500] Loss: 0.071132 | Time: 0.19s | Mean latent norm: 0.981802\n",
      "[Epoch 1252/2500] Loss: 0.070684 | Time: 0.19s | Mean latent norm: 0.981807\n",
      "[Epoch 1253/2500] Loss: 0.073219 | Time: 0.19s | Mean latent norm: 0.981780\n",
      "[Epoch 1254/2500] Loss: 0.072087 | Time: 0.19s | Mean latent norm: 0.981742\n",
      "[Epoch 1255/2500] Loss: 0.072150 | Time: 0.19s | Mean latent norm: 0.981732\n",
      "[Epoch 1256/2500] Loss: 0.070728 | Time: 0.19s | Mean latent norm: 0.981710\n",
      "[Epoch 1257/2500] Loss: 0.068492 | Time: 0.19s | Mean latent norm: 0.981700\n",
      "New best model saved at epoch 1257 with loss 0.068492\n",
      "[Epoch 1258/2500] Loss: 0.068608 | Time: 0.19s | Mean latent norm: 0.981685\n",
      "[Epoch 1259/2500] Loss: 0.068697 | Time: 0.19s | Mean latent norm: 0.981687\n",
      "[Epoch 1260/2500] Loss: 0.069057 | Time: 0.19s | Mean latent norm: 0.981688\n",
      "[Epoch 1261/2500] Loss: 0.069103 | Time: 0.19s | Mean latent norm: 0.981678\n",
      "[Epoch 1262/2500] Loss: 0.069262 | Time: 0.19s | Mean latent norm: 0.981665\n",
      "[Epoch 1263/2500] Loss: 0.069857 | Time: 0.19s | Mean latent norm: 0.981661\n",
      "[Epoch 1264/2500] Loss: 0.068033 | Time: 0.20s | Mean latent norm: 0.981657\n",
      "New best model saved at epoch 1264 with loss 0.068033\n",
      "[Epoch 1265/2500] Loss: 0.067856 | Time: 0.19s | Mean latent norm: 0.981644\n",
      "New best model saved at epoch 1265 with loss 0.067856\n",
      "[Epoch 1266/2500] Loss: 0.067679 | Time: 0.19s | Mean latent norm: 0.981643\n",
      "New best model saved at epoch 1266 with loss 0.067679\n",
      "[Epoch 1267/2500] Loss: 0.068109 | Time: 0.20s | Mean latent norm: 0.981647\n",
      "[Epoch 1268/2500] Loss: 0.067977 | Time: 0.20s | Mean latent norm: 0.981630\n",
      "[Epoch 1269/2500] Loss: 0.067969 | Time: 0.19s | Mean latent norm: 0.981622\n",
      "[Epoch 1270/2500] Loss: 0.066982 | Time: 0.19s | Mean latent norm: 0.981620\n",
      "New best model saved at epoch 1270 with loss 0.066982\n",
      "[Epoch 1271/2500] Loss: 0.067430 | Time: 0.19s | Mean latent norm: 0.981616\n",
      "[Epoch 1272/2500] Loss: 0.066644 | Time: 0.19s | Mean latent norm: 0.981610\n",
      "New best model saved at epoch 1272 with loss 0.066644\n",
      "[Epoch 1273/2500] Loss: 0.068019 | Time: 0.20s | Mean latent norm: 0.981614\n",
      "[Epoch 1274/2500] Loss: 0.067137 | Time: 0.19s | Mean latent norm: 0.981611\n",
      "[Epoch 1275/2500] Loss: 0.067256 | Time: 0.19s | Mean latent norm: 0.981598\n",
      "[Epoch 1276/2500] Loss: 0.067534 | Time: 0.19s | Mean latent norm: 0.981583\n",
      "[Epoch 1277/2500] Loss: 0.068385 | Time: 0.19s | Mean latent norm: 0.981601\n",
      "[Epoch 1278/2500] Loss: 0.068917 | Time: 0.20s | Mean latent norm: 0.981590\n",
      "[Epoch 1279/2500] Loss: 0.070008 | Time: 0.18s | Mean latent norm: 0.981560\n",
      "[Epoch 1280/2500] Loss: 0.070612 | Time: 0.18s | Mean latent norm: 0.981537\n",
      "[Epoch 1281/2500] Loss: 0.069162 | Time: 0.17s | Mean latent norm: 0.981545\n",
      "[Epoch 1282/2500] Loss: 0.069430 | Time: 0.18s | Mean latent norm: 0.981519\n",
      "[Epoch 1283/2500] Loss: 0.069676 | Time: 0.18s | Mean latent norm: 0.981519\n",
      "[Epoch 1284/2500] Loss: 0.070759 | Time: 0.18s | Mean latent norm: 0.981508\n",
      "[Epoch 1285/2500] Loss: 0.069944 | Time: 0.17s | Mean latent norm: 0.981503\n",
      "[Epoch 1286/2500] Loss: 0.069479 | Time: 0.20s | Mean latent norm: 0.981480\n",
      "[Epoch 1287/2500] Loss: 0.068870 | Time: 0.20s | Mean latent norm: 0.981474\n",
      "[Epoch 1288/2500] Loss: 0.068149 | Time: 0.23s | Mean latent norm: 0.981467\n",
      "[Epoch 1289/2500] Loss: 0.069403 | Time: 0.20s | Mean latent norm: 0.981464\n",
      "[Epoch 1290/2500] Loss: 0.068899 | Time: 0.18s | Mean latent norm: 0.981423\n",
      "[Epoch 1291/2500] Loss: 0.068641 | Time: 0.20s | Mean latent norm: 0.981432\n",
      "[Epoch 1292/2500] Loss: 0.069704 | Time: 0.19s | Mean latent norm: 0.981423\n",
      "[Epoch 1293/2500] Loss: 0.070987 | Time: 0.20s | Mean latent norm: 0.981395\n",
      "[Epoch 1294/2500] Loss: 0.072159 | Time: 0.20s | Mean latent norm: 0.981380\n",
      "[Epoch 1295/2500] Loss: 0.070730 | Time: 0.20s | Mean latent norm: 0.981394\n",
      "[Epoch 1296/2500] Loss: 0.077538 | Time: 0.20s | Mean latent norm: 0.981347\n",
      "[Epoch 1297/2500] Loss: 0.075577 | Time: 0.20s | Mean latent norm: 0.981324\n",
      "[Epoch 1298/2500] Loss: 0.077841 | Time: 0.18s | Mean latent norm: 0.981304\n",
      "[Epoch 1299/2500] Loss: 0.072611 | Time: 0.18s | Mean latent norm: 0.981287\n",
      "[Epoch 1300/2500] Loss: 0.070645 | Time: 0.17s | Mean latent norm: 0.981256\n",
      "[Epoch 1301/2500] Loss: 0.070918 | Time: 0.17s | Mean latent norm: 0.981239\n",
      "[Epoch 1302/2500] Loss: 0.070562 | Time: 0.17s | Mean latent norm: 0.981235\n",
      "[Epoch 1303/2500] Loss: 0.068274 | Time: 0.17s | Mean latent norm: 0.981226\n",
      "[Epoch 1304/2500] Loss: 0.068069 | Time: 0.17s | Mean latent norm: 0.981216\n",
      "[Epoch 1305/2500] Loss: 0.068277 | Time: 0.17s | Mean latent norm: 0.981215\n",
      "[Epoch 1306/2500] Loss: 0.068340 | Time: 0.17s | Mean latent norm: 0.981195\n",
      "[Epoch 1307/2500] Loss: 0.069774 | Time: 0.17s | Mean latent norm: 0.981196\n",
      "[Epoch 1308/2500] Loss: 0.068403 | Time: 0.17s | Mean latent norm: 0.981186\n",
      "[Epoch 1309/2500] Loss: 0.068235 | Time: 0.17s | Mean latent norm: 0.981174\n",
      "[Epoch 1310/2500] Loss: 0.068440 | Time: 0.17s | Mean latent norm: 0.981162\n",
      "[Epoch 1311/2500] Loss: 0.069011 | Time: 0.17s | Mean latent norm: 0.981162\n",
      "[Epoch 1312/2500] Loss: 0.069174 | Time: 0.17s | Mean latent norm: 0.981153\n",
      "[Epoch 1313/2500] Loss: 0.068702 | Time: 0.17s | Mean latent norm: 0.981135\n",
      "[Epoch 1314/2500] Loss: 0.068390 | Time: 0.17s | Mean latent norm: 0.981129\n",
      "[Epoch 1315/2500] Loss: 0.068148 | Time: 0.17s | Mean latent norm: 0.981136\n",
      "[Epoch 1316/2500] Loss: 0.067347 | Time: 0.17s | Mean latent norm: 0.981118\n",
      "[Epoch 1317/2500] Loss: 0.067933 | Time: 0.17s | Mean latent norm: 0.981103\n",
      "[Epoch 1318/2500] Loss: 0.067747 | Time: 0.17s | Mean latent norm: 0.981112\n",
      "[Epoch 1319/2500] Loss: 0.067825 | Time: 0.17s | Mean latent norm: 0.981101\n",
      "[Epoch 1320/2500] Loss: 0.069735 | Time: 0.17s | Mean latent norm: 0.981083\n",
      "[Epoch 1321/2500] Loss: 0.068864 | Time: 0.17s | Mean latent norm: 0.981073\n",
      "[Epoch 1322/2500] Loss: 0.068300 | Time: 0.17s | Mean latent norm: 0.981068\n",
      "[Epoch 1323/2500] Loss: 0.069059 | Time: 0.17s | Mean latent norm: 0.981062\n",
      "[Epoch 1324/2500] Loss: 0.070855 | Time: 0.17s | Mean latent norm: 0.981041\n",
      "[Epoch 1325/2500] Loss: 0.071349 | Time: 0.18s | Mean latent norm: 0.981022\n",
      "[Epoch 1326/2500] Loss: 0.070986 | Time: 0.17s | Mean latent norm: 0.981011\n",
      "[Epoch 1327/2500] Loss: 0.068357 | Time: 0.17s | Mean latent norm: 0.980994\n",
      "[Epoch 1328/2500] Loss: 0.067681 | Time: 0.17s | Mean latent norm: 0.980982\n",
      "[Epoch 1329/2500] Loss: 0.066994 | Time: 0.17s | Mean latent norm: 0.980984\n",
      "[Epoch 1330/2500] Loss: 0.066946 | Time: 0.17s | Mean latent norm: 0.980973\n",
      "[Epoch 1331/2500] Loss: 0.066866 | Time: 0.17s | Mean latent norm: 0.980971\n",
      "[Epoch 1332/2500] Loss: 0.067311 | Time: 0.17s | Mean latent norm: 0.980961\n",
      "[Epoch 1333/2500] Loss: 0.067070 | Time: 0.17s | Mean latent norm: 0.980961\n",
      "[Epoch 1334/2500] Loss: 0.067096 | Time: 0.17s | Mean latent norm: 0.980956\n",
      "[Epoch 1335/2500] Loss: 0.068637 | Time: 0.18s | Mean latent norm: 0.980938\n",
      "[Epoch 1336/2500] Loss: 0.068309 | Time: 0.17s | Mean latent norm: 0.980934\n",
      "[Epoch 1337/2500] Loss: 0.068174 | Time: 0.18s | Mean latent norm: 0.980914\n",
      "[Epoch 1338/2500] Loss: 0.067120 | Time: 0.18s | Mean latent norm: 0.980910\n",
      "[Epoch 1339/2500] Loss: 0.066939 | Time: 0.17s | Mean latent norm: 0.980897\n",
      "[Epoch 1340/2500] Loss: 0.067697 | Time: 0.18s | Mean latent norm: 0.980892\n",
      "[Epoch 1341/2500] Loss: 0.070098 | Time: 0.17s | Mean latent norm: 0.980876\n",
      "[Epoch 1342/2500] Loss: 0.069920 | Time: 0.17s | Mean latent norm: 0.980872\n",
      "[Epoch 1343/2500] Loss: 0.071855 | Time: 0.17s | Mean latent norm: 0.980860\n",
      "[Epoch 1344/2500] Loss: 0.068834 | Time: 0.17s | Mean latent norm: 0.980830\n",
      "[Epoch 1345/2500] Loss: 0.070799 | Time: 0.17s | Mean latent norm: 0.980815\n",
      "[Epoch 1346/2500] Loss: 0.069086 | Time: 0.17s | Mean latent norm: 0.980815\n",
      "[Epoch 1347/2500] Loss: 0.067961 | Time: 0.17s | Mean latent norm: 0.980801\n",
      "[Epoch 1348/2500] Loss: 0.066760 | Time: 0.17s | Mean latent norm: 0.980794\n",
      "[Epoch 1349/2500] Loss: 0.067142 | Time: 0.17s | Mean latent norm: 0.980784\n",
      "[Epoch 1350/2500] Loss: 0.069795 | Time: 0.17s | Mean latent norm: 0.980776\n",
      "[Epoch 1351/2500] Loss: 0.069808 | Time: 0.17s | Mean latent norm: 0.980756\n",
      "[Epoch 1352/2500] Loss: 0.069083 | Time: 0.17s | Mean latent norm: 0.980764\n",
      "[Epoch 1353/2500] Loss: 0.073972 | Time: 0.17s | Mean latent norm: 0.980759\n",
      "[Epoch 1354/2500] Loss: 0.075532 | Time: 0.17s | Mean latent norm: 0.980730\n",
      "[Epoch 1355/2500] Loss: 0.074139 | Time: 0.17s | Mean latent norm: 0.980697\n",
      "[Epoch 1356/2500] Loss: 0.071018 | Time: 0.17s | Mean latent norm: 0.980684\n",
      "[Epoch 1357/2500] Loss: 0.073135 | Time: 0.17s | Mean latent norm: 0.980668\n",
      "[Epoch 1358/2500] Loss: 0.072824 | Time: 0.17s | Mean latent norm: 0.980639\n",
      "[Epoch 1359/2500] Loss: 0.072451 | Time: 0.18s | Mean latent norm: 0.980629\n",
      "[Epoch 1360/2500] Loss: 0.071598 | Time: 0.17s | Mean latent norm: 0.980604\n",
      "[Epoch 1361/2500] Loss: 0.070890 | Time: 0.17s | Mean latent norm: 0.980573\n",
      "[Epoch 1362/2500] Loss: 0.069390 | Time: 0.18s | Mean latent norm: 0.980567\n",
      "[Epoch 1363/2500] Loss: 0.067329 | Time: 0.17s | Mean latent norm: 0.980565\n",
      "[Epoch 1364/2500] Loss: 0.066541 | Time: 0.18s | Mean latent norm: 0.980547\n",
      "New best model saved at epoch 1364 with loss 0.066541\n",
      "[Epoch 1365/2500] Loss: 0.066085 | Time: 0.17s | Mean latent norm: 0.980537\n",
      "New best model saved at epoch 1365 with loss 0.066085\n",
      "[Epoch 1366/2500] Loss: 0.066573 | Time: 0.17s | Mean latent norm: 0.980546\n",
      "[Epoch 1367/2500] Loss: 0.067416 | Time: 0.18s | Mean latent norm: 0.980531\n",
      "[Epoch 1368/2500] Loss: 0.067390 | Time: 0.17s | Mean latent norm: 0.980532\n",
      "[Epoch 1369/2500] Loss: 0.068706 | Time: 0.17s | Mean latent norm: 0.980515\n",
      "[Epoch 1370/2500] Loss: 0.066868 | Time: 0.17s | Mean latent norm: 0.980524\n",
      "[Epoch 1371/2500] Loss: 0.067014 | Time: 0.17s | Mean latent norm: 0.980495\n",
      "[Epoch 1372/2500] Loss: 0.066943 | Time: 0.17s | Mean latent norm: 0.980491\n",
      "[Epoch 1373/2500] Loss: 0.069799 | Time: 0.18s | Mean latent norm: 0.980494\n",
      "[Epoch 1374/2500] Loss: 0.070047 | Time: 0.17s | Mean latent norm: 0.980491\n",
      "[Epoch 1375/2500] Loss: 0.068599 | Time: 0.17s | Mean latent norm: 0.980453\n",
      "[Epoch 1376/2500] Loss: 0.067080 | Time: 0.17s | Mean latent norm: 0.980434\n",
      "[Epoch 1377/2500] Loss: 0.066950 | Time: 0.17s | Mean latent norm: 0.980431\n",
      "[Epoch 1378/2500] Loss: 0.067309 | Time: 0.17s | Mean latent norm: 0.980432\n",
      "[Epoch 1379/2500] Loss: 0.066493 | Time: 0.18s | Mean latent norm: 0.980400\n",
      "[Epoch 1380/2500] Loss: 0.068425 | Time: 0.17s | Mean latent norm: 0.980401\n",
      "[Epoch 1381/2500] Loss: 0.067192 | Time: 0.17s | Mean latent norm: 0.980383\n",
      "[Epoch 1382/2500] Loss: 0.067714 | Time: 0.17s | Mean latent norm: 0.980379\n",
      "[Epoch 1383/2500] Loss: 0.065908 | Time: 0.17s | Mean latent norm: 0.980373\n",
      "New best model saved at epoch 1383 with loss 0.065908\n",
      "[Epoch 1384/2500] Loss: 0.066332 | Time: 0.17s | Mean latent norm: 0.980372\n",
      "[Epoch 1385/2500] Loss: 0.066707 | Time: 0.17s | Mean latent norm: 0.980357\n",
      "[Epoch 1386/2500] Loss: 0.066667 | Time: 0.17s | Mean latent norm: 0.980352\n",
      "[Epoch 1387/2500] Loss: 0.069784 | Time: 0.17s | Mean latent norm: 0.980347\n",
      "[Epoch 1388/2500] Loss: 0.066471 | Time: 0.17s | Mean latent norm: 0.980346\n",
      "[Epoch 1389/2500] Loss: 0.068728 | Time: 0.17s | Mean latent norm: 0.980325\n",
      "[Epoch 1390/2500] Loss: 0.078344 | Time: 0.17s | Mean latent norm: 0.980308\n",
      "[Epoch 1391/2500] Loss: 0.071624 | Time: 0.17s | Mean latent norm: 0.980278\n",
      "[Epoch 1392/2500] Loss: 0.078472 | Time: 0.17s | Mean latent norm: 0.980244\n",
      "[Epoch 1393/2500] Loss: 0.074732 | Time: 0.17s | Mean latent norm: 0.980215\n",
      "[Epoch 1394/2500] Loss: 0.070093 | Time: 0.17s | Mean latent norm: 0.980216\n",
      "[Epoch 1395/2500] Loss: 0.072745 | Time: 0.18s | Mean latent norm: 0.980198\n",
      "[Epoch 1396/2500] Loss: 0.069314 | Time: 0.17s | Mean latent norm: 0.980178\n",
      "[Epoch 1397/2500] Loss: 0.067625 | Time: 0.18s | Mean latent norm: 0.980161\n",
      "[Epoch 1398/2500] Loss: 0.067994 | Time: 0.17s | Mean latent norm: 0.980159\n",
      "[Epoch 1399/2500] Loss: 0.068484 | Time: 0.17s | Mean latent norm: 0.980157\n",
      "[Epoch 1400/2500] Loss: 0.068975 | Time: 0.17s | Mean latent norm: 0.980137\n",
      "[Epoch 1401/2500] Loss: 0.067319 | Time: 0.17s | Mean latent norm: 0.980110\n",
      "[Epoch 1402/2500] Loss: 0.066820 | Time: 0.17s | Mean latent norm: 0.980100\n",
      "[Epoch 1403/2500] Loss: 0.068901 | Time: 0.17s | Mean latent norm: 0.980090\n",
      "[Epoch 1404/2500] Loss: 0.069927 | Time: 0.17s | Mean latent norm: 0.980080\n",
      "[Epoch 1405/2500] Loss: 0.070355 | Time: 0.17s | Mean latent norm: 0.980058\n",
      "[Epoch 1406/2500] Loss: 0.069181 | Time: 0.17s | Mean latent norm: 0.980047\n",
      "[Epoch 1407/2500] Loss: 0.069749 | Time: 0.17s | Mean latent norm: 0.980036\n",
      "[Epoch 1408/2500] Loss: 0.069939 | Time: 0.17s | Mean latent norm: 0.980029\n",
      "[Epoch 1409/2500] Loss: 0.070612 | Time: 0.17s | Mean latent norm: 0.980014\n",
      "[Epoch 1410/2500] Loss: 0.073348 | Time: 0.17s | Mean latent norm: 0.979997\n",
      "[Epoch 1411/2500] Loss: 0.076188 | Time: 0.17s | Mean latent norm: 0.979986\n",
      "[Epoch 1412/2500] Loss: 0.071194 | Time: 0.17s | Mean latent norm: 0.979974\n",
      "[Epoch 1413/2500] Loss: 0.071186 | Time: 0.17s | Mean latent norm: 0.979938\n",
      "[Epoch 1414/2500] Loss: 0.069907 | Time: 0.17s | Mean latent norm: 0.979910\n",
      "[Epoch 1415/2500] Loss: 0.067349 | Time: 0.17s | Mean latent norm: 0.979887\n",
      "[Epoch 1416/2500] Loss: 0.067208 | Time: 0.17s | Mean latent norm: 0.979891\n",
      "[Epoch 1417/2500] Loss: 0.067534 | Time: 0.17s | Mean latent norm: 0.979884\n",
      "[Epoch 1418/2500] Loss: 0.068740 | Time: 0.17s | Mean latent norm: 0.979878\n",
      "[Epoch 1419/2500] Loss: 0.068678 | Time: 0.17s | Mean latent norm: 0.979852\n",
      "[Epoch 1420/2500] Loss: 0.068105 | Time: 0.17s | Mean latent norm: 0.979850\n",
      "[Epoch 1421/2500] Loss: 0.066550 | Time: 0.18s | Mean latent norm: 0.979845\n",
      "[Epoch 1422/2500] Loss: 0.066324 | Time: 0.18s | Mean latent norm: 0.979843\n",
      "[Epoch 1423/2500] Loss: 0.067860 | Time: 0.18s | Mean latent norm: 0.979821\n",
      "[Epoch 1424/2500] Loss: 0.066269 | Time: 0.17s | Mean latent norm: 0.979817\n",
      "[Epoch 1425/2500] Loss: 0.065355 | Time: 0.18s | Mean latent norm: 0.979815\n",
      "New best model saved at epoch 1425 with loss 0.065355\n",
      "[Epoch 1426/2500] Loss: 0.065749 | Time: 0.17s | Mean latent norm: 0.979814\n",
      "[Epoch 1427/2500] Loss: 0.065912 | Time: 0.17s | Mean latent norm: 0.979812\n",
      "[Epoch 1428/2500] Loss: 0.065558 | Time: 0.17s | Mean latent norm: 0.979811\n",
      "[Epoch 1429/2500] Loss: 0.066072 | Time: 0.17s | Mean latent norm: 0.979799\n",
      "[Epoch 1430/2500] Loss: 0.065808 | Time: 0.17s | Mean latent norm: 0.979787\n",
      "[Epoch 1431/2500] Loss: 0.065362 | Time: 0.17s | Mean latent norm: 0.979788\n",
      "[Epoch 1432/2500] Loss: 0.065993 | Time: 0.17s | Mean latent norm: 0.979793\n",
      "[Epoch 1433/2500] Loss: 0.066196 | Time: 0.17s | Mean latent norm: 0.979781\n",
      "[Epoch 1434/2500] Loss: 0.066814 | Time: 0.17s | Mean latent norm: 0.979769\n",
      "[Epoch 1435/2500] Loss: 0.066834 | Time: 0.17s | Mean latent norm: 0.979774\n",
      "[Epoch 1436/2500] Loss: 0.065193 | Time: 0.17s | Mean latent norm: 0.979762\n",
      "New best model saved at epoch 1436 with loss 0.065193\n",
      "[Epoch 1437/2500] Loss: 0.064611 | Time: 0.17s | Mean latent norm: 0.979750\n",
      "New best model saved at epoch 1437 with loss 0.064611\n",
      "[Epoch 1438/2500] Loss: 0.064629 | Time: 0.17s | Mean latent norm: 0.979737\n",
      "[Epoch 1439/2500] Loss: 0.064853 | Time: 0.17s | Mean latent norm: 0.979741\n",
      "[Epoch 1440/2500] Loss: 0.065577 | Time: 0.17s | Mean latent norm: 0.979751\n",
      "[Epoch 1441/2500] Loss: 0.065887 | Time: 0.17s | Mean latent norm: 0.979735\n",
      "[Epoch 1442/2500] Loss: 0.065907 | Time: 0.17s | Mean latent norm: 0.979734\n",
      "[Epoch 1443/2500] Loss: 0.064753 | Time: 0.17s | Mean latent norm: 0.979724\n",
      "[Epoch 1444/2500] Loss: 0.064698 | Time: 0.17s | Mean latent norm: 0.979726\n",
      "[Epoch 1445/2500] Loss: 0.065153 | Time: 0.17s | Mean latent norm: 0.979705\n",
      "[Epoch 1446/2500] Loss: 0.065037 | Time: 0.17s | Mean latent norm: 0.979712\n",
      "[Epoch 1447/2500] Loss: 0.065368 | Time: 0.17s | Mean latent norm: 0.979699\n",
      "[Epoch 1448/2500] Loss: 0.065608 | Time: 0.17s | Mean latent norm: 0.979686\n",
      "[Epoch 1449/2500] Loss: 0.065764 | Time: 0.17s | Mean latent norm: 0.979682\n",
      "[Epoch 1450/2500] Loss: 0.066099 | Time: 0.18s | Mean latent norm: 0.979681\n",
      "[Epoch 1451/2500] Loss: 0.066277 | Time: 0.17s | Mean latent norm: 0.979656\n",
      "[Epoch 1452/2500] Loss: 0.067450 | Time: 0.17s | Mean latent norm: 0.979672\n",
      "[Epoch 1453/2500] Loss: 0.066892 | Time: 0.17s | Mean latent norm: 0.979646\n",
      "[Epoch 1454/2500] Loss: 0.067404 | Time: 0.17s | Mean latent norm: 0.979628\n",
      "[Epoch 1455/2500] Loss: 0.065824 | Time: 0.17s | Mean latent norm: 0.979613\n",
      "[Epoch 1456/2500] Loss: 0.069109 | Time: 0.17s | Mean latent norm: 0.979620\n",
      "[Epoch 1457/2500] Loss: 0.068792 | Time: 0.17s | Mean latent norm: 0.979585\n",
      "[Epoch 1458/2500] Loss: 0.069705 | Time: 0.17s | Mean latent norm: 0.979568\n",
      "[Epoch 1459/2500] Loss: 0.068891 | Time: 0.17s | Mean latent norm: 0.979536\n",
      "[Epoch 1460/2500] Loss: 0.070757 | Time: 0.17s | Mean latent norm: 0.979526\n",
      "[Epoch 1461/2500] Loss: 0.075386 | Time: 0.17s | Mean latent norm: 0.979490\n",
      "[Epoch 1462/2500] Loss: 0.081068 | Time: 0.17s | Mean latent norm: 0.979455\n",
      "[Epoch 1463/2500] Loss: 0.075870 | Time: 0.17s | Mean latent norm: 0.979407\n",
      "[Epoch 1464/2500] Loss: 0.074673 | Time: 0.17s | Mean latent norm: 0.979384\n",
      "[Epoch 1465/2500] Loss: 0.069105 | Time: 0.17s | Mean latent norm: 0.979387\n",
      "[Epoch 1466/2500] Loss: 0.068734 | Time: 0.17s | Mean latent norm: 0.979361\n",
      "[Epoch 1467/2500] Loss: 0.066846 | Time: 0.17s | Mean latent norm: 0.979354\n",
      "[Epoch 1468/2500] Loss: 0.065158 | Time: 0.17s | Mean latent norm: 0.979340\n",
      "[Epoch 1469/2500] Loss: 0.065208 | Time: 0.17s | Mean latent norm: 0.979338\n",
      "[Epoch 1470/2500] Loss: 0.065309 | Time: 0.17s | Mean latent norm: 0.979338\n",
      "[Epoch 1471/2500] Loss: 0.064959 | Time: 0.17s | Mean latent norm: 0.979329\n",
      "[Epoch 1472/2500] Loss: 0.064723 | Time: 0.17s | Mean latent norm: 0.979326\n",
      "[Epoch 1473/2500] Loss: 0.064904 | Time: 0.17s | Mean latent norm: 0.979323\n",
      "[Epoch 1474/2500] Loss: 0.064933 | Time: 0.17s | Mean latent norm: 0.979328\n",
      "[Epoch 1475/2500] Loss: 0.066900 | Time: 0.17s | Mean latent norm: 0.979318\n",
      "[Epoch 1476/2500] Loss: 0.065069 | Time: 0.17s | Mean latent norm: 0.979308\n",
      "[Epoch 1477/2500] Loss: 0.065579 | Time: 0.17s | Mean latent norm: 0.979286\n",
      "[Epoch 1478/2500] Loss: 0.065729 | Time: 0.17s | Mean latent norm: 0.979290\n",
      "[Epoch 1479/2500] Loss: 0.066405 | Time: 0.18s | Mean latent norm: 0.979269\n",
      "[Epoch 1480/2500] Loss: 0.065437 | Time: 0.17s | Mean latent norm: 0.979268\n",
      "[Epoch 1481/2500] Loss: 0.065765 | Time: 0.17s | Mean latent norm: 0.979245\n",
      "[Epoch 1482/2500] Loss: 0.064877 | Time: 0.17s | Mean latent norm: 0.979250\n",
      "[Epoch 1483/2500] Loss: 0.065065 | Time: 0.17s | Mean latent norm: 0.979248\n",
      "[Epoch 1484/2500] Loss: 0.065711 | Time: 0.17s | Mean latent norm: 0.979257\n",
      "[Epoch 1485/2500] Loss: 0.065732 | Time: 0.17s | Mean latent norm: 0.979226\n",
      "[Epoch 1486/2500] Loss: 0.066433 | Time: 0.17s | Mean latent norm: 0.979216\n",
      "[Epoch 1487/2500] Loss: 0.065136 | Time: 0.17s | Mean latent norm: 0.979207\n",
      "[Epoch 1488/2500] Loss: 0.065623 | Time: 0.17s | Mean latent norm: 0.979207\n",
      "[Epoch 1489/2500] Loss: 0.067028 | Time: 0.17s | Mean latent norm: 0.979179\n",
      "[Epoch 1490/2500] Loss: 0.066711 | Time: 0.17s | Mean latent norm: 0.979161\n",
      "[Epoch 1491/2500] Loss: 0.067849 | Time: 0.17s | Mean latent norm: 0.979175\n",
      "[Epoch 1492/2500] Loss: 0.068270 | Time: 0.17s | Mean latent norm: 0.979178\n",
      "[Epoch 1493/2500] Loss: 0.068262 | Time: 0.17s | Mean latent norm: 0.979129\n",
      "[Epoch 1494/2500] Loss: 0.065759 | Time: 0.17s | Mean latent norm: 0.979108\n",
      "[Epoch 1495/2500] Loss: 0.064706 | Time: 0.17s | Mean latent norm: 0.979105\n",
      "[Epoch 1496/2500] Loss: 0.065614 | Time: 0.17s | Mean latent norm: 0.979106\n",
      "[Epoch 1497/2500] Loss: 0.065666 | Time: 0.17s | Mean latent norm: 0.979095\n",
      "[Epoch 1498/2500] Loss: 0.065403 | Time: 0.17s | Mean latent norm: 0.979066\n",
      "[Epoch 1499/2500] Loss: 0.064985 | Time: 0.17s | Mean latent norm: 0.979066\n",
      "[Epoch 1500/2500] Loss: 0.064858 | Time: 0.17s | Mean latent norm: 0.979066\n",
      "[Epoch 1501/2500] Loss: 0.065976 | Time: 0.17s | Mean latent norm: 0.979048\n",
      "[Epoch 1502/2500] Loss: 0.066682 | Time: 0.17s | Mean latent norm: 0.979033\n",
      "[Epoch 1503/2500] Loss: 0.066435 | Time: 0.17s | Mean latent norm: 0.979024\n",
      "[Epoch 1504/2500] Loss: 0.065238 | Time: 0.17s | Mean latent norm: 0.979008\n",
      "[Epoch 1505/2500] Loss: 0.065283 | Time: 0.17s | Mean latent norm: 0.979015\n",
      "[Epoch 1506/2500] Loss: 0.065377 | Time: 0.18s | Mean latent norm: 0.978993\n",
      "[Epoch 1507/2500] Loss: 0.064866 | Time: 0.17s | Mean latent norm: 0.978985\n",
      "[Epoch 1508/2500] Loss: 0.065768 | Time: 0.17s | Mean latent norm: 0.978969\n",
      "[Epoch 1509/2500] Loss: 0.065491 | Time: 0.17s | Mean latent norm: 0.978966\n",
      "[Epoch 1510/2500] Loss: 0.065274 | Time: 0.17s | Mean latent norm: 0.978955\n",
      "[Epoch 1511/2500] Loss: 0.065622 | Time: 0.17s | Mean latent norm: 0.978938\n",
      "[Epoch 1512/2500] Loss: 0.066418 | Time: 0.17s | Mean latent norm: 0.978932\n",
      "[Epoch 1513/2500] Loss: 0.065422 | Time: 0.17s | Mean latent norm: 0.978922\n",
      "[Epoch 1514/2500] Loss: 0.068231 | Time: 0.17s | Mean latent norm: 0.978913\n",
      "[Epoch 1515/2500] Loss: 0.067695 | Time: 0.17s | Mean latent norm: 0.978899\n",
      "[Epoch 1516/2500] Loss: 0.068458 | Time: 0.17s | Mean latent norm: 0.978878\n",
      "[Epoch 1517/2500] Loss: 0.067469 | Time: 0.17s | Mean latent norm: 0.978864\n",
      "[Epoch 1518/2500] Loss: 0.064226 | Time: 0.17s | Mean latent norm: 0.978853\n",
      "New best model saved at epoch 1518 with loss 0.064226\n",
      "[Epoch 1519/2500] Loss: 0.064419 | Time: 0.17s | Mean latent norm: 0.978851\n",
      "[Epoch 1520/2500] Loss: 0.065541 | Time: 0.17s | Mean latent norm: 0.978835\n",
      "[Epoch 1521/2500] Loss: 0.064628 | Time: 0.17s | Mean latent norm: 0.978836\n",
      "[Epoch 1522/2500] Loss: 0.067137 | Time: 0.17s | Mean latent norm: 0.978832\n",
      "[Epoch 1523/2500] Loss: 0.064457 | Time: 0.17s | Mean latent norm: 0.978804\n",
      "[Epoch 1524/2500] Loss: 0.066263 | Time: 0.17s | Mean latent norm: 0.978794\n",
      "[Epoch 1525/2500] Loss: 0.065141 | Time: 0.17s | Mean latent norm: 0.978798\n",
      "[Epoch 1526/2500] Loss: 0.067580 | Time: 0.17s | Mean latent norm: 0.978774\n",
      "[Epoch 1527/2500] Loss: 0.066960 | Time: 0.17s | Mean latent norm: 0.978763\n",
      "[Epoch 1528/2500] Loss: 0.068859 | Time: 0.17s | Mean latent norm: 0.978736\n",
      "[Epoch 1529/2500] Loss: 0.066923 | Time: 0.17s | Mean latent norm: 0.978723\n",
      "[Epoch 1530/2500] Loss: 0.065429 | Time: 0.17s | Mean latent norm: 0.978719\n",
      "[Epoch 1531/2500] Loss: 0.065432 | Time: 0.17s | Mean latent norm: 0.978720\n",
      "[Epoch 1532/2500] Loss: 0.067606 | Time: 0.17s | Mean latent norm: 0.978702\n",
      "[Epoch 1533/2500] Loss: 0.065657 | Time: 0.19s | Mean latent norm: 0.978676\n",
      "[Epoch 1534/2500] Loss: 0.068642 | Time: 0.17s | Mean latent norm: 0.978671\n",
      "[Epoch 1535/2500] Loss: 0.066281 | Time: 0.18s | Mean latent norm: 0.978668\n",
      "[Epoch 1536/2500] Loss: 0.065925 | Time: 0.18s | Mean latent norm: 0.978660\n",
      "[Epoch 1537/2500] Loss: 0.067286 | Time: 0.17s | Mean latent norm: 0.978616\n",
      "[Epoch 1538/2500] Loss: 0.066429 | Time: 0.17s | Mean latent norm: 0.978618\n",
      "[Epoch 1539/2500] Loss: 0.068690 | Time: 0.17s | Mean latent norm: 0.978615\n",
      "[Epoch 1540/2500] Loss: 0.068539 | Time: 0.18s | Mean latent norm: 0.978584\n",
      "[Epoch 1541/2500] Loss: 0.065086 | Time: 0.17s | Mean latent norm: 0.978570\n",
      "[Epoch 1542/2500] Loss: 0.066455 | Time: 0.17s | Mean latent norm: 0.978559\n",
      "[Epoch 1543/2500] Loss: 0.065426 | Time: 0.17s | Mean latent norm: 0.978559\n",
      "[Epoch 1544/2500] Loss: 0.067804 | Time: 0.17s | Mean latent norm: 0.978535\n",
      "[Epoch 1545/2500] Loss: 0.066363 | Time: 0.17s | Mean latent norm: 0.978531\n",
      "[Epoch 1546/2500] Loss: 0.066214 | Time: 0.17s | Mean latent norm: 0.978525\n",
      "[Epoch 1547/2500] Loss: 0.065216 | Time: 0.17s | Mean latent norm: 0.978516\n",
      "[Epoch 1548/2500] Loss: 0.064271 | Time: 0.17s | Mean latent norm: 0.978512\n",
      "[Epoch 1549/2500] Loss: 0.066141 | Time: 0.17s | Mean latent norm: 0.978503\n",
      "[Epoch 1550/2500] Loss: 0.065490 | Time: 0.17s | Mean latent norm: 0.978492\n",
      "[Epoch 1551/2500] Loss: 0.064496 | Time: 0.17s | Mean latent norm: 0.978481\n",
      "[Epoch 1552/2500] Loss: 0.067870 | Time: 0.17s | Mean latent norm: 0.978470\n",
      "[Epoch 1553/2500] Loss: 0.068608 | Time: 0.17s | Mean latent norm: 0.978466\n",
      "[Epoch 1554/2500] Loss: 0.070212 | Time: 0.17s | Mean latent norm: 0.978445\n",
      "[Epoch 1555/2500] Loss: 0.070881 | Time: 0.17s | Mean latent norm: 0.978426\n",
      "[Epoch 1556/2500] Loss: 0.071328 | Time: 0.17s | Mean latent norm: 0.978390\n",
      "[Epoch 1557/2500] Loss: 0.067145 | Time: 0.17s | Mean latent norm: 0.978371\n",
      "[Epoch 1558/2500] Loss: 0.066008 | Time: 0.17s | Mean latent norm: 0.978370\n",
      "[Epoch 1559/2500] Loss: 0.064752 | Time: 0.18s | Mean latent norm: 0.978361\n",
      "[Epoch 1560/2500] Loss: 0.066719 | Time: 0.17s | Mean latent norm: 0.978356\n",
      "[Epoch 1561/2500] Loss: 0.064700 | Time: 0.17s | Mean latent norm: 0.978356\n",
      "[Epoch 1562/2500] Loss: 0.066901 | Time: 0.17s | Mean latent norm: 0.978342\n",
      "[Epoch 1563/2500] Loss: 0.070124 | Time: 0.17s | Mean latent norm: 0.978320\n",
      "[Epoch 1564/2500] Loss: 0.068446 | Time: 0.18s | Mean latent norm: 0.978310\n",
      "[Epoch 1565/2500] Loss: 0.067912 | Time: 0.17s | Mean latent norm: 0.978295\n",
      "[Epoch 1566/2500] Loss: 0.067946 | Time: 0.18s | Mean latent norm: 0.978291\n",
      "[Epoch 1567/2500] Loss: 0.068502 | Time: 0.17s | Mean latent norm: 0.978274\n",
      "[Epoch 1568/2500] Loss: 0.066609 | Time: 0.17s | Mean latent norm: 0.978235\n",
      "[Epoch 1569/2500] Loss: 0.067459 | Time: 0.17s | Mean latent norm: 0.978233\n",
      "[Epoch 1570/2500] Loss: 0.064161 | Time: 0.17s | Mean latent norm: 0.978235\n",
      "New best model saved at epoch 1570 with loss 0.064161\n",
      "[Epoch 1571/2500] Loss: 0.064358 | Time: 0.17s | Mean latent norm: 0.978222\n",
      "[Epoch 1572/2500] Loss: 0.063667 | Time: 0.17s | Mean latent norm: 0.978219\n",
      "New best model saved at epoch 1572 with loss 0.063667\n",
      "[Epoch 1573/2500] Loss: 0.064044 | Time: 0.17s | Mean latent norm: 0.978206\n",
      "[Epoch 1574/2500] Loss: 0.063638 | Time: 0.17s | Mean latent norm: 0.978201\n",
      "New best model saved at epoch 1574 with loss 0.063638\n",
      "[Epoch 1575/2500] Loss: 0.064717 | Time: 0.17s | Mean latent norm: 0.978205\n",
      "[Epoch 1576/2500] Loss: 0.063608 | Time: 0.17s | Mean latent norm: 0.978193\n",
      "New best model saved at epoch 1576 with loss 0.063608\n",
      "[Epoch 1577/2500] Loss: 0.063190 | Time: 0.17s | Mean latent norm: 0.978179\n",
      "New best model saved at epoch 1577 with loss 0.063190\n",
      "[Epoch 1578/2500] Loss: 0.063062 | Time: 0.17s | Mean latent norm: 0.978179\n",
      "New best model saved at epoch 1578 with loss 0.063062\n",
      "[Epoch 1579/2500] Loss: 0.062898 | Time: 0.17s | Mean latent norm: 0.978182\n",
      "New best model saved at epoch 1579 with loss 0.062898\n",
      "[Epoch 1580/2500] Loss: 0.062635 | Time: 0.17s | Mean latent norm: 0.978168\n",
      "New best model saved at epoch 1580 with loss 0.062635\n",
      "[Epoch 1581/2500] Loss: 0.062742 | Time: 0.18s | Mean latent norm: 0.978176\n",
      "[Epoch 1582/2500] Loss: 0.062444 | Time: 0.17s | Mean latent norm: 0.978175\n",
      "New best model saved at epoch 1582 with loss 0.062444\n",
      "[Epoch 1583/2500] Loss: 0.063097 | Time: 0.17s | Mean latent norm: 0.978171\n",
      "[Epoch 1584/2500] Loss: 0.063405 | Time: 0.17s | Mean latent norm: 0.978158\n",
      "[Epoch 1585/2500] Loss: 0.064291 | Time: 0.17s | Mean latent norm: 0.978157\n",
      "[Epoch 1586/2500] Loss: 0.063997 | Time: 0.17s | Mean latent norm: 0.978154\n",
      "[Epoch 1587/2500] Loss: 0.064093 | Time: 0.17s | Mean latent norm: 0.978147\n",
      "[Epoch 1588/2500] Loss: 0.063916 | Time: 0.17s | Mean latent norm: 0.978135\n",
      "[Epoch 1589/2500] Loss: 0.064667 | Time: 0.17s | Mean latent norm: 0.978129\n",
      "[Epoch 1590/2500] Loss: 0.063470 | Time: 0.17s | Mean latent norm: 0.978116\n",
      "[Epoch 1591/2500] Loss: 0.063750 | Time: 0.17s | Mean latent norm: 0.978118\n",
      "[Epoch 1592/2500] Loss: 0.063781 | Time: 0.17s | Mean latent norm: 0.978105\n",
      "[Epoch 1593/2500] Loss: 0.064138 | Time: 0.17s | Mean latent norm: 0.978099\n",
      "[Epoch 1594/2500] Loss: 0.063815 | Time: 0.17s | Mean latent norm: 0.978080\n",
      "[Epoch 1595/2500] Loss: 0.066121 | Time: 0.17s | Mean latent norm: 0.978087\n",
      "[Epoch 1596/2500] Loss: 0.068405 | Time: 0.17s | Mean latent norm: 0.978071\n",
      "[Epoch 1597/2500] Loss: 0.072746 | Time: 0.17s | Mean latent norm: 0.978049\n",
      "[Epoch 1598/2500] Loss: 0.069426 | Time: 0.17s | Mean latent norm: 0.978018\n",
      "[Epoch 1599/2500] Loss: 0.065963 | Time: 0.17s | Mean latent norm: 0.978002\n",
      "[Epoch 1600/2500] Loss: 0.067518 | Time: 0.17s | Mean latent norm: 0.977994\n",
      "[Epoch 1601/2500] Loss: 0.070748 | Time: 0.17s | Mean latent norm: 0.977964\n",
      "[Epoch 1602/2500] Loss: 0.070352 | Time: 0.17s | Mean latent norm: 0.977937\n",
      "[Epoch 1603/2500] Loss: 0.066843 | Time: 0.18s | Mean latent norm: 0.977919\n",
      "[Epoch 1604/2500] Loss: 0.064516 | Time: 0.17s | Mean latent norm: 0.977914\n",
      "[Epoch 1605/2500] Loss: 0.063718 | Time: 0.17s | Mean latent norm: 0.977914\n",
      "[Epoch 1606/2500] Loss: 0.063611 | Time: 0.17s | Mean latent norm: 0.977917\n",
      "[Epoch 1607/2500] Loss: 0.063256 | Time: 0.17s | Mean latent norm: 0.977897\n",
      "[Epoch 1608/2500] Loss: 0.064113 | Time: 0.18s | Mean latent norm: 0.977890\n",
      "[Epoch 1609/2500] Loss: 0.063326 | Time: 0.17s | Mean latent norm: 0.977890\n",
      "[Epoch 1610/2500] Loss: 0.063273 | Time: 0.17s | Mean latent norm: 0.977887\n",
      "[Epoch 1611/2500] Loss: 0.063703 | Time: 0.17s | Mean latent norm: 0.977875\n",
      "[Epoch 1612/2500] Loss: 0.062799 | Time: 0.17s | Mean latent norm: 0.977863\n",
      "[Epoch 1613/2500] Loss: 0.063225 | Time: 0.17s | Mean latent norm: 0.977866\n",
      "[Epoch 1614/2500] Loss: 0.063634 | Time: 0.17s | Mean latent norm: 0.977859\n",
      "[Epoch 1615/2500] Loss: 0.062570 | Time: 0.17s | Mean latent norm: 0.977861\n",
      "[Epoch 1616/2500] Loss: 0.064374 | Time: 0.17s | Mean latent norm: 0.977837\n",
      "[Epoch 1617/2500] Loss: 0.064855 | Time: 0.17s | Mean latent norm: 0.977817\n",
      "[Epoch 1618/2500] Loss: 0.065041 | Time: 0.18s | Mean latent norm: 0.977821\n",
      "[Epoch 1619/2500] Loss: 0.064743 | Time: 0.17s | Mean latent norm: 0.977819\n",
      "[Epoch 1620/2500] Loss: 0.064314 | Time: 0.18s | Mean latent norm: 0.977801\n",
      "[Epoch 1621/2500] Loss: 0.063326 | Time: 0.17s | Mean latent norm: 0.977779\n",
      "[Epoch 1622/2500] Loss: 0.063838 | Time: 0.17s | Mean latent norm: 0.977774\n",
      "[Epoch 1623/2500] Loss: 0.064778 | Time: 0.17s | Mean latent norm: 0.977765\n",
      "[Epoch 1624/2500] Loss: 0.064792 | Time: 0.17s | Mean latent norm: 0.977755\n",
      "[Epoch 1625/2500] Loss: 0.067043 | Time: 0.17s | Mean latent norm: 0.977734\n",
      "[Epoch 1626/2500] Loss: 0.066722 | Time: 0.17s | Mean latent norm: 0.977738\n",
      "[Epoch 1627/2500] Loss: 0.063402 | Time: 0.17s | Mean latent norm: 0.977715\n",
      "[Epoch 1628/2500] Loss: 0.064000 | Time: 0.17s | Mean latent norm: 0.977720\n",
      "[Epoch 1629/2500] Loss: 0.063629 | Time: 0.17s | Mean latent norm: 0.977698\n",
      "[Epoch 1630/2500] Loss: 0.064294 | Time: 0.17s | Mean latent norm: 0.977699\n",
      "[Epoch 1631/2500] Loss: 0.065322 | Time: 0.17s | Mean latent norm: 0.977676\n",
      "[Epoch 1632/2500] Loss: 0.065023 | Time: 0.17s | Mean latent norm: 0.977683\n",
      "[Epoch 1633/2500] Loss: 0.068176 | Time: 0.17s | Mean latent norm: 0.977658\n",
      "[Epoch 1634/2500] Loss: 0.066553 | Time: 0.17s | Mean latent norm: 0.977653\n",
      "[Epoch 1635/2500] Loss: 0.066862 | Time: 0.18s | Mean latent norm: 0.977631\n",
      "[Epoch 1636/2500] Loss: 0.066096 | Time: 0.17s | Mean latent norm: 0.977623\n",
      "[Epoch 1637/2500] Loss: 0.063968 | Time: 0.17s | Mean latent norm: 0.977603\n",
      "[Epoch 1638/2500] Loss: 0.064499 | Time: 0.17s | Mean latent norm: 0.977591\n",
      "[Epoch 1639/2500] Loss: 0.064133 | Time: 0.17s | Mean latent norm: 0.977577\n",
      "[Epoch 1640/2500] Loss: 0.065592 | Time: 0.17s | Mean latent norm: 0.977569\n",
      "[Epoch 1641/2500] Loss: 0.067709 | Time: 0.17s | Mean latent norm: 0.977556\n",
      "[Epoch 1642/2500] Loss: 0.066602 | Time: 0.17s | Mean latent norm: 0.977546\n",
      "[Epoch 1643/2500] Loss: 0.071073 | Time: 0.17s | Mean latent norm: 0.977529\n",
      "[Epoch 1644/2500] Loss: 0.068751 | Time: 0.17s | Mean latent norm: 0.977516\n",
      "[Epoch 1645/2500] Loss: 0.066397 | Time: 0.17s | Mean latent norm: 0.977502\n",
      "[Epoch 1646/2500] Loss: 0.069790 | Time: 0.17s | Mean latent norm: 0.977486\n",
      "[Epoch 1647/2500] Loss: 0.066831 | Time: 0.17s | Mean latent norm: 0.977481\n",
      "[Epoch 1648/2500] Loss: 0.066841 | Time: 0.17s | Mean latent norm: 0.977464\n",
      "[Epoch 1649/2500] Loss: 0.066302 | Time: 0.17s | Mean latent norm: 0.977456\n",
      "[Epoch 1650/2500] Loss: 0.066625 | Time: 0.18s | Mean latent norm: 0.977447\n",
      "[Epoch 1651/2500] Loss: 0.068178 | Time: 0.18s | Mean latent norm: 0.977424\n",
      "[Epoch 1652/2500] Loss: 0.065100 | Time: 0.18s | Mean latent norm: 0.977405\n",
      "[Epoch 1653/2500] Loss: 0.066332 | Time: 0.17s | Mean latent norm: 0.977405\n",
      "[Epoch 1654/2500] Loss: 0.065756 | Time: 0.17s | Mean latent norm: 0.977374\n",
      "[Epoch 1655/2500] Loss: 0.064822 | Time: 0.18s | Mean latent norm: 0.977345\n",
      "[Epoch 1656/2500] Loss: 0.064384 | Time: 0.17s | Mean latent norm: 0.977348\n",
      "[Epoch 1657/2500] Loss: 0.063401 | Time: 0.17s | Mean latent norm: 0.977328\n",
      "[Epoch 1658/2500] Loss: 0.065066 | Time: 0.17s | Mean latent norm: 0.977330\n",
      "[Epoch 1659/2500] Loss: 0.066443 | Time: 0.17s | Mean latent norm: 0.977305\n",
      "[Epoch 1660/2500] Loss: 0.066479 | Time: 0.18s | Mean latent norm: 0.977301\n",
      "[Epoch 1661/2500] Loss: 0.065668 | Time: 0.18s | Mean latent norm: 0.977283\n",
      "[Epoch 1662/2500] Loss: 0.065892 | Time: 0.18s | Mean latent norm: 0.977268\n",
      "[Epoch 1663/2500] Loss: 0.065083 | Time: 0.18s | Mean latent norm: 0.977257\n",
      "[Epoch 1664/2500] Loss: 0.064240 | Time: 0.19s | Mean latent norm: 0.977242\n",
      "[Epoch 1665/2500] Loss: 0.062407 | Time: 0.19s | Mean latent norm: 0.977229\n",
      "New best model saved at epoch 1665 with loss 0.062407\n",
      "[Epoch 1666/2500] Loss: 0.063514 | Time: 0.18s | Mean latent norm: 0.977229\n",
      "[Epoch 1667/2500] Loss: 0.063969 | Time: 0.18s | Mean latent norm: 0.977217\n",
      "[Epoch 1668/2500] Loss: 0.068812 | Time: 0.18s | Mean latent norm: 0.977200\n",
      "[Epoch 1669/2500] Loss: 0.067344 | Time: 0.18s | Mean latent norm: 0.977184\n",
      "[Epoch 1670/2500] Loss: 0.065706 | Time: 0.18s | Mean latent norm: 0.977178\n",
      "[Epoch 1671/2500] Loss: 0.062706 | Time: 0.18s | Mean latent norm: 0.977174\n",
      "[Epoch 1672/2500] Loss: 0.065174 | Time: 0.19s | Mean latent norm: 0.977154\n",
      "[Epoch 1673/2500] Loss: 0.064649 | Time: 0.18s | Mean latent norm: 0.977150\n",
      "[Epoch 1674/2500] Loss: 0.063579 | Time: 0.18s | Mean latent norm: 0.977128\n",
      "[Epoch 1675/2500] Loss: 0.063250 | Time: 0.18s | Mean latent norm: 0.977125\n",
      "[Epoch 1676/2500] Loss: 0.062754 | Time: 0.18s | Mean latent norm: 0.977110\n",
      "[Epoch 1677/2500] Loss: 0.062591 | Time: 0.18s | Mean latent norm: 0.977120\n",
      "[Epoch 1678/2500] Loss: 0.062322 | Time: 0.18s | Mean latent norm: 0.977103\n",
      "New best model saved at epoch 1678 with loss 0.062322\n",
      "[Epoch 1679/2500] Loss: 0.063705 | Time: 0.18s | Mean latent norm: 0.977109\n",
      "[Epoch 1680/2500] Loss: 0.063777 | Time: 0.18s | Mean latent norm: 0.977093\n",
      "[Epoch 1681/2500] Loss: 0.064338 | Time: 0.18s | Mean latent norm: 0.977098\n",
      "[Epoch 1682/2500] Loss: 0.065678 | Time: 0.18s | Mean latent norm: 0.977062\n",
      "[Epoch 1683/2500] Loss: 0.065288 | Time: 0.18s | Mean latent norm: 0.977069\n",
      "[Epoch 1684/2500] Loss: 0.063161 | Time: 0.18s | Mean latent norm: 0.977047\n",
      "[Epoch 1685/2500] Loss: 0.064667 | Time: 0.18s | Mean latent norm: 0.977033\n",
      "[Epoch 1686/2500] Loss: 0.064452 | Time: 0.19s | Mean latent norm: 0.977021\n",
      "[Epoch 1687/2500] Loss: 0.062971 | Time: 0.20s | Mean latent norm: 0.977007\n",
      "[Epoch 1688/2500] Loss: 0.065243 | Time: 0.19s | Mean latent norm: 0.976994\n",
      "[Epoch 1689/2500] Loss: 0.064138 | Time: 0.18s | Mean latent norm: 0.976979\n",
      "[Epoch 1690/2500] Loss: 0.063413 | Time: 0.19s | Mean latent norm: 0.976968\n",
      "[Epoch 1691/2500] Loss: 0.064358 | Time: 0.20s | Mean latent norm: 0.976969\n",
      "[Epoch 1692/2500] Loss: 0.064261 | Time: 0.19s | Mean latent norm: 0.976961\n",
      "[Epoch 1693/2500] Loss: 0.064538 | Time: 0.20s | Mean latent norm: 0.976939\n",
      "[Epoch 1694/2500] Loss: 0.065440 | Time: 0.22s | Mean latent norm: 0.976908\n",
      "[Epoch 1695/2500] Loss: 0.063585 | Time: 0.21s | Mean latent norm: 0.976906\n",
      "[Epoch 1696/2500] Loss: 0.063761 | Time: 0.20s | Mean latent norm: 0.976913\n",
      "[Epoch 1697/2500] Loss: 0.062730 | Time: 0.19s | Mean latent norm: 0.976893\n",
      "[Epoch 1698/2500] Loss: 0.063358 | Time: 0.19s | Mean latent norm: 0.976868\n",
      "[Epoch 1699/2500] Loss: 0.063311 | Time: 0.19s | Mean latent norm: 0.976867\n",
      "[Epoch 1700/2500] Loss: 0.062647 | Time: 0.20s | Mean latent norm: 0.976866\n",
      "[Epoch 1701/2500] Loss: 0.062777 | Time: 0.20s | Mean latent norm: 0.976859\n",
      "[Epoch 1702/2500] Loss: 0.062434 | Time: 0.20s | Mean latent norm: 0.976851\n",
      "[Epoch 1703/2500] Loss: 0.063495 | Time: 0.20s | Mean latent norm: 0.976842\n",
      "[Epoch 1704/2500] Loss: 0.063030 | Time: 0.20s | Mean latent norm: 0.976811\n",
      "[Epoch 1705/2500] Loss: 0.062336 | Time: 0.19s | Mean latent norm: 0.976812\n",
      "[Epoch 1706/2500] Loss: 0.062386 | Time: 0.21s | Mean latent norm: 0.976809\n",
      "[Epoch 1707/2500] Loss: 0.062247 | Time: 0.20s | Mean latent norm: 0.976804\n",
      "New best model saved at epoch 1707 with loss 0.062247\n",
      "[Epoch 1708/2500] Loss: 0.062071 | Time: 0.21s | Mean latent norm: 0.976798\n",
      "New best model saved at epoch 1708 with loss 0.062071\n",
      "[Epoch 1709/2500] Loss: 0.063543 | Time: 0.19s | Mean latent norm: 0.976789\n",
      "[Epoch 1710/2500] Loss: 0.063354 | Time: 0.18s | Mean latent norm: 0.976785\n",
      "[Epoch 1711/2500] Loss: 0.065161 | Time: 0.18s | Mean latent norm: 0.976785\n",
      "[Epoch 1712/2500] Loss: 0.064847 | Time: 0.20s | Mean latent norm: 0.976758\n",
      "[Epoch 1713/2500] Loss: 0.066302 | Time: 0.18s | Mean latent norm: 0.976744\n",
      "[Epoch 1714/2500] Loss: 0.066602 | Time: 0.19s | Mean latent norm: 0.976727\n",
      "[Epoch 1715/2500] Loss: 0.066867 | Time: 0.18s | Mean latent norm: 0.976706\n",
      "[Epoch 1716/2500] Loss: 0.070914 | Time: 0.18s | Mean latent norm: 0.976710\n",
      "[Epoch 1717/2500] Loss: 0.071132 | Time: 0.18s | Mean latent norm: 0.976706\n",
      "[Epoch 1718/2500] Loss: 0.074528 | Time: 0.18s | Mean latent norm: 0.976657\n",
      "[Epoch 1719/2500] Loss: 0.070020 | Time: 0.18s | Mean latent norm: 0.976627\n",
      "[Epoch 1720/2500] Loss: 0.068308 | Time: 0.18s | Mean latent norm: 0.976621\n",
      "[Epoch 1721/2500] Loss: 0.063827 | Time: 0.18s | Mean latent norm: 0.976614\n",
      "[Epoch 1722/2500] Loss: 0.062688 | Time: 0.18s | Mean latent norm: 0.976599\n",
      "[Epoch 1723/2500] Loss: 0.061914 | Time: 0.18s | Mean latent norm: 0.976599\n",
      "New best model saved at epoch 1723 with loss 0.061914\n",
      "[Epoch 1724/2500] Loss: 0.061681 | Time: 0.18s | Mean latent norm: 0.976593\n",
      "New best model saved at epoch 1724 with loss 0.061681\n",
      "[Epoch 1725/2500] Loss: 0.062088 | Time: 0.18s | Mean latent norm: 0.976604\n",
      "[Epoch 1726/2500] Loss: 0.062658 | Time: 0.18s | Mean latent norm: 0.976602\n",
      "[Epoch 1727/2500] Loss: 0.062478 | Time: 0.18s | Mean latent norm: 0.976586\n",
      "[Epoch 1728/2500] Loss: 0.062878 | Time: 0.18s | Mean latent norm: 0.976565\n",
      "[Epoch 1729/2500] Loss: 0.062037 | Time: 0.18s | Mean latent norm: 0.976567\n",
      "[Epoch 1730/2500] Loss: 0.062858 | Time: 0.20s | Mean latent norm: 0.976564\n",
      "[Epoch 1731/2500] Loss: 0.063433 | Time: 0.18s | Mean latent norm: 0.976560\n",
      "[Epoch 1732/2500] Loss: 0.063271 | Time: 0.18s | Mean latent norm: 0.976530\n",
      "[Epoch 1733/2500] Loss: 0.063434 | Time: 0.18s | Mean latent norm: 0.976535\n",
      "[Epoch 1734/2500] Loss: 0.062779 | Time: 0.18s | Mean latent norm: 0.976527\n",
      "[Epoch 1735/2500] Loss: 0.062438 | Time: 0.18s | Mean latent norm: 0.976514\n",
      "[Epoch 1736/2500] Loss: 0.061679 | Time: 0.18s | Mean latent norm: 0.976501\n",
      "New best model saved at epoch 1736 with loss 0.061679\n",
      "[Epoch 1737/2500] Loss: 0.063046 | Time: 0.18s | Mean latent norm: 0.976505\n",
      "[Epoch 1738/2500] Loss: 0.064425 | Time: 0.18s | Mean latent norm: 0.976498\n",
      "[Epoch 1739/2500] Loss: 0.066565 | Time: 0.18s | Mean latent norm: 0.976496\n",
      "[Epoch 1740/2500] Loss: 0.069797 | Time: 0.19s | Mean latent norm: 0.976475\n",
      "[Epoch 1741/2500] Loss: 0.073632 | Time: 0.21s | Mean latent norm: 0.976439\n",
      "[Epoch 1742/2500] Loss: 0.071329 | Time: 0.19s | Mean latent norm: 0.976408\n",
      "[Epoch 1743/2500] Loss: 0.065695 | Time: 0.18s | Mean latent norm: 0.976388\n",
      "[Epoch 1744/2500] Loss: 0.065553 | Time: 0.18s | Mean latent norm: 0.976393\n",
      "[Epoch 1745/2500] Loss: 0.064645 | Time: 0.19s | Mean latent norm: 0.976362\n",
      "[Epoch 1746/2500] Loss: 0.066025 | Time: 0.20s | Mean latent norm: 0.976359\n",
      "[Epoch 1747/2500] Loss: 0.067359 | Time: 0.18s | Mean latent norm: 0.976356\n",
      "[Epoch 1748/2500] Loss: 0.066138 | Time: 0.18s | Mean latent norm: 0.976335\n",
      "[Epoch 1749/2500] Loss: 0.068557 | Time: 0.18s | Mean latent norm: 0.976300\n",
      "[Epoch 1750/2500] Loss: 0.065596 | Time: 0.18s | Mean latent norm: 0.976285\n",
      "[Epoch 1751/2500] Loss: 0.066007 | Time: 0.19s | Mean latent norm: 0.976282\n",
      "[Epoch 1752/2500] Loss: 0.063688 | Time: 0.20s | Mean latent norm: 0.976246\n",
      "[Epoch 1753/2500] Loss: 0.064411 | Time: 0.20s | Mean latent norm: 0.976243\n",
      "[Epoch 1754/2500] Loss: 0.065876 | Time: 0.20s | Mean latent norm: 0.976233\n",
      "[Epoch 1755/2500] Loss: 0.065961 | Time: 0.18s | Mean latent norm: 0.976216\n",
      "[Epoch 1756/2500] Loss: 0.064903 | Time: 0.18s | Mean latent norm: 0.976191\n",
      "[Epoch 1757/2500] Loss: 0.062658 | Time: 0.18s | Mean latent norm: 0.976184\n",
      "[Epoch 1758/2500] Loss: 0.062145 | Time: 0.18s | Mean latent norm: 0.976170\n",
      "[Epoch 1759/2500] Loss: 0.061636 | Time: 0.18s | Mean latent norm: 0.976178\n",
      "New best model saved at epoch 1759 with loss 0.061636\n",
      "[Epoch 1760/2500] Loss: 0.061489 | Time: 0.18s | Mean latent norm: 0.976170\n",
      "New best model saved at epoch 1760 with loss 0.061489\n",
      "[Epoch 1761/2500] Loss: 0.062422 | Time: 0.18s | Mean latent norm: 0.976160\n",
      "[Epoch 1762/2500] Loss: 0.061871 | Time: 0.18s | Mean latent norm: 0.976147\n",
      "[Epoch 1763/2500] Loss: 0.062756 | Time: 0.18s | Mean latent norm: 0.976156\n",
      "[Epoch 1764/2500] Loss: 0.063316 | Time: 0.18s | Mean latent norm: 0.976143\n",
      "[Epoch 1765/2500] Loss: 0.063143 | Time: 0.18s | Mean latent norm: 0.976130\n",
      "[Epoch 1766/2500] Loss: 0.061598 | Time: 0.18s | Mean latent norm: 0.976132\n",
      "[Epoch 1767/2500] Loss: 0.062390 | Time: 0.18s | Mean latent norm: 0.976134\n",
      "[Epoch 1768/2500] Loss: 0.064968 | Time: 0.20s | Mean latent norm: 0.976115\n",
      "[Epoch 1769/2500] Loss: 0.063481 | Time: 0.21s | Mean latent norm: 0.976115\n",
      "[Epoch 1770/2500] Loss: 0.063309 | Time: 0.20s | Mean latent norm: 0.976086\n",
      "[Epoch 1771/2500] Loss: 0.061762 | Time: 0.18s | Mean latent norm: 0.976077\n",
      "[Epoch 1772/2500] Loss: 0.061230 | Time: 0.20s | Mean latent norm: 0.976072\n",
      "New best model saved at epoch 1772 with loss 0.061230\n",
      "[Epoch 1773/2500] Loss: 0.061441 | Time: 0.18s | Mean latent norm: 0.976065\n",
      "[Epoch 1774/2500] Loss: 0.061219 | Time: 0.18s | Mean latent norm: 0.976049\n",
      "New best model saved at epoch 1774 with loss 0.061219\n",
      "[Epoch 1775/2500] Loss: 0.061316 | Time: 0.18s | Mean latent norm: 0.976060\n",
      "[Epoch 1776/2500] Loss: 0.061881 | Time: 0.18s | Mean latent norm: 0.976052\n",
      "[Epoch 1777/2500] Loss: 0.062136 | Time: 0.18s | Mean latent norm: 0.976043\n",
      "[Epoch 1778/2500] Loss: 0.061605 | Time: 0.18s | Mean latent norm: 0.976044\n",
      "[Epoch 1779/2500] Loss: 0.062672 | Time: 0.18s | Mean latent norm: 0.976037\n",
      "[Epoch 1780/2500] Loss: 0.062162 | Time: 0.18s | Mean latent norm: 0.976035\n",
      "[Epoch 1781/2500] Loss: 0.063673 | Time: 0.18s | Mean latent norm: 0.976004\n",
      "[Epoch 1782/2500] Loss: 0.063234 | Time: 0.18s | Mean latent norm: 0.975981\n",
      "[Epoch 1783/2500] Loss: 0.062436 | Time: 0.18s | Mean latent norm: 0.975979\n",
      "[Epoch 1784/2500] Loss: 0.062410 | Time: 0.18s | Mean latent norm: 0.975975\n",
      "[Epoch 1785/2500] Loss: 0.063673 | Time: 0.19s | Mean latent norm: 0.975957\n",
      "[Epoch 1786/2500] Loss: 0.063322 | Time: 0.21s | Mean latent norm: 0.975951\n",
      "[Epoch 1787/2500] Loss: 0.063380 | Time: 0.19s | Mean latent norm: 0.975947\n",
      "[Epoch 1788/2500] Loss: 0.062037 | Time: 0.18s | Mean latent norm: 0.975939\n",
      "[Epoch 1789/2500] Loss: 0.062490 | Time: 0.18s | Mean latent norm: 0.975925\n",
      "[Epoch 1790/2500] Loss: 0.064471 | Time: 0.19s | Mean latent norm: 0.975905\n",
      "[Epoch 1791/2500] Loss: 0.067989 | Time: 0.18s | Mean latent norm: 0.975908\n",
      "[Epoch 1792/2500] Loss: 0.070193 | Time: 0.17s | Mean latent norm: 0.975880\n",
      "[Epoch 1793/2500] Loss: 0.066850 | Time: 0.17s | Mean latent norm: 0.975858\n",
      "[Epoch 1794/2500] Loss: 0.063079 | Time: 0.17s | Mean latent norm: 0.975852\n",
      "[Epoch 1795/2500] Loss: 0.061934 | Time: 0.17s | Mean latent norm: 0.975832\n",
      "[Epoch 1796/2500] Loss: 0.062956 | Time: 0.18s | Mean latent norm: 0.975814\n",
      "[Epoch 1797/2500] Loss: 0.061636 | Time: 0.18s | Mean latent norm: 0.975821\n",
      "[Epoch 1798/2500] Loss: 0.061564 | Time: 0.17s | Mean latent norm: 0.975793\n",
      "[Epoch 1799/2500] Loss: 0.061448 | Time: 0.17s | Mean latent norm: 0.975782\n",
      "[Epoch 1800/2500] Loss: 0.061505 | Time: 0.17s | Mean latent norm: 0.975795\n",
      "[Epoch 1801/2500] Loss: 0.061953 | Time: 0.18s | Mean latent norm: 0.975794\n",
      "[Epoch 1802/2500] Loss: 0.062113 | Time: 0.17s | Mean latent norm: 0.975782\n",
      "[Epoch 1803/2500] Loss: 0.061981 | Time: 0.17s | Mean latent norm: 0.975761\n",
      "[Epoch 1804/2500] Loss: 0.064191 | Time: 0.17s | Mean latent norm: 0.975754\n",
      "[Epoch 1805/2500] Loss: 0.064733 | Time: 0.17s | Mean latent norm: 0.975753\n",
      "[Epoch 1806/2500] Loss: 0.067590 | Time: 0.17s | Mean latent norm: 0.975721\n",
      "[Epoch 1807/2500] Loss: 0.066821 | Time: 0.17s | Mean latent norm: 0.975721\n",
      "[Epoch 1808/2500] Loss: 0.070190 | Time: 0.17s | Mean latent norm: 0.975708\n",
      "[Epoch 1809/2500] Loss: 0.069257 | Time: 0.17s | Mean latent norm: 0.975650\n",
      "[Epoch 1810/2500] Loss: 0.069759 | Time: 0.17s | Mean latent norm: 0.975622\n",
      "[Epoch 1811/2500] Loss: 0.067561 | Time: 0.17s | Mean latent norm: 0.975618\n",
      "[Epoch 1812/2500] Loss: 0.064093 | Time: 0.17s | Mean latent norm: 0.975612\n",
      "[Epoch 1813/2500] Loss: 0.065661 | Time: 0.18s | Mean latent norm: 0.975586\n",
      "[Epoch 1814/2500] Loss: 0.066239 | Time: 0.17s | Mean latent norm: 0.975591\n",
      "[Epoch 1815/2500] Loss: 0.065460 | Time: 0.18s | Mean latent norm: 0.975566\n",
      "[Epoch 1816/2500] Loss: 0.062663 | Time: 0.17s | Mean latent norm: 0.975542\n",
      "[Epoch 1817/2500] Loss: 0.066173 | Time: 0.17s | Mean latent norm: 0.975533\n",
      "[Epoch 1818/2500] Loss: 0.066150 | Time: 0.18s | Mean latent norm: 0.975541\n",
      "[Epoch 1819/2500] Loss: 0.070613 | Time: 0.17s | Mean latent norm: 0.975511\n",
      "[Epoch 1820/2500] Loss: 0.066008 | Time: 0.17s | Mean latent norm: 0.975463\n",
      "[Epoch 1821/2500] Loss: 0.062522 | Time: 0.17s | Mean latent norm: 0.975460\n",
      "[Epoch 1822/2500] Loss: 0.061996 | Time: 0.17s | Mean latent norm: 0.975451\n",
      "[Epoch 1823/2500] Loss: 0.061439 | Time: 0.17s | Mean latent norm: 0.975450\n",
      "[Epoch 1824/2500] Loss: 0.061485 | Time: 0.17s | Mean latent norm: 0.975447\n",
      "[Epoch 1825/2500] Loss: 0.061298 | Time: 0.17s | Mean latent norm: 0.975436\n",
      "[Epoch 1826/2500] Loss: 0.060685 | Time: 0.17s | Mean latent norm: 0.975444\n",
      "New best model saved at epoch 1826 with loss 0.060685\n",
      "[Epoch 1827/2500] Loss: 0.060625 | Time: 0.17s | Mean latent norm: 0.975435\n",
      "New best model saved at epoch 1827 with loss 0.060625\n",
      "[Epoch 1828/2500] Loss: 0.060164 | Time: 0.17s | Mean latent norm: 0.975430\n",
      "New best model saved at epoch 1828 with loss 0.060164\n",
      "[Epoch 1829/2500] Loss: 0.059917 | Time: 0.18s | Mean latent norm: 0.975434\n",
      "New best model saved at epoch 1829 with loss 0.059917\n",
      "[Epoch 1830/2500] Loss: 0.060385 | Time: 0.17s | Mean latent norm: 0.975426\n",
      "[Epoch 1831/2500] Loss: 0.060151 | Time: 0.17s | Mean latent norm: 0.975425\n",
      "[Epoch 1832/2500] Loss: 0.060484 | Time: 0.17s | Mean latent norm: 0.975418\n",
      "[Epoch 1833/2500] Loss: 0.060352 | Time: 0.18s | Mean latent norm: 0.975421\n",
      "[Epoch 1834/2500] Loss: 0.060375 | Time: 0.17s | Mean latent norm: 0.975417\n",
      "[Epoch 1835/2500] Loss: 0.060459 | Time: 0.17s | Mean latent norm: 0.975416\n",
      "[Epoch 1836/2500] Loss: 0.060163 | Time: 0.18s | Mean latent norm: 0.975395\n",
      "[Epoch 1837/2500] Loss: 0.061213 | Time: 0.17s | Mean latent norm: 0.975397\n",
      "[Epoch 1838/2500] Loss: 0.061248 | Time: 0.17s | Mean latent norm: 0.975407\n",
      "[Epoch 1839/2500] Loss: 0.061616 | Time: 0.18s | Mean latent norm: 0.975396\n",
      "[Epoch 1840/2500] Loss: 0.060589 | Time: 0.17s | Mean latent norm: 0.975369\n",
      "[Epoch 1841/2500] Loss: 0.061795 | Time: 0.17s | Mean latent norm: 0.975391\n",
      "[Epoch 1842/2500] Loss: 0.062429 | Time: 0.17s | Mean latent norm: 0.975362\n",
      "[Epoch 1843/2500] Loss: 0.062310 | Time: 0.17s | Mean latent norm: 0.975358\n",
      "[Epoch 1844/2500] Loss: 0.062942 | Time: 0.17s | Mean latent norm: 0.975338\n",
      "[Epoch 1845/2500] Loss: 0.063430 | Time: 0.17s | Mean latent norm: 0.975350\n",
      "[Epoch 1846/2500] Loss: 0.064097 | Time: 0.17s | Mean latent norm: 0.975328\n",
      "[Epoch 1847/2500] Loss: 0.062184 | Time: 0.17s | Mean latent norm: 0.975307\n",
      "[Epoch 1848/2500] Loss: 0.061737 | Time: 0.17s | Mean latent norm: 0.975291\n",
      "[Epoch 1849/2500] Loss: 0.061499 | Time: 0.17s | Mean latent norm: 0.975304\n",
      "[Epoch 1850/2500] Loss: 0.061553 | Time: 0.17s | Mean latent norm: 0.975281\n",
      "[Epoch 1851/2500] Loss: 0.063222 | Time: 0.17s | Mean latent norm: 0.975266\n",
      "[Epoch 1852/2500] Loss: 0.062210 | Time: 0.17s | Mean latent norm: 0.975276\n",
      "[Epoch 1853/2500] Loss: 0.063696 | Time: 0.17s | Mean latent norm: 0.975261\n",
      "[Epoch 1854/2500] Loss: 0.064438 | Time: 0.17s | Mean latent norm: 0.975233\n",
      "[Epoch 1855/2500] Loss: 0.062422 | Time: 0.17s | Mean latent norm: 0.975202\n",
      "[Epoch 1856/2500] Loss: 0.061171 | Time: 0.17s | Mean latent norm: 0.975210\n",
      "[Epoch 1857/2500] Loss: 0.060211 | Time: 0.17s | Mean latent norm: 0.975203\n",
      "[Epoch 1858/2500] Loss: 0.060843 | Time: 0.17s | Mean latent norm: 0.975197\n",
      "[Epoch 1859/2500] Loss: 0.062726 | Time: 0.17s | Mean latent norm: 0.975179\n",
      "[Epoch 1860/2500] Loss: 0.061619 | Time: 0.18s | Mean latent norm: 0.975185\n",
      "[Epoch 1861/2500] Loss: 0.062385 | Time: 0.17s | Mean latent norm: 0.975158\n",
      "[Epoch 1862/2500] Loss: 0.062306 | Time: 0.17s | Mean latent norm: 0.975157\n",
      "[Epoch 1863/2500] Loss: 0.062782 | Time: 0.17s | Mean latent norm: 0.975149\n",
      "[Epoch 1864/2500] Loss: 0.062214 | Time: 0.17s | Mean latent norm: 0.975125\n",
      "[Epoch 1865/2500] Loss: 0.061431 | Time: 0.17s | Mean latent norm: 0.975114\n",
      "[Epoch 1866/2500] Loss: 0.059979 | Time: 0.17s | Mean latent norm: 0.975116\n",
      "[Epoch 1867/2500] Loss: 0.061532 | Time: 0.17s | Mean latent norm: 0.975098\n",
      "[Epoch 1868/2500] Loss: 0.061668 | Time: 0.18s | Mean latent norm: 0.975097\n",
      "[Epoch 1869/2500] Loss: 0.062039 | Time: 0.18s | Mean latent norm: 0.975083\n",
      "[Epoch 1870/2500] Loss: 0.062961 | Time: 0.18s | Mean latent norm: 0.975071\n",
      "[Epoch 1871/2500] Loss: 0.062182 | Time: 0.17s | Mean latent norm: 0.975055\n",
      "[Epoch 1872/2500] Loss: 0.061204 | Time: 0.17s | Mean latent norm: 0.975057\n",
      "[Epoch 1873/2500] Loss: 0.061549 | Time: 0.17s | Mean latent norm: 0.975055\n",
      "[Epoch 1874/2500] Loss: 0.061896 | Time: 0.17s | Mean latent norm: 0.975054\n",
      "[Epoch 1875/2500] Loss: 0.062432 | Time: 0.17s | Mean latent norm: 0.975017\n",
      "[Epoch 1876/2500] Loss: 0.065436 | Time: 0.17s | Mean latent norm: 0.975015\n",
      "[Epoch 1877/2500] Loss: 0.063793 | Time: 0.17s | Mean latent norm: 0.974988\n",
      "[Epoch 1878/2500] Loss: 0.064081 | Time: 0.17s | Mean latent norm: 0.974986\n",
      "[Epoch 1879/2500] Loss: 0.062238 | Time: 0.17s | Mean latent norm: 0.974958\n",
      "[Epoch 1880/2500] Loss: 0.062780 | Time: 0.17s | Mean latent norm: 0.974972\n",
      "[Epoch 1881/2500] Loss: 0.063866 | Time: 0.17s | Mean latent norm: 0.974947\n",
      "[Epoch 1882/2500] Loss: 0.063403 | Time: 0.17s | Mean latent norm: 0.974932\n",
      "[Epoch 1883/2500] Loss: 0.062136 | Time: 0.17s | Mean latent norm: 0.974924\n",
      "[Epoch 1884/2500] Loss: 0.063182 | Time: 0.17s | Mean latent norm: 0.974911\n",
      "[Epoch 1885/2500] Loss: 0.061737 | Time: 0.18s | Mean latent norm: 0.974905\n",
      "[Epoch 1886/2500] Loss: 0.062450 | Time: 0.18s | Mean latent norm: 0.974904\n",
      "[Epoch 1887/2500] Loss: 0.062573 | Time: 0.18s | Mean latent norm: 0.974886\n",
      "[Epoch 1888/2500] Loss: 0.061416 | Time: 0.17s | Mean latent norm: 0.974876\n",
      "[Epoch 1889/2500] Loss: 0.061635 | Time: 0.18s | Mean latent norm: 0.974863\n",
      "[Epoch 1890/2500] Loss: 0.061084 | Time: 0.17s | Mean latent norm: 0.974856\n",
      "[Epoch 1891/2500] Loss: 0.060867 | Time: 0.17s | Mean latent norm: 0.974857\n",
      "[Epoch 1892/2500] Loss: 0.061508 | Time: 0.17s | Mean latent norm: 0.974838\n",
      "[Epoch 1893/2500] Loss: 0.061217 | Time: 0.17s | Mean latent norm: 0.974835\n",
      "[Epoch 1894/2500] Loss: 0.060429 | Time: 0.17s | Mean latent norm: 0.974827\n",
      "[Epoch 1895/2500] Loss: 0.059813 | Time: 0.17s | Mean latent norm: 0.974819\n",
      "New best model saved at epoch 1895 with loss 0.059813\n",
      "[Epoch 1896/2500] Loss: 0.061420 | Time: 0.17s | Mean latent norm: 0.974821\n",
      "[Epoch 1897/2500] Loss: 0.060782 | Time: 0.18s | Mean latent norm: 0.974812\n",
      "[Epoch 1898/2500] Loss: 0.061329 | Time: 0.17s | Mean latent norm: 0.974805\n",
      "[Epoch 1899/2500] Loss: 0.061870 | Time: 0.17s | Mean latent norm: 0.974807\n",
      "[Epoch 1900/2500] Loss: 0.063972 | Time: 0.17s | Mean latent norm: 0.974787\n",
      "[Epoch 1901/2500] Loss: 0.062413 | Time: 0.17s | Mean latent norm: 0.974773\n",
      "[Epoch 1902/2500] Loss: 0.061925 | Time: 0.17s | Mean latent norm: 0.974761\n",
      "[Epoch 1903/2500] Loss: 0.061956 | Time: 0.17s | Mean latent norm: 0.974741\n",
      "[Epoch 1904/2500] Loss: 0.060456 | Time: 0.18s | Mean latent norm: 0.974738\n",
      "[Epoch 1905/2500] Loss: 0.060193 | Time: 0.18s | Mean latent norm: 0.974730\n",
      "[Epoch 1906/2500] Loss: 0.059903 | Time: 0.17s | Mean latent norm: 0.974727\n",
      "[Epoch 1907/2500] Loss: 0.060347 | Time: 0.17s | Mean latent norm: 0.974713\n",
      "[Epoch 1908/2500] Loss: 0.060418 | Time: 0.18s | Mean latent norm: 0.974719\n",
      "[Epoch 1909/2500] Loss: 0.061246 | Time: 0.17s | Mean latent norm: 0.974702\n",
      "[Epoch 1910/2500] Loss: 0.060926 | Time: 0.17s | Mean latent norm: 0.974688\n",
      "[Epoch 1911/2500] Loss: 0.060782 | Time: 0.17s | Mean latent norm: 0.974675\n",
      "[Epoch 1912/2500] Loss: 0.059557 | Time: 0.17s | Mean latent norm: 0.974677\n",
      "New best model saved at epoch 1912 with loss 0.059557\n",
      "[Epoch 1913/2500] Loss: 0.060259 | Time: 0.18s | Mean latent norm: 0.974658\n",
      "[Epoch 1914/2500] Loss: 0.060268 | Time: 0.17s | Mean latent norm: 0.974651\n",
      "[Epoch 1915/2500] Loss: 0.060920 | Time: 0.17s | Mean latent norm: 0.974640\n",
      "[Epoch 1916/2500] Loss: 0.060123 | Time: 0.18s | Mean latent norm: 0.974652\n",
      "[Epoch 1917/2500] Loss: 0.060391 | Time: 0.18s | Mean latent norm: 0.974634\n",
      "[Epoch 1918/2500] Loss: 0.060027 | Time: 0.17s | Mean latent norm: 0.974628\n",
      "[Epoch 1919/2500] Loss: 0.060498 | Time: 0.17s | Mean latent norm: 0.974623\n",
      "[Epoch 1920/2500] Loss: 0.060949 | Time: 0.17s | Mean latent norm: 0.974621\n",
      "[Epoch 1921/2500] Loss: 0.060556 | Time: 0.17s | Mean latent norm: 0.974619\n",
      "[Epoch 1922/2500] Loss: 0.060838 | Time: 0.17s | Mean latent norm: 0.974619\n",
      "[Epoch 1923/2500] Loss: 0.061194 | Time: 0.17s | Mean latent norm: 0.974593\n",
      "[Epoch 1924/2500] Loss: 0.061662 | Time: 0.17s | Mean latent norm: 0.974584\n",
      "[Epoch 1925/2500] Loss: 0.062132 | Time: 0.17s | Mean latent norm: 0.974572\n",
      "[Epoch 1926/2500] Loss: 0.060940 | Time: 0.17s | Mean latent norm: 0.974577\n",
      "[Epoch 1927/2500] Loss: 0.062602 | Time: 0.17s | Mean latent norm: 0.974547\n",
      "[Epoch 1928/2500] Loss: 0.062133 | Time: 0.17s | Mean latent norm: 0.974542\n",
      "[Epoch 1929/2500] Loss: 0.061800 | Time: 0.17s | Mean latent norm: 0.974536\n",
      "[Epoch 1930/2500] Loss: 0.062250 | Time: 0.18s | Mean latent norm: 0.974535\n",
      "[Epoch 1931/2500] Loss: 0.062569 | Time: 0.17s | Mean latent norm: 0.974515\n",
      "[Epoch 1932/2500] Loss: 0.063887 | Time: 0.17s | Mean latent norm: 0.974517\n",
      "[Epoch 1933/2500] Loss: 0.061889 | Time: 0.17s | Mean latent norm: 0.974490\n",
      "[Epoch 1934/2500] Loss: 0.062786 | Time: 0.18s | Mean latent norm: 0.974461\n",
      "[Epoch 1935/2500] Loss: 0.064082 | Time: 0.17s | Mean latent norm: 0.974447\n",
      "[Epoch 1936/2500] Loss: 0.061188 | Time: 0.17s | Mean latent norm: 0.974451\n",
      "[Epoch 1937/2500] Loss: 0.061197 | Time: 0.17s | Mean latent norm: 0.974416\n",
      "[Epoch 1938/2500] Loss: 0.061862 | Time: 0.17s | Mean latent norm: 0.974416\n",
      "[Epoch 1939/2500] Loss: 0.061723 | Time: 0.17s | Mean latent norm: 0.974405\n",
      "[Epoch 1940/2500] Loss: 0.062892 | Time: 0.17s | Mean latent norm: 0.974394\n",
      "[Epoch 1941/2500] Loss: 0.061735 | Time: 0.17s | Mean latent norm: 0.974380\n",
      "[Epoch 1942/2500] Loss: 0.063515 | Time: 0.17s | Mean latent norm: 0.974371\n",
      "[Epoch 1943/2500] Loss: 0.062627 | Time: 0.17s | Mean latent norm: 0.974357\n",
      "[Epoch 1944/2500] Loss: 0.064223 | Time: 0.17s | Mean latent norm: 0.974336\n",
      "[Epoch 1945/2500] Loss: 0.063576 | Time: 0.18s | Mean latent norm: 0.974297\n",
      "[Epoch 1946/2500] Loss: 0.061636 | Time: 0.17s | Mean latent norm: 0.974306\n",
      "[Epoch 1947/2500] Loss: 0.060989 | Time: 0.17s | Mean latent norm: 0.974275\n",
      "[Epoch 1948/2500] Loss: 0.062017 | Time: 0.17s | Mean latent norm: 0.974268\n",
      "[Epoch 1949/2500] Loss: 0.062394 | Time: 0.17s | Mean latent norm: 0.974255\n",
      "[Epoch 1950/2500] Loss: 0.061403 | Time: 0.17s | Mean latent norm: 0.974244\n",
      "[Epoch 1951/2500] Loss: 0.065413 | Time: 0.17s | Mean latent norm: 0.974233\n",
      "[Epoch 1952/2500] Loss: 0.063769 | Time: 0.17s | Mean latent norm: 0.974217\n",
      "[Epoch 1953/2500] Loss: 0.063060 | Time: 0.18s | Mean latent norm: 0.974197\n",
      "[Epoch 1954/2500] Loss: 0.066146 | Time: 0.17s | Mean latent norm: 0.974185\n",
      "[Epoch 1955/2500] Loss: 0.071190 | Time: 0.17s | Mean latent norm: 0.974178\n",
      "[Epoch 1956/2500] Loss: 0.064498 | Time: 0.17s | Mean latent norm: 0.974164\n",
      "[Epoch 1957/2500] Loss: 0.068525 | Time: 0.18s | Mean latent norm: 0.974121\n",
      "[Epoch 1958/2500] Loss: 0.065627 | Time: 0.17s | Mean latent norm: 0.974113\n",
      "[Epoch 1959/2500] Loss: 0.066834 | Time: 0.17s | Mean latent norm: 0.974074\n",
      "[Epoch 1960/2500] Loss: 0.065901 | Time: 0.17s | Mean latent norm: 0.974059\n",
      "[Epoch 1961/2500] Loss: 0.061953 | Time: 0.17s | Mean latent norm: 0.974055\n",
      "[Epoch 1962/2500] Loss: 0.061204 | Time: 0.17s | Mean latent norm: 0.974035\n",
      "[Epoch 1963/2500] Loss: 0.063305 | Time: 0.18s | Mean latent norm: 0.974034\n",
      "[Epoch 1964/2500] Loss: 0.060189 | Time: 0.17s | Mean latent norm: 0.974029\n",
      "[Epoch 1965/2500] Loss: 0.060437 | Time: 0.17s | Mean latent norm: 0.974019\n",
      "[Epoch 1966/2500] Loss: 0.059794 | Time: 0.17s | Mean latent norm: 0.974015\n",
      "[Epoch 1967/2500] Loss: 0.059426 | Time: 0.17s | Mean latent norm: 0.974015\n",
      "New best model saved at epoch 1967 with loss 0.059426\n",
      "[Epoch 1968/2500] Loss: 0.059767 | Time: 0.18s | Mean latent norm: 0.974009\n",
      "[Epoch 1969/2500] Loss: 0.060273 | Time: 0.17s | Mean latent norm: 0.973997\n",
      "[Epoch 1970/2500] Loss: 0.059382 | Time: 0.17s | Mean latent norm: 0.973998\n",
      "New best model saved at epoch 1970 with loss 0.059382\n",
      "[Epoch 1971/2500] Loss: 0.059611 | Time: 0.17s | Mean latent norm: 0.974003\n",
      "[Epoch 1972/2500] Loss: 0.058966 | Time: 0.17s | Mean latent norm: 0.973987\n",
      "New best model saved at epoch 1972 with loss 0.058966\n",
      "[Epoch 1973/2500] Loss: 0.059249 | Time: 0.17s | Mean latent norm: 0.973981\n",
      "[Epoch 1974/2500] Loss: 0.058961 | Time: 0.18s | Mean latent norm: 0.973975\n",
      "New best model saved at epoch 1974 with loss 0.058961\n",
      "[Epoch 1975/2500] Loss: 0.059073 | Time: 0.18s | Mean latent norm: 0.973980\n",
      "[Epoch 1976/2500] Loss: 0.059674 | Time: 0.18s | Mean latent norm: 0.973983\n",
      "[Epoch 1977/2500] Loss: 0.060554 | Time: 0.17s | Mean latent norm: 0.973972\n",
      "[Epoch 1978/2500] Loss: 0.060536 | Time: 0.17s | Mean latent norm: 0.973960\n",
      "[Epoch 1979/2500] Loss: 0.061534 | Time: 0.17s | Mean latent norm: 0.973966\n",
      "[Epoch 1980/2500] Loss: 0.063808 | Time: 0.18s | Mean latent norm: 0.973944\n",
      "[Epoch 1981/2500] Loss: 0.062982 | Time: 0.17s | Mean latent norm: 0.973916\n",
      "[Epoch 1982/2500] Loss: 0.062492 | Time: 0.17s | Mean latent norm: 0.973893\n",
      "[Epoch 1983/2500] Loss: 0.062896 | Time: 0.17s | Mean latent norm: 0.973902\n",
      "[Epoch 1984/2500] Loss: 0.062856 | Time: 0.17s | Mean latent norm: 0.973869\n",
      "[Epoch 1985/2500] Loss: 0.063468 | Time: 0.17s | Mean latent norm: 0.973864\n",
      "[Epoch 1986/2500] Loss: 0.063347 | Time: 0.18s | Mean latent norm: 0.973837\n",
      "[Epoch 1987/2500] Loss: 0.061834 | Time: 0.17s | Mean latent norm: 0.973822\n",
      "[Epoch 1988/2500] Loss: 0.061306 | Time: 0.17s | Mean latent norm: 0.973806\n",
      "[Epoch 1989/2500] Loss: 0.061148 | Time: 0.17s | Mean latent norm: 0.973805\n",
      "[Epoch 1990/2500] Loss: 0.060082 | Time: 0.17s | Mean latent norm: 0.973797\n",
      "[Epoch 1991/2500] Loss: 0.060146 | Time: 0.17s | Mean latent norm: 0.973795\n",
      "[Epoch 1992/2500] Loss: 0.059153 | Time: 0.18s | Mean latent norm: 0.973782\n",
      "[Epoch 1993/2500] Loss: 0.060265 | Time: 0.17s | Mean latent norm: 0.973783\n",
      "[Epoch 1994/2500] Loss: 0.059303 | Time: 0.17s | Mean latent norm: 0.973783\n",
      "[Epoch 1995/2500] Loss: 0.059645 | Time: 0.17s | Mean latent norm: 0.973775\n",
      "[Epoch 1996/2500] Loss: 0.059310 | Time: 0.17s | Mean latent norm: 0.973752\n",
      "[Epoch 1997/2500] Loss: 0.059318 | Time: 0.17s | Mean latent norm: 0.973751\n",
      "[Epoch 1998/2500] Loss: 0.060105 | Time: 0.18s | Mean latent norm: 0.973768\n",
      "[Epoch 1999/2500] Loss: 0.062820 | Time: 0.17s | Mean latent norm: 0.973750\n",
      "[Epoch 2000/2500] Loss: 0.064060 | Time: 0.18s | Mean latent norm: 0.973717\n",
      "[Epoch 2001/2500] Loss: 0.068648 | Time: 0.17s | Mean latent norm: 0.973713\n",
      "[Epoch 2002/2500] Loss: 0.067844 | Time: 0.17s | Mean latent norm: 0.973680\n",
      "[Epoch 2003/2500] Loss: 0.067469 | Time: 0.17s | Mean latent norm: 0.973652\n",
      "[Epoch 2004/2500] Loss: 0.071668 | Time: 0.18s | Mean latent norm: 0.973622\n",
      "[Epoch 2005/2500] Loss: 0.082059 | Time: 0.17s | Mean latent norm: 0.973588\n",
      "[Epoch 2006/2500] Loss: 0.086919 | Time: 0.17s | Mean latent norm: 0.973576\n",
      "[Epoch 2007/2500] Loss: 0.072412 | Time: 0.18s | Mean latent norm: 0.973541\n",
      "[Epoch 2008/2500] Loss: 0.067616 | Time: 0.17s | Mean latent norm: 0.973514\n",
      "[Epoch 2009/2500] Loss: 0.064076 | Time: 0.17s | Mean latent norm: 0.973490\n",
      "[Epoch 2010/2500] Loss: 0.063998 | Time: 0.18s | Mean latent norm: 0.973476\n",
      "[Epoch 2011/2500] Loss: 0.064705 | Time: 0.17s | Mean latent norm: 0.973482\n",
      "[Epoch 2012/2500] Loss: 0.064669 | Time: 0.17s | Mean latent norm: 0.973463\n",
      "[Epoch 2013/2500] Loss: 0.069238 | Time: 0.18s | Mean latent norm: 0.973453\n",
      "[Epoch 2014/2500] Loss: 0.068985 | Time: 0.17s | Mean latent norm: 0.973426\n",
      "[Epoch 2015/2500] Loss: 0.064506 | Time: 0.17s | Mean latent norm: 0.973431\n",
      "[Epoch 2016/2500] Loss: 0.061002 | Time: 0.18s | Mean latent norm: 0.973409\n",
      "[Epoch 2017/2500] Loss: 0.060143 | Time: 0.18s | Mean latent norm: 0.973409\n",
      "[Epoch 2018/2500] Loss: 0.059809 | Time: 0.17s | Mean latent norm: 0.973414\n",
      "[Epoch 2019/2500] Loss: 0.059600 | Time: 0.17s | Mean latent norm: 0.973409\n",
      "[Epoch 2020/2500] Loss: 0.059862 | Time: 0.17s | Mean latent norm: 0.973390\n",
      "[Epoch 2021/2500] Loss: 0.060042 | Time: 0.17s | Mean latent norm: 0.973393\n",
      "[Epoch 2022/2500] Loss: 0.060379 | Time: 0.17s | Mean latent norm: 0.973379\n",
      "[Epoch 2023/2500] Loss: 0.059897 | Time: 0.18s | Mean latent norm: 0.973369\n",
      "[Epoch 2024/2500] Loss: 0.059864 | Time: 0.17s | Mean latent norm: 0.973362\n",
      "[Epoch 2025/2500] Loss: 0.060192 | Time: 0.17s | Mean latent norm: 0.973365\n",
      "[Epoch 2026/2500] Loss: 0.059735 | Time: 0.17s | Mean latent norm: 0.973357\n",
      "[Epoch 2027/2500] Loss: 0.060444 | Time: 0.17s | Mean latent norm: 0.973349\n",
      "[Epoch 2028/2500] Loss: 0.059787 | Time: 0.17s | Mean latent norm: 0.973339\n",
      "[Epoch 2029/2500] Loss: 0.059773 | Time: 0.17s | Mean latent norm: 0.973337\n",
      "[Epoch 2030/2500] Loss: 0.059731 | Time: 0.17s | Mean latent norm: 0.973334\n",
      "[Epoch 2031/2500] Loss: 0.060658 | Time: 0.17s | Mean latent norm: 0.973323\n",
      "[Epoch 2032/2500] Loss: 0.062158 | Time: 0.17s | Mean latent norm: 0.973309\n",
      "[Epoch 2033/2500] Loss: 0.061218 | Time: 0.17s | Mean latent norm: 0.973300\n",
      "[Epoch 2034/2500] Loss: 0.060956 | Time: 0.17s | Mean latent norm: 0.973283\n",
      "[Epoch 2035/2500] Loss: 0.061831 | Time: 0.17s | Mean latent norm: 0.973269\n",
      "[Epoch 2036/2500] Loss: 0.060816 | Time: 0.17s | Mean latent norm: 0.973268\n",
      "[Epoch 2037/2500] Loss: 0.060609 | Time: 0.17s | Mean latent norm: 0.973252\n",
      "[Epoch 2038/2500] Loss: 0.059990 | Time: 0.18s | Mean latent norm: 0.973247\n",
      "[Epoch 2039/2500] Loss: 0.061791 | Time: 0.17s | Mean latent norm: 0.973239\n",
      "[Epoch 2040/2500] Loss: 0.060533 | Time: 0.18s | Mean latent norm: 0.973246\n",
      "[Epoch 2041/2500] Loss: 0.059707 | Time: 0.17s | Mean latent norm: 0.973233\n",
      "[Epoch 2042/2500] Loss: 0.060069 | Time: 0.17s | Mean latent norm: 0.973239\n",
      "[Epoch 2043/2500] Loss: 0.060798 | Time: 0.17s | Mean latent norm: 0.973223\n",
      "[Epoch 2044/2500] Loss: 0.060384 | Time: 0.18s | Mean latent norm: 0.973211\n",
      "[Epoch 2045/2500] Loss: 0.060849 | Time: 0.18s | Mean latent norm: 0.973204\n",
      "[Epoch 2046/2500] Loss: 0.062139 | Time: 0.17s | Mean latent norm: 0.973198\n",
      "[Epoch 2047/2500] Loss: 0.059940 | Time: 0.17s | Mean latent norm: 0.973176\n",
      "[Epoch 2048/2500] Loss: 0.059953 | Time: 0.17s | Mean latent norm: 0.973172\n",
      "[Epoch 2049/2500] Loss: 0.060849 | Time: 0.17s | Mean latent norm: 0.973150\n",
      "[Epoch 2050/2500] Loss: 0.062942 | Time: 0.18s | Mean latent norm: 0.973139\n",
      "[Epoch 2051/2500] Loss: 0.061631 | Time: 0.17s | Mean latent norm: 0.973140\n",
      "[Epoch 2052/2500] Loss: 0.059797 | Time: 0.18s | Mean latent norm: 0.973131\n",
      "[Epoch 2053/2500] Loss: 0.060285 | Time: 0.17s | Mean latent norm: 0.973115\n",
      "[Epoch 2054/2500] Loss: 0.059465 | Time: 0.17s | Mean latent norm: 0.973095\n",
      "[Epoch 2055/2500] Loss: 0.060327 | Time: 0.17s | Mean latent norm: 0.973092\n",
      "[Epoch 2056/2500] Loss: 0.060946 | Time: 0.17s | Mean latent norm: 0.973079\n",
      "[Epoch 2057/2500] Loss: 0.060810 | Time: 0.17s | Mean latent norm: 0.973063\n",
      "[Epoch 2058/2500] Loss: 0.060635 | Time: 0.17s | Mean latent norm: 0.973043\n",
      "[Epoch 2059/2500] Loss: 0.059807 | Time: 0.17s | Mean latent norm: 0.973055\n",
      "[Epoch 2060/2500] Loss: 0.059859 | Time: 0.17s | Mean latent norm: 0.973050\n",
      "[Epoch 2061/2500] Loss: 0.059403 | Time: 0.17s | Mean latent norm: 0.973039\n",
      "[Epoch 2062/2500] Loss: 0.059260 | Time: 0.17s | Mean latent norm: 0.973035\n",
      "[Epoch 2063/2500] Loss: 0.058490 | Time: 0.17s | Mean latent norm: 0.973025\n",
      "New best model saved at epoch 2063 with loss 0.058490\n",
      "[Epoch 2064/2500] Loss: 0.059123 | Time: 0.18s | Mean latent norm: 0.973026\n",
      "[Epoch 2065/2500] Loss: 0.060449 | Time: 0.18s | Mean latent norm: 0.973020\n",
      "[Epoch 2066/2500] Loss: 0.060151 | Time: 0.17s | Mean latent norm: 0.973003\n",
      "[Epoch 2067/2500] Loss: 0.060721 | Time: 0.17s | Mean latent norm: 0.973001\n",
      "[Epoch 2068/2500] Loss: 0.058681 | Time: 0.18s | Mean latent norm: 0.972989\n",
      "[Epoch 2069/2500] Loss: 0.058765 | Time: 0.18s | Mean latent norm: 0.972992\n",
      "[Epoch 2070/2500] Loss: 0.058770 | Time: 0.18s | Mean latent norm: 0.972984\n",
      "[Epoch 2071/2500] Loss: 0.060492 | Time: 0.17s | Mean latent norm: 0.972979\n",
      "[Epoch 2072/2500] Loss: 0.060066 | Time: 0.17s | Mean latent norm: 0.972959\n",
      "[Epoch 2073/2500] Loss: 0.059690 | Time: 0.17s | Mean latent norm: 0.972954\n",
      "[Epoch 2074/2500] Loss: 0.059258 | Time: 0.17s | Mean latent norm: 0.972958\n",
      "[Epoch 2075/2500] Loss: 0.060403 | Time: 0.17s | Mean latent norm: 0.972938\n",
      "[Epoch 2076/2500] Loss: 0.060672 | Time: 0.17s | Mean latent norm: 0.972918\n",
      "[Epoch 2077/2500] Loss: 0.059684 | Time: 0.17s | Mean latent norm: 0.972920\n",
      "[Epoch 2078/2500] Loss: 0.061211 | Time: 0.17s | Mean latent norm: 0.972917\n",
      "[Epoch 2079/2500] Loss: 0.061343 | Time: 0.18s | Mean latent norm: 0.972900\n",
      "[Epoch 2080/2500] Loss: 0.065075 | Time: 0.17s | Mean latent norm: 0.972882\n",
      "[Epoch 2081/2500] Loss: 0.063344 | Time: 0.17s | Mean latent norm: 0.972871\n",
      "[Epoch 2082/2500] Loss: 0.059096 | Time: 0.17s | Mean latent norm: 0.972873\n",
      "[Epoch 2083/2500] Loss: 0.058597 | Time: 0.17s | Mean latent norm: 0.972864\n",
      "[Epoch 2084/2500] Loss: 0.058725 | Time: 0.17s | Mean latent norm: 0.972852\n",
      "[Epoch 2085/2500] Loss: 0.058478 | Time: 0.17s | Mean latent norm: 0.972839\n",
      "New best model saved at epoch 2085 with loss 0.058478\n",
      "[Epoch 2086/2500] Loss: 0.058646 | Time: 0.17s | Mean latent norm: 0.972829\n",
      "[Epoch 2087/2500] Loss: 0.058995 | Time: 0.17s | Mean latent norm: 0.972831\n",
      "[Epoch 2088/2500] Loss: 0.060163 | Time: 0.18s | Mean latent norm: 0.972830\n",
      "[Epoch 2089/2500] Loss: 0.059799 | Time: 0.17s | Mean latent norm: 0.972821\n",
      "[Epoch 2090/2500] Loss: 0.059857 | Time: 0.18s | Mean latent norm: 0.972797\n",
      "[Epoch 2091/2500] Loss: 0.059167 | Time: 0.17s | Mean latent norm: 0.972785\n",
      "[Epoch 2092/2500] Loss: 0.059235 | Time: 0.18s | Mean latent norm: 0.972776\n",
      "[Epoch 2093/2500] Loss: 0.059689 | Time: 0.17s | Mean latent norm: 0.972771\n",
      "[Epoch 2094/2500] Loss: 0.060853 | Time: 0.17s | Mean latent norm: 0.972776\n",
      "[Epoch 2095/2500] Loss: 0.064026 | Time: 0.17s | Mean latent norm: 0.972746\n",
      "[Epoch 2096/2500] Loss: 0.063342 | Time: 0.17s | Mean latent norm: 0.972741\n",
      "[Epoch 2097/2500] Loss: 0.063672 | Time: 0.17s | Mean latent norm: 0.972710\n",
      "[Epoch 2098/2500] Loss: 0.061150 | Time: 0.17s | Mean latent norm: 0.972693\n",
      "[Epoch 2099/2500] Loss: 0.060252 | Time: 0.18s | Mean latent norm: 0.972674\n",
      "[Epoch 2100/2500] Loss: 0.063954 | Time: 0.17s | Mean latent norm: 0.972651\n",
      "[Epoch 2101/2500] Loss: 0.062092 | Time: 0.17s | Mean latent norm: 0.972637\n",
      "[Epoch 2102/2500] Loss: 0.063773 | Time: 0.17s | Mean latent norm: 0.972629\n",
      "[Epoch 2103/2500] Loss: 0.061205 | Time: 0.17s | Mean latent norm: 0.972624\n",
      "[Epoch 2104/2500] Loss: 0.064153 | Time: 0.17s | Mean latent norm: 0.972605\n",
      "[Epoch 2105/2500] Loss: 0.065007 | Time: 0.17s | Mean latent norm: 0.972574\n",
      "[Epoch 2106/2500] Loss: 0.064820 | Time: 0.17s | Mean latent norm: 0.972573\n",
      "[Epoch 2107/2500] Loss: 0.074336 | Time: 0.17s | Mean latent norm: 0.972551\n",
      "[Epoch 2108/2500] Loss: 0.068201 | Time: 0.17s | Mean latent norm: 0.972529\n",
      "[Epoch 2109/2500] Loss: 0.079192 | Time: 0.17s | Mean latent norm: 0.972485\n",
      "[Epoch 2110/2500] Loss: 0.066946 | Time: 0.17s | Mean latent norm: 0.972452\n",
      "[Epoch 2111/2500] Loss: 0.063999 | Time: 0.17s | Mean latent norm: 0.972427\n",
      "[Epoch 2112/2500] Loss: 0.062197 | Time: 0.18s | Mean latent norm: 0.972417\n",
      "[Epoch 2113/2500] Loss: 0.063320 | Time: 0.17s | Mean latent norm: 0.972422\n",
      "[Epoch 2114/2500] Loss: 0.064791 | Time: 0.17s | Mean latent norm: 0.972409\n",
      "[Epoch 2115/2500] Loss: 0.072975 | Time: 0.17s | Mean latent norm: 0.972398\n",
      "[Epoch 2116/2500] Loss: 0.063893 | Time: 0.17s | Mean latent norm: 0.972380\n",
      "[Epoch 2117/2500] Loss: 0.066008 | Time: 0.17s | Mean latent norm: 0.972363\n",
      "[Epoch 2118/2500] Loss: 0.065556 | Time: 0.17s | Mean latent norm: 0.972354\n",
      "[Epoch 2119/2500] Loss: 0.065364 | Time: 0.18s | Mean latent norm: 0.972331\n",
      "[Epoch 2120/2500] Loss: 0.068980 | Time: 0.17s | Mean latent norm: 0.972344\n",
      "[Epoch 2121/2500] Loss: 0.067670 | Time: 0.17s | Mean latent norm: 0.972302\n",
      "[Epoch 2122/2500] Loss: 0.061603 | Time: 0.17s | Mean latent norm: 0.972290\n",
      "[Epoch 2123/2500] Loss: 0.059668 | Time: 0.17s | Mean latent norm: 0.972284\n",
      "[Epoch 2124/2500] Loss: 0.058316 | Time: 0.17s | Mean latent norm: 0.972261\n",
      "New best model saved at epoch 2124 with loss 0.058316\n",
      "[Epoch 2125/2500] Loss: 0.058346 | Time: 0.18s | Mean latent norm: 0.972268\n",
      "[Epoch 2126/2500] Loss: 0.058530 | Time: 0.17s | Mean latent norm: 0.972271\n",
      "[Epoch 2127/2500] Loss: 0.058369 | Time: 0.17s | Mean latent norm: 0.972251\n",
      "[Epoch 2128/2500] Loss: 0.057984 | Time: 0.18s | Mean latent norm: 0.972262\n",
      "New best model saved at epoch 2128 with loss 0.057984\n",
      "[Epoch 2129/2500] Loss: 0.058310 | Time: 0.17s | Mean latent norm: 0.972258\n",
      "[Epoch 2130/2500] Loss: 0.058551 | Time: 0.17s | Mean latent norm: 0.972253\n",
      "[Epoch 2131/2500] Loss: 0.058158 | Time: 0.17s | Mean latent norm: 0.972253\n",
      "[Epoch 2132/2500] Loss: 0.058712 | Time: 0.17s | Mean latent norm: 0.972243\n",
      "[Epoch 2133/2500] Loss: 0.058152 | Time: 0.17s | Mean latent norm: 0.972237\n",
      "[Epoch 2134/2500] Loss: 0.058167 | Time: 0.17s | Mean latent norm: 0.972241\n",
      "[Epoch 2135/2500] Loss: 0.058355 | Time: 0.18s | Mean latent norm: 0.972225\n",
      "[Epoch 2136/2500] Loss: 0.059655 | Time: 0.17s | Mean latent norm: 0.972227\n",
      "[Epoch 2137/2500] Loss: 0.058841 | Time: 0.17s | Mean latent norm: 0.972212\n",
      "[Epoch 2138/2500] Loss: 0.058751 | Time: 0.17s | Mean latent norm: 0.972205\n",
      "[Epoch 2139/2500] Loss: 0.058398 | Time: 0.17s | Mean latent norm: 0.972198\n",
      "[Epoch 2140/2500] Loss: 0.058253 | Time: 0.17s | Mean latent norm: 0.972212\n",
      "[Epoch 2141/2500] Loss: 0.058603 | Time: 0.17s | Mean latent norm: 0.972200\n",
      "[Epoch 2142/2500] Loss: 0.059162 | Time: 0.17s | Mean latent norm: 0.972196\n",
      "[Epoch 2143/2500] Loss: 0.059710 | Time: 0.17s | Mean latent norm: 0.972194\n",
      "[Epoch 2144/2500] Loss: 0.059731 | Time: 0.17s | Mean latent norm: 0.972184\n",
      "[Epoch 2145/2500] Loss: 0.060074 | Time: 0.17s | Mean latent norm: 0.972174\n",
      "[Epoch 2146/2500] Loss: 0.058629 | Time: 0.17s | Mean latent norm: 0.972181\n",
      "[Epoch 2147/2500] Loss: 0.061409 | Time: 0.17s | Mean latent norm: 0.972167\n",
      "[Epoch 2148/2500] Loss: 0.067873 | Time: 0.17s | Mean latent norm: 0.972145\n",
      "[Epoch 2149/2500] Loss: 0.063298 | Time: 0.17s | Mean latent norm: 0.972102\n",
      "[Epoch 2150/2500] Loss: 0.061847 | Time: 0.18s | Mean latent norm: 0.972104\n",
      "[Epoch 2151/2500] Loss: 0.061961 | Time: 0.18s | Mean latent norm: 0.972107\n",
      "[Epoch 2152/2500] Loss: 0.060448 | Time: 0.17s | Mean latent norm: 0.972090\n",
      "[Epoch 2153/2500] Loss: 0.061386 | Time: 0.17s | Mean latent norm: 0.972069\n",
      "[Epoch 2154/2500] Loss: 0.059069 | Time: 0.17s | Mean latent norm: 0.972068\n",
      "[Epoch 2155/2500] Loss: 0.059552 | Time: 0.17s | Mean latent norm: 0.972063\n",
      "[Epoch 2156/2500] Loss: 0.060518 | Time: 0.17s | Mean latent norm: 0.972054\n",
      "[Epoch 2157/2500] Loss: 0.059282 | Time: 0.17s | Mean latent norm: 0.972056\n",
      "[Epoch 2158/2500] Loss: 0.061430 | Time: 0.18s | Mean latent norm: 0.972036\n",
      "[Epoch 2159/2500] Loss: 0.062099 | Time: 0.17s | Mean latent norm: 0.972032\n",
      "[Epoch 2160/2500] Loss: 0.064248 | Time: 0.17s | Mean latent norm: 0.972022\n",
      "[Epoch 2161/2500] Loss: 0.066661 | Time: 0.17s | Mean latent norm: 0.972002\n",
      "[Epoch 2162/2500] Loss: 0.063763 | Time: 0.17s | Mean latent norm: 0.971983\n",
      "[Epoch 2163/2500] Loss: 0.063358 | Time: 0.17s | Mean latent norm: 0.971969\n",
      "[Epoch 2164/2500] Loss: 0.063339 | Time: 0.18s | Mean latent norm: 0.971950\n",
      "[Epoch 2165/2500] Loss: 0.058902 | Time: 0.17s | Mean latent norm: 0.971935\n",
      "[Epoch 2166/2500] Loss: 0.059472 | Time: 0.17s | Mean latent norm: 0.971941\n",
      "[Epoch 2167/2500] Loss: 0.059124 | Time: 0.17s | Mean latent norm: 0.971937\n",
      "[Epoch 2168/2500] Loss: 0.058666 | Time: 0.17s | Mean latent norm: 0.971920\n",
      "[Epoch 2169/2500] Loss: 0.059653 | Time: 0.17s | Mean latent norm: 0.971897\n",
      "[Epoch 2170/2500] Loss: 0.059097 | Time: 0.18s | Mean latent norm: 0.971914\n",
      "[Epoch 2171/2500] Loss: 0.059603 | Time: 0.17s | Mean latent norm: 0.971913\n",
      "[Epoch 2172/2500] Loss: 0.058823 | Time: 0.17s | Mean latent norm: 0.971876\n",
      "[Epoch 2173/2500] Loss: 0.059145 | Time: 0.17s | Mean latent norm: 0.971855\n",
      "[Epoch 2174/2500] Loss: 0.058957 | Time: 0.17s | Mean latent norm: 0.971860\n",
      "[Epoch 2175/2500] Loss: 0.058293 | Time: 0.17s | Mean latent norm: 0.971863\n",
      "[Epoch 2176/2500] Loss: 0.059290 | Time: 0.17s | Mean latent norm: 0.971844\n",
      "[Epoch 2177/2500] Loss: 0.058826 | Time: 0.17s | Mean latent norm: 0.971826\n",
      "[Epoch 2178/2500] Loss: 0.058062 | Time: 0.17s | Mean latent norm: 0.971826\n",
      "[Epoch 2179/2500] Loss: 0.058179 | Time: 0.17s | Mean latent norm: 0.971826\n",
      "[Epoch 2180/2500] Loss: 0.058344 | Time: 0.18s | Mean latent norm: 0.971815\n",
      "[Epoch 2181/2500] Loss: 0.058302 | Time: 0.17s | Mean latent norm: 0.971820\n",
      "[Epoch 2182/2500] Loss: 0.059232 | Time: 0.17s | Mean latent norm: 0.971813\n",
      "[Epoch 2183/2500] Loss: 0.060694 | Time: 0.17s | Mean latent norm: 0.971790\n",
      "[Epoch 2184/2500] Loss: 0.060017 | Time: 0.18s | Mean latent norm: 0.971781\n",
      "[Epoch 2185/2500] Loss: 0.060134 | Time: 0.17s | Mean latent norm: 0.971782\n",
      "[Epoch 2186/2500] Loss: 0.060273 | Time: 0.17s | Mean latent norm: 0.971766\n",
      "[Epoch 2187/2500] Loss: 0.058854 | Time: 0.18s | Mean latent norm: 0.971753\n",
      "[Epoch 2188/2500] Loss: 0.058760 | Time: 0.18s | Mean latent norm: 0.971738\n",
      "[Epoch 2189/2500] Loss: 0.058493 | Time: 0.17s | Mean latent norm: 0.971744\n",
      "[Epoch 2190/2500] Loss: 0.059730 | Time: 0.17s | Mean latent norm: 0.971720\n",
      "[Epoch 2191/2500] Loss: 0.059721 | Time: 0.17s | Mean latent norm: 0.971696\n",
      "[Epoch 2192/2500] Loss: 0.059828 | Time: 0.17s | Mean latent norm: 0.971695\n",
      "[Epoch 2193/2500] Loss: 0.059788 | Time: 0.18s | Mean latent norm: 0.971703\n",
      "[Epoch 2194/2500] Loss: 0.058668 | Time: 0.17s | Mean latent norm: 0.971662\n",
      "[Epoch 2195/2500] Loss: 0.059924 | Time: 0.17s | Mean latent norm: 0.971645\n",
      "[Epoch 2196/2500] Loss: 0.059288 | Time: 0.17s | Mean latent norm: 0.971642\n",
      "[Epoch 2197/2500] Loss: 0.059611 | Time: 0.17s | Mean latent norm: 0.971643\n",
      "[Epoch 2198/2500] Loss: 0.058723 | Time: 0.17s | Mean latent norm: 0.971620\n",
      "[Epoch 2199/2500] Loss: 0.058796 | Time: 0.18s | Mean latent norm: 0.971609\n",
      "[Epoch 2200/2500] Loss: 0.059717 | Time: 0.17s | Mean latent norm: 0.971589\n",
      "[Epoch 2201/2500] Loss: 0.059557 | Time: 0.17s | Mean latent norm: 0.971587\n",
      "[Epoch 2202/2500] Loss: 0.060211 | Time: 0.17s | Mean latent norm: 0.971583\n",
      "[Epoch 2203/2500] Loss: 0.058862 | Time: 0.18s | Mean latent norm: 0.971594\n",
      "[Epoch 2204/2500] Loss: 0.059347 | Time: 0.17s | Mean latent norm: 0.971554\n",
      "[Epoch 2205/2500] Loss: 0.058539 | Time: 0.17s | Mean latent norm: 0.971550\n",
      "[Epoch 2206/2500] Loss: 0.058533 | Time: 0.17s | Mean latent norm: 0.971535\n",
      "[Epoch 2207/2500] Loss: 0.057522 | Time: 0.17s | Mean latent norm: 0.971532\n",
      "New best model saved at epoch 2207 with loss 0.057522\n",
      "[Epoch 2208/2500] Loss: 0.057701 | Time: 0.17s | Mean latent norm: 0.971529\n",
      "[Epoch 2209/2500] Loss: 0.058246 | Time: 0.17s | Mean latent norm: 0.971525\n",
      "[Epoch 2210/2500] Loss: 0.057974 | Time: 0.17s | Mean latent norm: 0.971511\n",
      "[Epoch 2211/2500] Loss: 0.058355 | Time: 0.17s | Mean latent norm: 0.971506\n",
      "[Epoch 2212/2500] Loss: 0.058110 | Time: 0.17s | Mean latent norm: 0.971503\n",
      "[Epoch 2213/2500] Loss: 0.057626 | Time: 0.17s | Mean latent norm: 0.971495\n",
      "[Epoch 2214/2500] Loss: 0.057852 | Time: 0.17s | Mean latent norm: 0.971499\n",
      "[Epoch 2215/2500] Loss: 0.057921 | Time: 0.17s | Mean latent norm: 0.971503\n",
      "[Epoch 2216/2500] Loss: 0.058350 | Time: 0.17s | Mean latent norm: 0.971491\n",
      "[Epoch 2217/2500] Loss: 0.057574 | Time: 0.17s | Mean latent norm: 0.971484\n",
      "[Epoch 2218/2500] Loss: 0.058648 | Time: 0.18s | Mean latent norm: 0.971479\n",
      "[Epoch 2219/2500] Loss: 0.058335 | Time: 0.17s | Mean latent norm: 0.971472\n",
      "[Epoch 2220/2500] Loss: 0.058701 | Time: 0.17s | Mean latent norm: 0.971453\n",
      "[Epoch 2221/2500] Loss: 0.059872 | Time: 0.17s | Mean latent norm: 0.971446\n",
      "[Epoch 2222/2500] Loss: 0.058700 | Time: 0.18s | Mean latent norm: 0.971432\n",
      "[Epoch 2223/2500] Loss: 0.058056 | Time: 0.18s | Mean latent norm: 0.971437\n",
      "[Epoch 2224/2500] Loss: 0.057675 | Time: 0.18s | Mean latent norm: 0.971424\n",
      "[Epoch 2225/2500] Loss: 0.057770 | Time: 0.17s | Mean latent norm: 0.971419\n",
      "[Epoch 2226/2500] Loss: 0.057494 | Time: 0.17s | Mean latent norm: 0.971405\n",
      "New best model saved at epoch 2226 with loss 0.057494\n",
      "[Epoch 2227/2500] Loss: 0.057463 | Time: 0.17s | Mean latent norm: 0.971406\n",
      "New best model saved at epoch 2227 with loss 0.057463\n",
      "[Epoch 2228/2500] Loss: 0.058619 | Time: 0.17s | Mean latent norm: 0.971386\n",
      "[Epoch 2229/2500] Loss: 0.058337 | Time: 0.18s | Mean latent norm: 0.971382\n",
      "[Epoch 2230/2500] Loss: 0.058734 | Time: 0.17s | Mean latent norm: 0.971371\n",
      "[Epoch 2231/2500] Loss: 0.059907 | Time: 0.17s | Mean latent norm: 0.971354\n",
      "[Epoch 2232/2500] Loss: 0.060577 | Time: 0.17s | Mean latent norm: 0.971332\n",
      "[Epoch 2233/2500] Loss: 0.060140 | Time: 0.17s | Mean latent norm: 0.971327\n",
      "[Epoch 2234/2500] Loss: 0.060383 | Time: 0.17s | Mean latent norm: 0.971303\n",
      "[Epoch 2235/2500] Loss: 0.059146 | Time: 0.18s | Mean latent norm: 0.971305\n",
      "[Epoch 2236/2500] Loss: 0.059356 | Time: 0.17s | Mean latent norm: 0.971288\n",
      "[Epoch 2237/2500] Loss: 0.062479 | Time: 0.17s | Mean latent norm: 0.971281\n",
      "[Epoch 2238/2500] Loss: 0.061690 | Time: 0.17s | Mean latent norm: 0.971276\n",
      "[Epoch 2239/2500] Loss: 0.062072 | Time: 0.17s | Mean latent norm: 0.971251\n",
      "[Epoch 2240/2500] Loss: 0.061069 | Time: 0.18s | Mean latent norm: 0.971242\n",
      "[Epoch 2241/2500] Loss: 0.059029 | Time: 0.18s | Mean latent norm: 0.971230\n",
      "[Epoch 2242/2500] Loss: 0.057730 | Time: 0.17s | Mean latent norm: 0.971225\n",
      "[Epoch 2243/2500] Loss: 0.059651 | Time: 0.17s | Mean latent norm: 0.971193\n",
      "[Epoch 2244/2500] Loss: 0.059173 | Time: 0.17s | Mean latent norm: 0.971191\n",
      "[Epoch 2245/2500] Loss: 0.058557 | Time: 0.17s | Mean latent norm: 0.971191\n",
      "[Epoch 2246/2500] Loss: 0.058496 | Time: 0.17s | Mean latent norm: 0.971186\n",
      "[Epoch 2247/2500] Loss: 0.058570 | Time: 0.17s | Mean latent norm: 0.971176\n",
      "[Epoch 2248/2500] Loss: 0.057944 | Time: 0.17s | Mean latent norm: 0.971174\n",
      "[Epoch 2249/2500] Loss: 0.057881 | Time: 0.17s | Mean latent norm: 0.971167\n",
      "[Epoch 2250/2500] Loss: 0.058006 | Time: 0.17s | Mean latent norm: 0.971167\n",
      "[Epoch 2251/2500] Loss: 0.058730 | Time: 0.17s | Mean latent norm: 0.971152\n",
      "[Epoch 2252/2500] Loss: 0.058334 | Time: 0.17s | Mean latent norm: 0.971158\n",
      "[Epoch 2253/2500] Loss: 0.058504 | Time: 0.17s | Mean latent norm: 0.971133\n",
      "[Epoch 2254/2500] Loss: 0.058542 | Time: 0.18s | Mean latent norm: 0.971128\n",
      "[Epoch 2255/2500] Loss: 0.060420 | Time: 0.17s | Mean latent norm: 0.971113\n",
      "[Epoch 2256/2500] Loss: 0.061654 | Time: 0.18s | Mean latent norm: 0.971111\n",
      "[Epoch 2257/2500] Loss: 0.058908 | Time: 0.17s | Mean latent norm: 0.971095\n",
      "[Epoch 2258/2500] Loss: 0.058952 | Time: 0.17s | Mean latent norm: 0.971087\n",
      "[Epoch 2259/2500] Loss: 0.058344 | Time: 0.17s | Mean latent norm: 0.971061\n",
      "[Epoch 2260/2500] Loss: 0.058083 | Time: 0.17s | Mean latent norm: 0.971062\n",
      "[Epoch 2261/2500] Loss: 0.058132 | Time: 0.17s | Mean latent norm: 0.971058\n",
      "[Epoch 2262/2500] Loss: 0.058217 | Time: 0.18s | Mean latent norm: 0.971045\n",
      "[Epoch 2263/2500] Loss: 0.059520 | Time: 0.17s | Mean latent norm: 0.971049\n",
      "[Epoch 2264/2500] Loss: 0.062084 | Time: 0.17s | Mean latent norm: 0.971041\n",
      "[Epoch 2265/2500] Loss: 0.060164 | Time: 0.17s | Mean latent norm: 0.971011\n",
      "[Epoch 2266/2500] Loss: 0.058652 | Time: 0.17s | Mean latent norm: 0.970989\n",
      "[Epoch 2267/2500] Loss: 0.059429 | Time: 0.17s | Mean latent norm: 0.970987\n",
      "[Epoch 2268/2500] Loss: 0.060852 | Time: 0.17s | Mean latent norm: 0.970992\n",
      "[Epoch 2269/2500] Loss: 0.058228 | Time: 0.17s | Mean latent norm: 0.970966\n",
      "[Epoch 2270/2500] Loss: 0.058019 | Time: 0.17s | Mean latent norm: 0.970952\n",
      "[Epoch 2271/2500] Loss: 0.058736 | Time: 0.17s | Mean latent norm: 0.970934\n",
      "[Epoch 2272/2500] Loss: 0.059174 | Time: 0.17s | Mean latent norm: 0.970944\n",
      "[Epoch 2273/2500] Loss: 0.058996 | Time: 0.17s | Mean latent norm: 0.970914\n",
      "[Epoch 2274/2500] Loss: 0.058653 | Time: 0.17s | Mean latent norm: 0.970907\n",
      "[Epoch 2275/2500] Loss: 0.058994 | Time: 0.17s | Mean latent norm: 0.970898\n",
      "[Epoch 2276/2500] Loss: 0.057885 | Time: 0.17s | Mean latent norm: 0.970885\n",
      "[Epoch 2277/2500] Loss: 0.058098 | Time: 0.17s | Mean latent norm: 0.970879\n",
      "[Epoch 2278/2500] Loss: 0.057610 | Time: 0.17s | Mean latent norm: 0.970884\n",
      "[Epoch 2279/2500] Loss: 0.058554 | Time: 0.17s | Mean latent norm: 0.970881\n",
      "[Epoch 2280/2500] Loss: 0.059738 | Time: 0.17s | Mean latent norm: 0.970859\n",
      "[Epoch 2281/2500] Loss: 0.059955 | Time: 0.17s | Mean latent norm: 0.970851\n",
      "[Epoch 2282/2500] Loss: 0.059946 | Time: 0.17s | Mean latent norm: 0.970819\n",
      "[Epoch 2283/2500] Loss: 0.061918 | Time: 0.17s | Mean latent norm: 0.970812\n",
      "[Epoch 2284/2500] Loss: 0.060634 | Time: 0.18s | Mean latent norm: 0.970802\n",
      "[Epoch 2285/2500] Loss: 0.062265 | Time: 0.17s | Mean latent norm: 0.970807\n",
      "[Epoch 2286/2500] Loss: 0.061292 | Time: 0.17s | Mean latent norm: 0.970776\n",
      "[Epoch 2287/2500] Loss: 0.062324 | Time: 0.17s | Mean latent norm: 0.970762\n",
      "[Epoch 2288/2500] Loss: 0.063350 | Time: 0.18s | Mean latent norm: 0.970709\n",
      "[Epoch 2289/2500] Loss: 0.060953 | Time: 0.17s | Mean latent norm: 0.970687\n",
      "[Epoch 2290/2500] Loss: 0.061197 | Time: 0.17s | Mean latent norm: 0.970674\n",
      "[Epoch 2291/2500] Loss: 0.063126 | Time: 0.17s | Mean latent norm: 0.970655\n",
      "[Epoch 2292/2500] Loss: 0.061937 | Time: 0.17s | Mean latent norm: 0.970658\n",
      "[Epoch 2293/2500] Loss: 0.060415 | Time: 0.17s | Mean latent norm: 0.970629\n",
      "[Epoch 2294/2500] Loss: 0.060641 | Time: 0.17s | Mean latent norm: 0.970615\n",
      "[Epoch 2295/2500] Loss: 0.059456 | Time: 0.17s | Mean latent norm: 0.970612\n",
      "[Epoch 2296/2500] Loss: 0.060827 | Time: 0.17s | Mean latent norm: 0.970601\n",
      "[Epoch 2297/2500] Loss: 0.061772 | Time: 0.17s | Mean latent norm: 0.970576\n",
      "[Epoch 2298/2500] Loss: 0.064616 | Time: 0.17s | Mean latent norm: 0.970556\n",
      "[Epoch 2299/2500] Loss: 0.067672 | Time: 0.17s | Mean latent norm: 0.970528\n",
      "[Epoch 2300/2500] Loss: 0.066312 | Time: 0.18s | Mean latent norm: 0.970518\n",
      "[Epoch 2301/2500] Loss: 0.065846 | Time: 0.17s | Mean latent norm: 0.970502\n",
      "[Epoch 2302/2500] Loss: 0.070308 | Time: 0.17s | Mean latent norm: 0.970517\n",
      "[Epoch 2303/2500] Loss: 0.072781 | Time: 0.17s | Mean latent norm: 0.970489\n",
      "[Epoch 2304/2500] Loss: 0.069616 | Time: 0.17s | Mean latent norm: 0.970469\n",
      "[Epoch 2305/2500] Loss: 0.063985 | Time: 0.17s | Mean latent norm: 0.970431\n",
      "[Epoch 2306/2500] Loss: 0.062738 | Time: 0.18s | Mean latent norm: 0.970420\n",
      "[Epoch 2307/2500] Loss: 0.065346 | Time: 0.17s | Mean latent norm: 0.970420\n",
      "[Epoch 2308/2500] Loss: 0.068834 | Time: 0.17s | Mean latent norm: 0.970396\n",
      "[Epoch 2309/2500] Loss: 0.060482 | Time: 0.17s | Mean latent norm: 0.970371\n",
      "[Epoch 2310/2500] Loss: 0.058877 | Time: 0.17s | Mean latent norm: 0.970356\n",
      "[Epoch 2311/2500] Loss: 0.057926 | Time: 0.17s | Mean latent norm: 0.970356\n",
      "[Epoch 2312/2500] Loss: 0.057445 | Time: 0.17s | Mean latent norm: 0.970351\n",
      "New best model saved at epoch 2312 with loss 0.057445\n",
      "[Epoch 2313/2500] Loss: 0.057949 | Time: 0.17s | Mean latent norm: 0.970363\n",
      "[Epoch 2314/2500] Loss: 0.057145 | Time: 0.17s | Mean latent norm: 0.970345\n",
      "New best model saved at epoch 2314 with loss 0.057145\n",
      "[Epoch 2315/2500] Loss: 0.057588 | Time: 0.17s | Mean latent norm: 0.970337\n",
      "[Epoch 2316/2500] Loss: 0.057060 | Time: 0.17s | Mean latent norm: 0.970330\n",
      "New best model saved at epoch 2316 with loss 0.057060\n",
      "[Epoch 2317/2500] Loss: 0.057104 | Time: 0.17s | Mean latent norm: 0.970332\n",
      "[Epoch 2318/2500] Loss: 0.057531 | Time: 0.17s | Mean latent norm: 0.970322\n",
      "[Epoch 2319/2500] Loss: 0.057970 | Time: 0.17s | Mean latent norm: 0.970324\n",
      "[Epoch 2320/2500] Loss: 0.058250 | Time: 0.17s | Mean latent norm: 0.970306\n",
      "[Epoch 2321/2500] Loss: 0.057684 | Time: 0.17s | Mean latent norm: 0.970305\n",
      "[Epoch 2322/2500] Loss: 0.057731 | Time: 0.17s | Mean latent norm: 0.970302\n",
      "[Epoch 2323/2500] Loss: 0.058857 | Time: 0.18s | Mean latent norm: 0.970300\n",
      "[Epoch 2324/2500] Loss: 0.056748 | Time: 0.17s | Mean latent norm: 0.970287\n",
      "New best model saved at epoch 2324 with loss 0.056748\n",
      "[Epoch 2325/2500] Loss: 0.057646 | Time: 0.18s | Mean latent norm: 0.970277\n",
      "[Epoch 2326/2500] Loss: 0.058482 | Time: 0.17s | Mean latent norm: 0.970275\n",
      "[Epoch 2327/2500] Loss: 0.057932 | Time: 0.17s | Mean latent norm: 0.970264\n",
      "[Epoch 2328/2500] Loss: 0.057884 | Time: 0.17s | Mean latent norm: 0.970255\n",
      "[Epoch 2329/2500] Loss: 0.058671 | Time: 0.17s | Mean latent norm: 0.970267\n",
      "[Epoch 2330/2500] Loss: 0.059980 | Time: 0.17s | Mean latent norm: 0.970264\n",
      "[Epoch 2331/2500] Loss: 0.057435 | Time: 0.18s | Mean latent norm: 0.970246\n",
      "[Epoch 2332/2500] Loss: 0.060848 | Time: 0.17s | Mean latent norm: 0.970220\n",
      "[Epoch 2333/2500] Loss: 0.058986 | Time: 0.17s | Mean latent norm: 0.970222\n",
      "[Epoch 2334/2500] Loss: 0.057249 | Time: 0.17s | Mean latent norm: 0.970212\n",
      "[Epoch 2335/2500] Loss: 0.057345 | Time: 0.17s | Mean latent norm: 0.970199\n",
      "[Epoch 2336/2500] Loss: 0.057092 | Time: 0.17s | Mean latent norm: 0.970200\n",
      "[Epoch 2337/2500] Loss: 0.056892 | Time: 0.17s | Mean latent norm: 0.970193\n",
      "[Epoch 2338/2500] Loss: 0.058457 | Time: 0.17s | Mean latent norm: 0.970181\n",
      "[Epoch 2339/2500] Loss: 0.057743 | Time: 0.17s | Mean latent norm: 0.970173\n",
      "[Epoch 2340/2500] Loss: 0.058014 | Time: 0.17s | Mean latent norm: 0.970176\n",
      "[Epoch 2341/2500] Loss: 0.057747 | Time: 0.18s | Mean latent norm: 0.970174\n",
      "[Epoch 2342/2500] Loss: 0.057180 | Time: 0.17s | Mean latent norm: 0.970154\n",
      "[Epoch 2343/2500] Loss: 0.057326 | Time: 0.17s | Mean latent norm: 0.970135\n",
      "[Epoch 2344/2500] Loss: 0.057526 | Time: 0.17s | Mean latent norm: 0.970138\n",
      "[Epoch 2345/2500] Loss: 0.057127 | Time: 0.18s | Mean latent norm: 0.970135\n",
      "[Epoch 2346/2500] Loss: 0.057196 | Time: 0.18s | Mean latent norm: 0.970129\n",
      "[Epoch 2347/2500] Loss: 0.058208 | Time: 0.18s | Mean latent norm: 0.970111\n",
      "[Epoch 2348/2500] Loss: 0.060334 | Time: 0.17s | Mean latent norm: 0.970110\n",
      "[Epoch 2349/2500] Loss: 0.058600 | Time: 0.18s | Mean latent norm: 0.970100\n",
      "[Epoch 2350/2500] Loss: 0.059676 | Time: 0.17s | Mean latent norm: 0.970088\n",
      "[Epoch 2351/2500] Loss: 0.059145 | Time: 0.17s | Mean latent norm: 0.970075\n",
      "[Epoch 2352/2500] Loss: 0.058342 | Time: 0.17s | Mean latent norm: 0.970059\n",
      "[Epoch 2353/2500] Loss: 0.057833 | Time: 0.18s | Mean latent norm: 0.970057\n",
      "[Epoch 2354/2500] Loss: 0.058968 | Time: 0.17s | Mean latent norm: 0.970035\n",
      "[Epoch 2355/2500] Loss: 0.060685 | Time: 0.17s | Mean latent norm: 0.970043\n",
      "[Epoch 2356/2500] Loss: 0.059541 | Time: 0.17s | Mean latent norm: 0.970025\n",
      "[Epoch 2357/2500] Loss: 0.058886 | Time: 0.18s | Mean latent norm: 0.970006\n",
      "[Epoch 2358/2500] Loss: 0.058701 | Time: 0.18s | Mean latent norm: 0.969992\n",
      "[Epoch 2359/2500] Loss: 0.059474 | Time: 0.17s | Mean latent norm: 0.969992\n",
      "[Epoch 2360/2500] Loss: 0.060919 | Time: 0.17s | Mean latent norm: 0.969990\n",
      "[Epoch 2361/2500] Loss: 0.059345 | Time: 0.17s | Mean latent norm: 0.969961\n",
      "[Epoch 2362/2500] Loss: 0.058612 | Time: 0.17s | Mean latent norm: 0.969947\n",
      "[Epoch 2363/2500] Loss: 0.056748 | Time: 0.18s | Mean latent norm: 0.969938\n",
      "New best model saved at epoch 2363 with loss 0.056748\n",
      "[Epoch 2364/2500] Loss: 0.056897 | Time: 0.18s | Mean latent norm: 0.969941\n",
      "[Epoch 2365/2500] Loss: 0.056045 | Time: 0.18s | Mean latent norm: 0.969939\n",
      "New best model saved at epoch 2365 with loss 0.056045\n",
      "[Epoch 2366/2500] Loss: 0.056239 | Time: 0.17s | Mean latent norm: 0.969936\n",
      "[Epoch 2367/2500] Loss: 0.056439 | Time: 0.17s | Mean latent norm: 0.969939\n",
      "[Epoch 2368/2500] Loss: 0.056663 | Time: 0.17s | Mean latent norm: 0.969927\n",
      "[Epoch 2369/2500] Loss: 0.056576 | Time: 0.18s | Mean latent norm: 0.969922\n",
      "[Epoch 2370/2500] Loss: 0.058078 | Time: 0.18s | Mean latent norm: 0.969926\n",
      "[Epoch 2371/2500] Loss: 0.057163 | Time: 0.17s | Mean latent norm: 0.969924\n",
      "[Epoch 2372/2500] Loss: 0.056477 | Time: 0.17s | Mean latent norm: 0.969896\n",
      "[Epoch 2373/2500] Loss: 0.057922 | Time: 0.18s | Mean latent norm: 0.969897\n",
      "[Epoch 2374/2500] Loss: 0.058139 | Time: 0.17s | Mean latent norm: 0.969878\n",
      "[Epoch 2375/2500] Loss: 0.058189 | Time: 0.17s | Mean latent norm: 0.969888\n",
      "[Epoch 2376/2500] Loss: 0.058049 | Time: 0.17s | Mean latent norm: 0.969864\n",
      "[Epoch 2377/2500] Loss: 0.057376 | Time: 0.17s | Mean latent norm: 0.969870\n",
      "[Epoch 2378/2500] Loss: 0.056984 | Time: 0.17s | Mean latent norm: 0.969848\n",
      "[Epoch 2379/2500] Loss: 0.057153 | Time: 0.17s | Mean latent norm: 0.969845\n",
      "[Epoch 2380/2500] Loss: 0.057906 | Time: 0.17s | Mean latent norm: 0.969829\n",
      "[Epoch 2381/2500] Loss: 0.057788 | Time: 0.18s | Mean latent norm: 0.969825\n",
      "[Epoch 2382/2500] Loss: 0.057473 | Time: 0.17s | Mean latent norm: 0.969810\n",
      "[Epoch 2383/2500] Loss: 0.059149 | Time: 0.17s | Mean latent norm: 0.969813\n",
      "[Epoch 2384/2500] Loss: 0.059399 | Time: 0.17s | Mean latent norm: 0.969811\n",
      "[Epoch 2385/2500] Loss: 0.058722 | Time: 0.18s | Mean latent norm: 0.969782\n",
      "[Epoch 2386/2500] Loss: 0.059287 | Time: 0.17s | Mean latent norm: 0.969760\n",
      "[Epoch 2387/2500] Loss: 0.059081 | Time: 0.17s | Mean latent norm: 0.969754\n",
      "[Epoch 2388/2500] Loss: 0.058935 | Time: 0.18s | Mean latent norm: 0.969747\n",
      "[Epoch 2389/2500] Loss: 0.058738 | Time: 0.17s | Mean latent norm: 0.969737\n",
      "[Epoch 2390/2500] Loss: 0.057514 | Time: 0.18s | Mean latent norm: 0.969709\n",
      "[Epoch 2391/2500] Loss: 0.057814 | Time: 0.18s | Mean latent norm: 0.969701\n",
      "[Epoch 2392/2500] Loss: 0.056050 | Time: 0.18s | Mean latent norm: 0.969707\n",
      "[Epoch 2393/2500] Loss: 0.056256 | Time: 0.18s | Mean latent norm: 0.969707\n",
      "[Epoch 2394/2500] Loss: 0.056230 | Time: 0.18s | Mean latent norm: 0.969686\n",
      "[Epoch 2395/2500] Loss: 0.056127 | Time: 0.18s | Mean latent norm: 0.969692\n",
      "[Epoch 2396/2500] Loss: 0.056082 | Time: 0.17s | Mean latent norm: 0.969692\n",
      "[Epoch 2397/2500] Loss: 0.056409 | Time: 0.18s | Mean latent norm: 0.969684\n",
      "[Epoch 2398/2500] Loss: 0.055981 | Time: 0.17s | Mean latent norm: 0.969669\n",
      "New best model saved at epoch 2398 with loss 0.055981\n",
      "[Epoch 2399/2500] Loss: 0.056164 | Time: 0.17s | Mean latent norm: 0.969676\n",
      "[Epoch 2400/2500] Loss: 0.056774 | Time: 0.17s | Mean latent norm: 0.969667\n",
      "[Epoch 2401/2500] Loss: 0.057191 | Time: 0.17s | Mean latent norm: 0.969651\n",
      "[Epoch 2402/2500] Loss: 0.059602 | Time: 0.17s | Mean latent norm: 0.969648\n",
      "[Epoch 2403/2500] Loss: 0.057680 | Time: 0.17s | Mean latent norm: 0.969647\n",
      "[Epoch 2404/2500] Loss: 0.059126 | Time: 0.18s | Mean latent norm: 0.969627\n",
      "[Epoch 2405/2500] Loss: 0.058852 | Time: 0.17s | Mean latent norm: 0.969616\n",
      "[Epoch 2406/2500] Loss: 0.057555 | Time: 0.18s | Mean latent norm: 0.969602\n",
      "[Epoch 2407/2500] Loss: 0.057716 | Time: 0.17s | Mean latent norm: 0.969578\n",
      "[Epoch 2408/2500] Loss: 0.058087 | Time: 0.17s | Mean latent norm: 0.969568\n",
      "[Epoch 2409/2500] Loss: 0.057012 | Time: 0.17s | Mean latent norm: 0.969575\n",
      "[Epoch 2410/2500] Loss: 0.058372 | Time: 0.17s | Mean latent norm: 0.969557\n",
      "[Epoch 2411/2500] Loss: 0.058922 | Time: 0.17s | Mean latent norm: 0.969542\n",
      "[Epoch 2412/2500] Loss: 0.057248 | Time: 0.18s | Mean latent norm: 0.969515\n",
      "[Epoch 2413/2500] Loss: 0.060603 | Time: 0.17s | Mean latent norm: 0.969517\n",
      "[Epoch 2414/2500] Loss: 0.060031 | Time: 0.17s | Mean latent norm: 0.969509\n",
      "[Epoch 2415/2500] Loss: 0.059180 | Time: 0.17s | Mean latent norm: 0.969488\n",
      "[Epoch 2416/2500] Loss: 0.063799 | Time: 0.17s | Mean latent norm: 0.969473\n",
      "[Epoch 2417/2500] Loss: 0.061594 | Time: 0.17s | Mean latent norm: 0.969449\n",
      "[Epoch 2418/2500] Loss: 0.060772 | Time: 0.18s | Mean latent norm: 0.969434\n",
      "[Epoch 2419/2500] Loss: 0.063396 | Time: 0.17s | Mean latent norm: 0.969427\n",
      "[Epoch 2420/2500] Loss: 0.064423 | Time: 0.17s | Mean latent norm: 0.969411\n",
      "[Epoch 2421/2500] Loss: 0.064933 | Time: 0.17s | Mean latent norm: 0.969397\n",
      "[Epoch 2422/2500] Loss: 0.071498 | Time: 0.17s | Mean latent norm: 0.969364\n",
      "[Epoch 2423/2500] Loss: 0.066336 | Time: 0.17s | Mean latent norm: 0.969320\n",
      "[Epoch 2424/2500] Loss: 0.067339 | Time: 0.18s | Mean latent norm: 0.969298\n",
      "[Epoch 2425/2500] Loss: 0.062600 | Time: 0.17s | Mean latent norm: 0.969295\n",
      "[Epoch 2426/2500] Loss: 0.062072 | Time: 0.17s | Mean latent norm: 0.969277\n",
      "[Epoch 2427/2500] Loss: 0.058756 | Time: 0.17s | Mean latent norm: 0.969266\n",
      "[Epoch 2428/2500] Loss: 0.057346 | Time: 0.17s | Mean latent norm: 0.969258\n",
      "[Epoch 2429/2500] Loss: 0.058771 | Time: 0.17s | Mean latent norm: 0.969251\n",
      "[Epoch 2430/2500] Loss: 0.059190 | Time: 0.17s | Mean latent norm: 0.969227\n",
      "[Epoch 2431/2500] Loss: 0.060922 | Time: 0.17s | Mean latent norm: 0.969248\n",
      "[Epoch 2432/2500] Loss: 0.061649 | Time: 0.17s | Mean latent norm: 0.969238\n",
      "[Epoch 2433/2500] Loss: 0.060243 | Time: 0.17s | Mean latent norm: 0.969210\n",
      "[Epoch 2434/2500] Loss: 0.060325 | Time: 0.17s | Mean latent norm: 0.969181\n",
      "[Epoch 2435/2500] Loss: 0.062531 | Time: 0.17s | Mean latent norm: 0.969181\n",
      "[Epoch 2436/2500] Loss: 0.059376 | Time: 0.18s | Mean latent norm: 0.969176\n",
      "[Epoch 2437/2500] Loss: 0.061368 | Time: 0.17s | Mean latent norm: 0.969161\n",
      "[Epoch 2438/2500] Loss: 0.061214 | Time: 0.17s | Mean latent norm: 0.969123\n",
      "[Epoch 2439/2500] Loss: 0.063104 | Time: 0.17s | Mean latent norm: 0.969142\n",
      "[Epoch 2440/2500] Loss: 0.060749 | Time: 0.17s | Mean latent norm: 0.969125\n",
      "[Epoch 2441/2500] Loss: 0.060029 | Time: 0.17s | Mean latent norm: 0.969106\n",
      "[Epoch 2442/2500] Loss: 0.057445 | Time: 0.17s | Mean latent norm: 0.969110\n",
      "[Epoch 2443/2500] Loss: 0.057045 | Time: 0.18s | Mean latent norm: 0.969104\n",
      "[Epoch 2444/2500] Loss: 0.057880 | Time: 0.17s | Mean latent norm: 0.969080\n",
      "[Epoch 2445/2500] Loss: 0.058049 | Time: 0.17s | Mean latent norm: 0.969084\n",
      "[Epoch 2446/2500] Loss: 0.057276 | Time: 0.17s | Mean latent norm: 0.969073\n",
      "[Epoch 2447/2500] Loss: 0.057172 | Time: 0.17s | Mean latent norm: 0.969057\n",
      "[Epoch 2448/2500] Loss: 0.057042 | Time: 0.17s | Mean latent norm: 0.969060\n",
      "[Epoch 2449/2500] Loss: 0.057775 | Time: 0.17s | Mean latent norm: 0.969065\n",
      "[Epoch 2450/2500] Loss: 0.057314 | Time: 0.17s | Mean latent norm: 0.969043\n",
      "[Epoch 2451/2500] Loss: 0.058647 | Time: 0.17s | Mean latent norm: 0.969035\n",
      "[Epoch 2452/2500] Loss: 0.057619 | Time: 0.17s | Mean latent norm: 0.969032\n",
      "[Epoch 2453/2500] Loss: 0.057377 | Time: 0.17s | Mean latent norm: 0.969010\n",
      "[Epoch 2454/2500] Loss: 0.057978 | Time: 0.17s | Mean latent norm: 0.968992\n",
      "[Epoch 2455/2500] Loss: 0.058281 | Time: 0.17s | Mean latent norm: 0.968980\n",
      "[Epoch 2456/2500] Loss: 0.057607 | Time: 0.17s | Mean latent norm: 0.968994\n",
      "[Epoch 2457/2500] Loss: 0.057638 | Time: 0.17s | Mean latent norm: 0.968981\n",
      "[Epoch 2458/2500] Loss: 0.056142 | Time: 0.17s | Mean latent norm: 0.968964\n",
      "[Epoch 2459/2500] Loss: 0.056369 | Time: 0.17s | Mean latent norm: 0.968971\n",
      "[Epoch 2460/2500] Loss: 0.056615 | Time: 0.17s | Mean latent norm: 0.968968\n",
      "[Epoch 2461/2500] Loss: 0.056279 | Time: 0.17s | Mean latent norm: 0.968954\n",
      "[Epoch 2462/2500] Loss: 0.056673 | Time: 0.18s | Mean latent norm: 0.968949\n",
      "[Epoch 2463/2500] Loss: 0.056943 | Time: 0.17s | Mean latent norm: 0.968946\n",
      "[Epoch 2464/2500] Loss: 0.057529 | Time: 0.17s | Mean latent norm: 0.968928\n",
      "[Epoch 2465/2500] Loss: 0.058331 | Time: 0.17s | Mean latent norm: 0.968915\n",
      "[Epoch 2466/2500] Loss: 0.056142 | Time: 0.17s | Mean latent norm: 0.968907\n",
      "[Epoch 2467/2500] Loss: 0.055988 | Time: 0.17s | Mean latent norm: 0.968892\n",
      "[Epoch 2468/2500] Loss: 0.055639 | Time: 0.17s | Mean latent norm: 0.968895\n",
      "New best model saved at epoch 2468 with loss 0.055639\n",
      "[Epoch 2469/2500] Loss: 0.055319 | Time: 0.17s | Mean latent norm: 0.968890\n",
      "New best model saved at epoch 2469 with loss 0.055319\n",
      "[Epoch 2470/2500] Loss: 0.055200 | Time: 0.17s | Mean latent norm: 0.968876\n",
      "New best model saved at epoch 2470 with loss 0.055200\n",
      "[Epoch 2471/2500] Loss: 0.055856 | Time: 0.17s | Mean latent norm: 0.968879\n",
      "[Epoch 2472/2500] Loss: 0.056313 | Time: 0.17s | Mean latent norm: 0.968882\n",
      "[Epoch 2473/2500] Loss: 0.055770 | Time: 0.17s | Mean latent norm: 0.968873\n",
      "[Epoch 2474/2500] Loss: 0.055973 | Time: 0.17s | Mean latent norm: 0.968867\n",
      "[Epoch 2475/2500] Loss: 0.055738 | Time: 0.17s | Mean latent norm: 0.968855\n",
      "[Epoch 2476/2500] Loss: 0.056076 | Time: 0.17s | Mean latent norm: 0.968851\n",
      "[Epoch 2477/2500] Loss: 0.056371 | Time: 0.17s | Mean latent norm: 0.968852\n",
      "[Epoch 2478/2500] Loss: 0.057990 | Time: 0.17s | Mean latent norm: 0.968841\n",
      "[Epoch 2479/2500] Loss: 0.057393 | Time: 0.17s | Mean latent norm: 0.968833\n",
      "[Epoch 2480/2500] Loss: 0.057393 | Time: 0.17s | Mean latent norm: 0.968813\n",
      "[Epoch 2481/2500] Loss: 0.058835 | Time: 0.18s | Mean latent norm: 0.968803\n",
      "[Epoch 2482/2500] Loss: 0.057987 | Time: 0.17s | Mean latent norm: 0.968797\n",
      "[Epoch 2483/2500] Loss: 0.058801 | Time: 0.17s | Mean latent norm: 0.968799\n",
      "[Epoch 2484/2500] Loss: 0.058066 | Time: 0.17s | Mean latent norm: 0.968778\n",
      "[Epoch 2485/2500] Loss: 0.056595 | Time: 0.17s | Mean latent norm: 0.968761\n",
      "[Epoch 2486/2500] Loss: 0.057009 | Time: 0.17s | Mean latent norm: 0.968769\n",
      "[Epoch 2487/2500] Loss: 0.062267 | Time: 0.17s | Mean latent norm: 0.968746\n",
      "[Epoch 2488/2500] Loss: 0.060472 | Time: 0.17s | Mean latent norm: 0.968724\n",
      "[Epoch 2489/2500] Loss: 0.058606 | Time: 0.17s | Mean latent norm: 0.968712\n",
      "[Epoch 2490/2500] Loss: 0.064831 | Time: 0.17s | Mean latent norm: 0.968705\n",
      "[Epoch 2491/2500] Loss: 0.062842 | Time: 0.17s | Mean latent norm: 0.968678\n",
      "[Epoch 2492/2500] Loss: 0.059473 | Time: 0.17s | Mean latent norm: 0.968656\n",
      "[Epoch 2493/2500] Loss: 0.062276 | Time: 0.17s | Mean latent norm: 0.968657\n",
      "[Epoch 2494/2500] Loss: 0.062464 | Time: 0.17s | Mean latent norm: 0.968648\n",
      "[Epoch 2495/2500] Loss: 0.065197 | Time: 0.17s | Mean latent norm: 0.968615\n",
      "[Epoch 2496/2500] Loss: 0.063557 | Time: 0.17s | Mean latent norm: 0.968618\n",
      "[Epoch 2497/2500] Loss: 0.063534 | Time: 0.17s | Mean latent norm: 0.968588\n",
      "[Epoch 2498/2500] Loss: 0.060309 | Time: 0.17s | Mean latent norm: 0.968585\n",
      "[Epoch 2499/2500] Loss: 0.058032 | Time: 0.17s | Mean latent norm: 0.968581\n",
      "[Epoch 2500/2500] Loss: 0.056903 | Time: 0.17s | Mean latent norm: 0.968574\n",
      "Loss curve saved to training_loss_curve.png\n"
     ]
    }
   ],
   "source": [
    "# Training:\n",
    "!python /home/lucasp/thesis/main.py --npz_folder /home/lucasp/thesis/npz_output --num_epochs 2500 --do_code_regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4531ba90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 shapes in dataset.\n",
      "/home/lucasp/miniconda3/envs/nnunet/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "[Epoch 1/500] Loss: 0.358405 | Time: 0.22s | Mean latent norm: 0.971923\n",
      "Average difference with latent perturbation: 0.007071135565638542\n",
      "New best model saved at epoch 1 with loss 0.358405\n",
      "[Epoch 2/500] Loss: 0.357094 | Time: 0.03s | Mean latent norm: 0.971923\n",
      "New best model saved at epoch 2 with loss 0.357094\n",
      "[Epoch 3/500] Loss: 0.355518 | Time: 0.03s | Mean latent norm: 0.971931\n",
      "New best model saved at epoch 3 with loss 0.355518\n",
      "[Epoch 4/500] Loss: 0.354092 | Time: 0.02s | Mean latent norm: 0.971950\n",
      "New best model saved at epoch 4 with loss 0.354092\n",
      "[Epoch 5/500] Loss: 0.352720 | Time: 0.02s | Mean latent norm: 0.971976\n",
      "New best model saved at epoch 5 with loss 0.352720\n",
      "[Epoch 6/500] Loss: 0.351168 | Time: 0.02s | Mean latent norm: 0.972010\n",
      "New best model saved at epoch 6 with loss 0.351168\n",
      "[Epoch 7/500] Loss: 0.349740 | Time: 0.03s | Mean latent norm: 0.972050\n",
      "New best model saved at epoch 7 with loss 0.349740\n",
      "[Epoch 8/500] Loss: 0.348158 | Time: 0.02s | Mean latent norm: 0.972096\n",
      "New best model saved at epoch 8 with loss 0.348158\n",
      "[Epoch 9/500] Loss: 0.346830 | Time: 0.03s | Mean latent norm: 0.972143\n",
      "New best model saved at epoch 9 with loss 0.346830\n",
      "[Epoch 10/500] Loss: 0.347023 | Time: 0.02s | Mean latent norm: 0.972158\n",
      "[Epoch 11/500] Loss: 0.348000 | Time: 0.03s | Mean latent norm: 0.972138\n",
      "Average difference with latent perturbation: 0.009141332469880581\n",
      "[Epoch 12/500] Loss: 0.348523 | Time: 0.03s | Mean latent norm: 0.972099\n",
      "[Epoch 13/500] Loss: 0.348496 | Time: 0.02s | Mean latent norm: 0.972048\n",
      "[Epoch 14/500] Loss: 0.347914 | Time: 0.03s | Mean latent norm: 0.971991\n",
      "[Epoch 15/500] Loss: 0.347188 | Time: 0.02s | Mean latent norm: 0.971932\n",
      "[Epoch 16/500] Loss: 0.346564 | Time: 0.02s | Mean latent norm: 0.971873\n",
      "New best model saved at epoch 16 with loss 0.346564\n",
      "[Epoch 17/500] Loss: 0.346321 | Time: 0.02s | Mean latent norm: 0.971820\n",
      "New best model saved at epoch 17 with loss 0.346321\n",
      "[Epoch 18/500] Loss: 0.346445 | Time: 0.02s | Mean latent norm: 0.971772\n",
      "[Epoch 19/500] Loss: 0.346597 | Time: 0.03s | Mean latent norm: 0.971731\n",
      "[Epoch 20/500] Loss: 0.346768 | Time: 0.02s | Mean latent norm: 0.971695\n",
      "[Epoch 21/500] Loss: 0.346754 | Time: 0.03s | Mean latent norm: 0.971666\n",
      "Average difference with latent perturbation: 0.00797903910279274\n",
      "[Epoch 22/500] Loss: 0.346687 | Time: 0.03s | Mean latent norm: 0.971642\n",
      "[Epoch 23/500] Loss: 0.346567 | Time: 0.03s | Mean latent norm: 0.971623\n",
      "[Epoch 24/500] Loss: 0.346206 | Time: 0.02s | Mean latent norm: 0.971608\n",
      "New best model saved at epoch 24 with loss 0.346206\n",
      "[Epoch 25/500] Loss: 0.345937 | Time: 0.02s | Mean latent norm: 0.971597\n",
      "New best model saved at epoch 25 with loss 0.345937\n",
      "[Epoch 26/500] Loss: 0.345665 | Time: 0.02s | Mean latent norm: 0.971587\n",
      "New best model saved at epoch 26 with loss 0.345665\n",
      "[Epoch 27/500] Loss: 0.345657 | Time: 0.02s | Mean latent norm: 0.971575\n",
      "New best model saved at epoch 27 with loss 0.345657\n",
      "[Epoch 28/500] Loss: 0.345611 | Time: 0.02s | Mean latent norm: 0.971559\n",
      "New best model saved at epoch 28 with loss 0.345611\n",
      "[Epoch 29/500] Loss: 0.345698 | Time: 0.02s | Mean latent norm: 0.971535\n",
      "[Epoch 30/500] Loss: 0.345666 | Time: 0.02s | Mean latent norm: 0.971504\n",
      "[Epoch 31/500] Loss: 0.345490 | Time: 0.03s | Mean latent norm: 0.971467\n",
      "Average difference with latent perturbation: 0.00843675434589386\n",
      "New best model saved at epoch 31 with loss 0.345490\n",
      "[Epoch 32/500] Loss: 0.345307 | Time: 0.03s | Mean latent norm: 0.971427\n",
      "New best model saved at epoch 32 with loss 0.345307\n",
      "[Epoch 33/500] Loss: 0.344852 | Time: 0.02s | Mean latent norm: 0.971388\n",
      "New best model saved at epoch 33 with loss 0.344852\n",
      "[Epoch 34/500] Loss: 0.344657 | Time: 0.02s | Mean latent norm: 0.971350\n",
      "New best model saved at epoch 34 with loss 0.344657\n",
      "[Epoch 35/500] Loss: 0.344519 | Time: 0.03s | Mean latent norm: 0.971318\n",
      "New best model saved at epoch 35 with loss 0.344519\n",
      "[Epoch 36/500] Loss: 0.344491 | Time: 0.02s | Mean latent norm: 0.971290\n",
      "New best model saved at epoch 36 with loss 0.344491\n",
      "[Epoch 37/500] Loss: 0.344147 | Time: 0.02s | Mean latent norm: 0.971267\n",
      "New best model saved at epoch 37 with loss 0.344147\n",
      "[Epoch 38/500] Loss: 0.343828 | Time: 0.02s | Mean latent norm: 0.971249\n",
      "New best model saved at epoch 38 with loss 0.343828\n",
      "[Epoch 39/500] Loss: 0.343597 | Time: 0.03s | Mean latent norm: 0.971233\n",
      "New best model saved at epoch 39 with loss 0.343597\n",
      "[Epoch 40/500] Loss: 0.343293 | Time: 0.02s | Mean latent norm: 0.971214\n",
      "New best model saved at epoch 40 with loss 0.343293\n",
      "[Epoch 41/500] Loss: 0.342921 | Time: 0.02s | Mean latent norm: 0.971190\n",
      "Average difference with latent perturbation: 0.008943199180066586\n",
      "New best model saved at epoch 41 with loss 0.342921\n",
      "[Epoch 42/500] Loss: 0.342593 | Time: 0.02s | Mean latent norm: 0.971158\n",
      "New best model saved at epoch 42 with loss 0.342593\n",
      "[Epoch 43/500] Loss: 0.342186 | Time: 0.02s | Mean latent norm: 0.971122\n",
      "New best model saved at epoch 43 with loss 0.342186\n",
      "[Epoch 44/500] Loss: 0.341612 | Time: 0.02s | Mean latent norm: 0.971086\n",
      "New best model saved at epoch 44 with loss 0.341612\n",
      "[Epoch 45/500] Loss: 0.341069 | Time: 0.02s | Mean latent norm: 0.971053\n",
      "New best model saved at epoch 45 with loss 0.341069\n",
      "[Epoch 46/500] Loss: 0.340562 | Time: 0.03s | Mean latent norm: 0.971025\n",
      "New best model saved at epoch 46 with loss 0.340562\n",
      "[Epoch 47/500] Loss: 0.339808 | Time: 0.03s | Mean latent norm: 0.971001\n",
      "New best model saved at epoch 47 with loss 0.339808\n",
      "[Epoch 48/500] Loss: 0.339028 | Time: 0.02s | Mean latent norm: 0.970976\n",
      "New best model saved at epoch 48 with loss 0.339028\n",
      "[Epoch 49/500] Loss: 0.338253 | Time: 0.02s | Mean latent norm: 0.970948\n",
      "New best model saved at epoch 49 with loss 0.338253\n",
      "[Epoch 50/500] Loss: 0.337299 | Time: 0.02s | Mean latent norm: 0.970915\n",
      "New best model saved at epoch 50 with loss 0.337299\n",
      "[Epoch 51/500] Loss: 0.336191 | Time: 0.02s | Mean latent norm: 0.970882\n",
      "Average difference with latent perturbation: 0.010356051847338676\n",
      "New best model saved at epoch 51 with loss 0.336191\n",
      "[Epoch 52/500] Loss: 0.334911 | Time: 0.02s | Mean latent norm: 0.970852\n",
      "New best model saved at epoch 52 with loss 0.334911\n",
      "[Epoch 53/500] Loss: 0.333467 | Time: 0.03s | Mean latent norm: 0.970825\n",
      "New best model saved at epoch 53 with loss 0.333467\n",
      "[Epoch 54/500] Loss: 0.331540 | Time: 0.03s | Mean latent norm: 0.970798\n",
      "New best model saved at epoch 54 with loss 0.331540\n",
      "[Epoch 55/500] Loss: 0.329746 | Time: 0.02s | Mean latent norm: 0.970770\n",
      "New best model saved at epoch 55 with loss 0.329746\n",
      "[Epoch 56/500] Loss: 0.327322 | Time: 0.02s | Mean latent norm: 0.970743\n",
      "New best model saved at epoch 56 with loss 0.327322\n",
      "[Epoch 57/500] Loss: 0.324621 | Time: 0.03s | Mean latent norm: 0.970717\n",
      "New best model saved at epoch 57 with loss 0.324621\n",
      "[Epoch 58/500] Loss: 0.321448 | Time: 0.02s | Mean latent norm: 0.970693\n",
      "New best model saved at epoch 58 with loss 0.321448\n",
      "[Epoch 59/500] Loss: 0.317819 | Time: 0.02s | Mean latent norm: 0.970672\n",
      "New best model saved at epoch 59 with loss 0.317819\n",
      "[Epoch 60/500] Loss: 0.313187 | Time: 0.02s | Mean latent norm: 0.970653\n",
      "New best model saved at epoch 60 with loss 0.313187\n",
      "[Epoch 61/500] Loss: 0.307970 | Time: 0.03s | Mean latent norm: 0.970634\n",
      "Average difference with latent perturbation: 0.022512035444378853\n",
      "New best model saved at epoch 61 with loss 0.307970\n",
      "[Epoch 62/500] Loss: 0.301784 | Time: 0.02s | Mean latent norm: 0.970622\n",
      "New best model saved at epoch 62 with loss 0.301784\n",
      "[Epoch 63/500] Loss: 0.294528 | Time: 0.02s | Mean latent norm: 0.970609\n",
      "New best model saved at epoch 63 with loss 0.294528\n",
      "[Epoch 64/500] Loss: 0.285989 | Time: 0.03s | Mean latent norm: 0.970600\n",
      "New best model saved at epoch 64 with loss 0.285989\n",
      "[Epoch 65/500] Loss: 0.276049 | Time: 0.02s | Mean latent norm: 0.970598\n",
      "New best model saved at epoch 65 with loss 0.276049\n",
      "[Epoch 66/500] Loss: 0.263800 | Time: 0.02s | Mean latent norm: 0.970586\n",
      "New best model saved at epoch 66 with loss 0.263800\n",
      "[Epoch 67/500] Loss: 0.251149 | Time: 0.02s | Mean latent norm: 0.970596\n",
      "New best model saved at epoch 67 with loss 0.251149\n",
      "[Epoch 68/500] Loss: 0.236356 | Time: 0.03s | Mean latent norm: 0.970593\n",
      "New best model saved at epoch 68 with loss 0.236356\n",
      "[Epoch 69/500] Loss: 0.220473 | Time: 0.02s | Mean latent norm: 0.970592\n",
      "New best model saved at epoch 69 with loss 0.220473\n",
      "[Epoch 70/500] Loss: 0.204565 | Time: 0.02s | Mean latent norm: 0.970609\n",
      "New best model saved at epoch 70 with loss 0.204565\n",
      "[Epoch 71/500] Loss: 0.188969 | Time: 0.02s | Mean latent norm: 0.970600\n",
      "Average difference with latent perturbation: 0.06493810564279556\n",
      "New best model saved at epoch 71 with loss 0.188969\n",
      "[Epoch 72/500] Loss: 0.174755 | Time: 0.02s | Mean latent norm: 0.970613\n",
      "New best model saved at epoch 72 with loss 0.174755\n",
      "[Epoch 73/500] Loss: 0.162031 | Time: 0.02s | Mean latent norm: 0.970616\n",
      "New best model saved at epoch 73 with loss 0.162031\n",
      "[Epoch 74/500] Loss: 0.151888 | Time: 0.02s | Mean latent norm: 0.970611\n",
      "New best model saved at epoch 74 with loss 0.151888\n",
      "[Epoch 75/500] Loss: 0.146988 | Time: 0.03s | Mean latent norm: 0.970623\n",
      "New best model saved at epoch 75 with loss 0.146988\n",
      "[Epoch 76/500] Loss: 0.143818 | Time: 0.02s | Mean latent norm: 0.970611\n",
      "New best model saved at epoch 76 with loss 0.143818\n",
      "[Epoch 77/500] Loss: 0.142285 | Time: 0.02s | Mean latent norm: 0.970602\n",
      "New best model saved at epoch 77 with loss 0.142285\n",
      "[Epoch 78/500] Loss: 0.142539 | Time: 0.02s | Mean latent norm: 0.970601\n",
      "[Epoch 79/500] Loss: 0.142487 | Time: 0.02s | Mean latent norm: 0.970591\n",
      "[Epoch 80/500] Loss: 0.143252 | Time: 0.03s | Mean latent norm: 0.970571\n",
      "[Epoch 81/500] Loss: 0.144136 | Time: 0.02s | Mean latent norm: 0.970553\n",
      "Average difference with latent perturbation: 0.05417923629283905\n",
      "[Epoch 82/500] Loss: 0.143606 | Time: 0.03s | Mean latent norm: 0.970537\n",
      "[Epoch 83/500] Loss: 0.143153 | Time: 0.02s | Mean latent norm: 0.970523\n",
      "[Epoch 84/500] Loss: 0.142807 | Time: 0.02s | Mean latent norm: 0.970504\n",
      "[Epoch 85/500] Loss: 0.141695 | Time: 0.02s | Mean latent norm: 0.970481\n",
      "New best model saved at epoch 85 with loss 0.141695\n",
      "[Epoch 86/500] Loss: 0.140272 | Time: 0.02s | Mean latent norm: 0.970459\n",
      "New best model saved at epoch 86 with loss 0.140272\n",
      "[Epoch 87/500] Loss: 0.137743 | Time: 0.03s | Mean latent norm: 0.970440\n",
      "New best model saved at epoch 87 with loss 0.137743\n",
      "[Epoch 88/500] Loss: 0.135735 | Time: 0.03s | Mean latent norm: 0.970430\n",
      "New best model saved at epoch 88 with loss 0.135735\n",
      "[Epoch 89/500] Loss: 0.132155 | Time: 0.02s | Mean latent norm: 0.970416\n",
      "New best model saved at epoch 89 with loss 0.132155\n",
      "[Epoch 90/500] Loss: 0.128703 | Time: 0.02s | Mean latent norm: 0.970403\n",
      "New best model saved at epoch 90 with loss 0.128703\n",
      "[Epoch 91/500] Loss: 0.125604 | Time: 0.02s | Mean latent norm: 0.970401\n",
      "Average difference with latent perturbation: 0.06133389100432396\n",
      "New best model saved at epoch 91 with loss 0.125604\n",
      "[Epoch 92/500] Loss: 0.123735 | Time: 0.02s | Mean latent norm: 0.970401\n",
      "New best model saved at epoch 92 with loss 0.123735\n",
      "[Epoch 93/500] Loss: 0.123674 | Time: 0.02s | Mean latent norm: 0.970386\n",
      "New best model saved at epoch 93 with loss 0.123674\n",
      "[Epoch 94/500] Loss: 0.124029 | Time: 0.02s | Mean latent norm: 0.970372\n",
      "[Epoch 95/500] Loss: 0.124136 | Time: 0.02s | Mean latent norm: 0.970351\n",
      "[Epoch 96/500] Loss: 0.123438 | Time: 0.02s | Mean latent norm: 0.970313\n",
      "New best model saved at epoch 96 with loss 0.123438\n",
      "[Epoch 97/500] Loss: 0.120931 | Time: 0.02s | Mean latent norm: 0.970279\n",
      "New best model saved at epoch 97 with loss 0.120931\n",
      "[Epoch 98/500] Loss: 0.118715 | Time: 0.02s | Mean latent norm: 0.970246\n",
      "New best model saved at epoch 98 with loss 0.118715\n",
      "[Epoch 99/500] Loss: 0.117560 | Time: 0.02s | Mean latent norm: 0.970203\n",
      "New best model saved at epoch 99 with loss 0.117560\n",
      "[Epoch 100/500] Loss: 0.116542 | Time: 0.02s | Mean latent norm: 0.970178\n",
      "New best model saved at epoch 100 with loss 0.116542\n",
      "[Epoch 101/500] Loss: 0.115538 | Time: 0.02s | Mean latent norm: 0.970143\n",
      "Average difference with latent perturbation: 0.07888536900281906\n",
      "New best model saved at epoch 101 with loss 0.115538\n",
      "[Epoch 102/500] Loss: 0.114892 | Time: 0.02s | Mean latent norm: 0.970124\n",
      "New best model saved at epoch 102 with loss 0.114892\n",
      "[Epoch 103/500] Loss: 0.114910 | Time: 0.02s | Mean latent norm: 0.970097\n",
      "[Epoch 104/500] Loss: 0.114240 | Time: 0.02s | Mean latent norm: 0.970092\n",
      "New best model saved at epoch 104 with loss 0.114240\n",
      "[Epoch 105/500] Loss: 0.114260 | Time: 0.02s | Mean latent norm: 0.970063\n",
      "[Epoch 106/500] Loss: 0.115629 | Time: 0.02s | Mean latent norm: 0.970075\n",
      "[Epoch 107/500] Loss: 0.115520 | Time: 0.02s | Mean latent norm: 0.970052\n",
      "[Epoch 108/500] Loss: 0.110141 | Time: 0.03s | Mean latent norm: 0.970039\n",
      "New best model saved at epoch 108 with loss 0.110141\n",
      "[Epoch 109/500] Loss: 0.112461 | Time: 0.03s | Mean latent norm: 0.970049\n",
      "[Epoch 110/500] Loss: 0.112237 | Time: 0.02s | Mean latent norm: 0.970035\n",
      "[Epoch 111/500] Loss: 0.108820 | Time: 0.02s | Mean latent norm: 0.970019\n",
      "Average difference with latent perturbation: 0.077791728079319\n",
      "New best model saved at epoch 111 with loss 0.108820\n",
      "[Epoch 112/500] Loss: 0.110785 | Time: 0.02s | Mean latent norm: 0.970024\n",
      "[Epoch 113/500] Loss: 0.109006 | Time: 0.02s | Mean latent norm: 0.970017\n",
      "[Epoch 114/500] Loss: 0.108548 | Time: 0.03s | Mean latent norm: 0.969996\n",
      "New best model saved at epoch 114 with loss 0.108548\n",
      "[Epoch 115/500] Loss: 0.108101 | Time: 0.02s | Mean latent norm: 0.969993\n",
      "New best model saved at epoch 115 with loss 0.108101\n",
      "[Epoch 116/500] Loss: 0.106463 | Time: 0.02s | Mean latent norm: 0.969995\n",
      "New best model saved at epoch 116 with loss 0.106463\n",
      "[Epoch 117/500] Loss: 0.107743 | Time: 0.03s | Mean latent norm: 0.969978\n",
      "[Epoch 118/500] Loss: 0.105210 | Time: 0.02s | Mean latent norm: 0.969968\n",
      "New best model saved at epoch 118 with loss 0.105210\n",
      "[Epoch 119/500] Loss: 0.106566 | Time: 0.03s | Mean latent norm: 0.969973\n",
      "[Epoch 120/500] Loss: 0.105793 | Time: 0.02s | Mean latent norm: 0.969967\n",
      "[Epoch 121/500] Loss: 0.104531 | Time: 0.03s | Mean latent norm: 0.969953\n",
      "Average difference with latent perturbation: 0.07884098589420319\n",
      "New best model saved at epoch 121 with loss 0.104531\n",
      "[Epoch 122/500] Loss: 0.104452 | Time: 0.02s | Mean latent norm: 0.969954\n",
      "New best model saved at epoch 122 with loss 0.104452\n",
      "[Epoch 123/500] Loss: 0.102350 | Time: 0.02s | Mean latent norm: 0.969960\n",
      "New best model saved at epoch 123 with loss 0.102350\n",
      "[Epoch 124/500] Loss: 0.104232 | Time: 0.02s | Mean latent norm: 0.969952\n",
      "[Epoch 125/500] Loss: 0.101435 | Time: 0.02s | Mean latent norm: 0.969947\n",
      "New best model saved at epoch 125 with loss 0.101435\n",
      "[Epoch 126/500] Loss: 0.102313 | Time: 0.02s | Mean latent norm: 0.969954\n",
      "[Epoch 127/500] Loss: 0.100752 | Time: 0.02s | Mean latent norm: 0.969954\n",
      "New best model saved at epoch 127 with loss 0.100752\n",
      "[Epoch 128/500] Loss: 0.101117 | Time: 0.02s | Mean latent norm: 0.969948\n",
      "[Epoch 129/500] Loss: 0.099805 | Time: 0.02s | Mean latent norm: 0.969948\n",
      "New best model saved at epoch 129 with loss 0.099805\n",
      "[Epoch 130/500] Loss: 0.100274 | Time: 0.02s | Mean latent norm: 0.969954\n",
      "[Epoch 131/500] Loss: 0.099315 | Time: 0.02s | Mean latent norm: 0.969951\n",
      "Average difference with latent perturbation: 0.07861009240150452\n",
      "New best model saved at epoch 131 with loss 0.099315\n",
      "[Epoch 132/500] Loss: 0.098562 | Time: 0.03s | Mean latent norm: 0.969945\n",
      "New best model saved at epoch 132 with loss 0.098562\n",
      "[Epoch 133/500] Loss: 0.098955 | Time: 0.03s | Mean latent norm: 0.969951\n",
      "[Epoch 134/500] Loss: 0.097429 | Time: 0.02s | Mean latent norm: 0.969948\n",
      "New best model saved at epoch 134 with loss 0.097429\n",
      "[Epoch 135/500] Loss: 0.097735 | Time: 0.02s | Mean latent norm: 0.969939\n",
      "[Epoch 136/500] Loss: 0.097193 | Time: 0.02s | Mean latent norm: 0.969941\n",
      "New best model saved at epoch 136 with loss 0.097193\n",
      "[Epoch 137/500] Loss: 0.096791 | Time: 0.03s | Mean latent norm: 0.969942\n",
      "New best model saved at epoch 137 with loss 0.096791\n",
      "[Epoch 138/500] Loss: 0.096166 | Time: 0.02s | Mean latent norm: 0.969937\n",
      "New best model saved at epoch 138 with loss 0.096166\n",
      "[Epoch 139/500] Loss: 0.095146 | Time: 0.03s | Mean latent norm: 0.969935\n",
      "New best model saved at epoch 139 with loss 0.095146\n",
      "[Epoch 140/500] Loss: 0.095810 | Time: 0.02s | Mean latent norm: 0.969939\n",
      "[Epoch 141/500] Loss: 0.094681 | Time: 0.03s | Mean latent norm: 0.969938\n",
      "Average difference with latent perturbation: 0.0820508673787117\n",
      "New best model saved at epoch 141 with loss 0.094681\n",
      "[Epoch 142/500] Loss: 0.093847 | Time: 0.03s | Mean latent norm: 0.969931\n",
      "New best model saved at epoch 142 with loss 0.093847\n",
      "[Epoch 143/500] Loss: 0.095520 | Time: 0.03s | Mean latent norm: 0.969940\n",
      "[Epoch 144/500] Loss: 0.093914 | Time: 0.03s | Mean latent norm: 0.969939\n",
      "[Epoch 145/500] Loss: 0.092742 | Time: 0.03s | Mean latent norm: 0.969932\n",
      "New best model saved at epoch 145 with loss 0.092742\n",
      "[Epoch 146/500] Loss: 0.093040 | Time: 0.02s | Mean latent norm: 0.969938\n",
      "[Epoch 147/500] Loss: 0.091983 | Time: 0.03s | Mean latent norm: 0.969940\n",
      "New best model saved at epoch 147 with loss 0.091983\n",
      "[Epoch 148/500] Loss: 0.090886 | Time: 0.03s | Mean latent norm: 0.969932\n",
      "New best model saved at epoch 148 with loss 0.090886\n",
      "[Epoch 149/500] Loss: 0.090500 | Time: 0.02s | Mean latent norm: 0.969934\n",
      "New best model saved at epoch 149 with loss 0.090500\n",
      "[Epoch 150/500] Loss: 0.090292 | Time: 0.02s | Mean latent norm: 0.969939\n",
      "New best model saved at epoch 150 with loss 0.090292\n",
      "[Epoch 151/500] Loss: 0.090982 | Time: 0.03s | Mean latent norm: 0.969933\n",
      "Average difference with latent perturbation: 0.08008134365081787\n",
      "[Epoch 152/500] Loss: 0.089225 | Time: 0.03s | Mean latent norm: 0.969938\n",
      "New best model saved at epoch 152 with loss 0.089225\n",
      "[Epoch 153/500] Loss: 0.088681 | Time: 0.03s | Mean latent norm: 0.969942\n",
      "New best model saved at epoch 153 with loss 0.088681\n",
      "[Epoch 154/500] Loss: 0.089373 | Time: 0.03s | Mean latent norm: 0.969938\n",
      "[Epoch 155/500] Loss: 0.088472 | Time: 0.03s | Mean latent norm: 0.969941\n",
      "New best model saved at epoch 155 with loss 0.088472\n",
      "[Epoch 156/500] Loss: 0.087622 | Time: 0.02s | Mean latent norm: 0.969945\n",
      "New best model saved at epoch 156 with loss 0.087622\n",
      "[Epoch 157/500] Loss: 0.086925 | Time: 0.03s | Mean latent norm: 0.969945\n",
      "New best model saved at epoch 157 with loss 0.086925\n",
      "[Epoch 158/500] Loss: 0.086725 | Time: 0.03s | Mean latent norm: 0.969949\n",
      "New best model saved at epoch 158 with loss 0.086725\n",
      "[Epoch 159/500] Loss: 0.085906 | Time: 0.03s | Mean latent norm: 0.969955\n",
      "New best model saved at epoch 159 with loss 0.085906\n",
      "[Epoch 160/500] Loss: 0.085208 | Time: 0.03s | Mean latent norm: 0.969954\n",
      "New best model saved at epoch 160 with loss 0.085208\n",
      "[Epoch 161/500] Loss: 0.084859 | Time: 0.03s | Mean latent norm: 0.969957\n",
      "Average difference with latent perturbation: 0.07975823432207108\n",
      "New best model saved at epoch 161 with loss 0.084859\n",
      "[Epoch 162/500] Loss: 0.084210 | Time: 0.03s | Mean latent norm: 0.969962\n",
      "New best model saved at epoch 162 with loss 0.084210\n",
      "[Epoch 163/500] Loss: 0.083605 | Time: 0.03s | Mean latent norm: 0.969961\n",
      "New best model saved at epoch 163 with loss 0.083605\n",
      "[Epoch 164/500] Loss: 0.083403 | Time: 0.02s | Mean latent norm: 0.969965\n",
      "New best model saved at epoch 164 with loss 0.083403\n",
      "[Epoch 165/500] Loss: 0.083306 | Time: 0.03s | Mean latent norm: 0.969972\n",
      "New best model saved at epoch 165 with loss 0.083306\n",
      "[Epoch 166/500] Loss: 0.083477 | Time: 0.02s | Mean latent norm: 0.969970\n",
      "[Epoch 167/500] Loss: 0.082679 | Time: 0.03s | Mean latent norm: 0.969972\n",
      "New best model saved at epoch 167 with loss 0.082679\n",
      "[Epoch 168/500] Loss: 0.081530 | Time: 0.03s | Mean latent norm: 0.969978\n",
      "New best model saved at epoch 168 with loss 0.081530\n",
      "[Epoch 169/500] Loss: 0.081316 | Time: 0.03s | Mean latent norm: 0.969975\n",
      "New best model saved at epoch 169 with loss 0.081316\n",
      "[Epoch 170/500] Loss: 0.081037 | Time: 0.03s | Mean latent norm: 0.969978\n",
      "New best model saved at epoch 170 with loss 0.081037\n",
      "[Epoch 171/500] Loss: 0.080350 | Time: 0.03s | Mean latent norm: 0.969986\n",
      "Average difference with latent perturbation: 0.07841480523347855\n",
      "New best model saved at epoch 171 with loss 0.080350\n",
      "[Epoch 172/500] Loss: 0.080618 | Time: 0.03s | Mean latent norm: 0.969983\n",
      "[Epoch 173/500] Loss: 0.079837 | Time: 0.03s | Mean latent norm: 0.969985\n",
      "New best model saved at epoch 173 with loss 0.079837\n",
      "[Epoch 174/500] Loss: 0.079205 | Time: 0.03s | Mean latent norm: 0.969991\n",
      "New best model saved at epoch 174 with loss 0.079205\n",
      "[Epoch 175/500] Loss: 0.079446 | Time: 0.03s | Mean latent norm: 0.969987\n",
      "[Epoch 176/500] Loss: 0.078188 | Time: 0.03s | Mean latent norm: 0.969989\n",
      "New best model saved at epoch 176 with loss 0.078188\n",
      "[Epoch 177/500] Loss: 0.077806 | Time: 0.02s | Mean latent norm: 0.969997\n",
      "New best model saved at epoch 177 with loss 0.077806\n",
      "[Epoch 178/500] Loss: 0.078571 | Time: 0.03s | Mean latent norm: 0.969990\n",
      "[Epoch 179/500] Loss: 0.076567 | Time: 0.02s | Mean latent norm: 0.969991\n",
      "New best model saved at epoch 179 with loss 0.076567\n",
      "[Epoch 180/500] Loss: 0.076556 | Time: 0.03s | Mean latent norm: 0.970000\n",
      "New best model saved at epoch 180 with loss 0.076556\n",
      "[Epoch 181/500] Loss: 0.076445 | Time: 0.03s | Mean latent norm: 0.969993\n",
      "Average difference with latent perturbation: 0.07541799545288086\n",
      "New best model saved at epoch 181 with loss 0.076445\n",
      "[Epoch 182/500] Loss: 0.076248 | Time: 0.03s | Mean latent norm: 0.969992\n",
      "New best model saved at epoch 182 with loss 0.076248\n",
      "[Epoch 183/500] Loss: 0.075584 | Time: 0.02s | Mean latent norm: 0.970001\n",
      "New best model saved at epoch 183 with loss 0.075584\n",
      "[Epoch 184/500] Loss: 0.076092 | Time: 0.02s | Mean latent norm: 0.969997\n",
      "[Epoch 185/500] Loss: 0.074711 | Time: 0.03s | Mean latent norm: 0.969994\n",
      "New best model saved at epoch 185 with loss 0.074711\n",
      "[Epoch 186/500] Loss: 0.075484 | Time: 0.03s | Mean latent norm: 0.970003\n",
      "[Epoch 187/500] Loss: 0.075059 | Time: 0.03s | Mean latent norm: 0.970000\n",
      "[Epoch 188/500] Loss: 0.074284 | Time: 0.03s | Mean latent norm: 0.969994\n",
      "New best model saved at epoch 188 with loss 0.074284\n",
      "[Epoch 189/500] Loss: 0.074497 | Time: 0.03s | Mean latent norm: 0.970003\n",
      "[Epoch 190/500] Loss: 0.072909 | Time: 0.03s | Mean latent norm: 0.970007\n",
      "New best model saved at epoch 190 with loss 0.072909\n",
      "[Epoch 191/500] Loss: 0.073764 | Time: 0.02s | Mean latent norm: 0.969998\n",
      "Average difference with latent perturbation: 0.07564704120159149\n",
      "[Epoch 192/500] Loss: 0.073423 | Time: 0.03s | Mean latent norm: 0.970002\n",
      "[Epoch 193/500] Loss: 0.071955 | Time: 0.03s | Mean latent norm: 0.970009\n",
      "New best model saved at epoch 193 with loss 0.071955\n",
      "[Epoch 194/500] Loss: 0.073710 | Time: 0.02s | Mean latent norm: 0.970000\n",
      "[Epoch 195/500] Loss: 0.071818 | Time: 0.03s | Mean latent norm: 0.970001\n",
      "New best model saved at epoch 195 with loss 0.071818\n",
      "[Epoch 196/500] Loss: 0.071725 | Time: 0.03s | Mean latent norm: 0.970008\n",
      "New best model saved at epoch 196 with loss 0.071725\n",
      "[Epoch 197/500] Loss: 0.071761 | Time: 0.02s | Mean latent norm: 0.970007\n",
      "[Epoch 198/500] Loss: 0.071410 | Time: 0.03s | Mean latent norm: 0.970004\n",
      "New best model saved at epoch 198 with loss 0.071410\n",
      "[Epoch 199/500] Loss: 0.071110 | Time: 0.03s | Mean latent norm: 0.970008\n",
      "New best model saved at epoch 199 with loss 0.071110\n",
      "[Epoch 200/500] Loss: 0.069933 | Time: 0.03s | Mean latent norm: 0.970014\n",
      "New best model saved at epoch 200 with loss 0.069933\n",
      "[Epoch 201/500] Loss: 0.070950 | Time: 0.03s | Mean latent norm: 0.970008\n",
      "Average difference with latent perturbation: 0.0766991451382637\n",
      "[Epoch 202/500] Loss: 0.070291 | Time: 0.03s | Mean latent norm: 0.970010\n",
      "[Epoch 203/500] Loss: 0.070212 | Time: 0.03s | Mean latent norm: 0.970016\n",
      "[Epoch 204/500] Loss: 0.069785 | Time: 0.03s | Mean latent norm: 0.970015\n",
      "New best model saved at epoch 204 with loss 0.069785\n",
      "[Epoch 205/500] Loss: 0.069404 | Time: 0.03s | Mean latent norm: 0.970011\n",
      "New best model saved at epoch 205 with loss 0.069404\n",
      "[Epoch 206/500] Loss: 0.070166 | Time: 0.03s | Mean latent norm: 0.970016\n",
      "[Epoch 207/500] Loss: 0.069400 | Time: 0.03s | Mean latent norm: 0.970020\n",
      "New best model saved at epoch 207 with loss 0.069400\n",
      "[Epoch 208/500] Loss: 0.069035 | Time: 0.03s | Mean latent norm: 0.970016\n",
      "New best model saved at epoch 208 with loss 0.069035\n",
      "[Epoch 209/500] Loss: 0.068477 | Time: 0.02s | Mean latent norm: 0.970017\n",
      "New best model saved at epoch 209 with loss 0.068477\n",
      "[Epoch 210/500] Loss: 0.068398 | Time: 0.03s | Mean latent norm: 0.970025\n",
      "New best model saved at epoch 210 with loss 0.068398\n",
      "[Epoch 211/500] Loss: 0.068716 | Time: 0.02s | Mean latent norm: 0.970024\n",
      "Average difference with latent perturbation: 0.0731486827135086\n",
      "[Epoch 212/500] Loss: 0.068480 | Time: 0.03s | Mean latent norm: 0.970021\n",
      "[Epoch 213/500] Loss: 0.067490 | Time: 0.03s | Mean latent norm: 0.970028\n",
      "New best model saved at epoch 213 with loss 0.067490\n",
      "[Epoch 214/500] Loss: 0.067297 | Time: 0.03s | Mean latent norm: 0.970031\n",
      "New best model saved at epoch 214 with loss 0.067297\n",
      "[Epoch 215/500] Loss: 0.067900 | Time: 0.02s | Mean latent norm: 0.970029\n",
      "[Epoch 216/500] Loss: 0.067412 | Time: 0.03s | Mean latent norm: 0.970033\n",
      "[Epoch 217/500] Loss: 0.067206 | Time: 0.03s | Mean latent norm: 0.970038\n",
      "New best model saved at epoch 217 with loss 0.067206\n",
      "[Epoch 218/500] Loss: 0.067168 | Time: 0.03s | Mean latent norm: 0.970035\n",
      "New best model saved at epoch 218 with loss 0.067168\n",
      "[Epoch 219/500] Loss: 0.066730 | Time: 0.02s | Mean latent norm: 0.970036\n",
      "New best model saved at epoch 219 with loss 0.066730\n",
      "[Epoch 220/500] Loss: 0.066409 | Time: 0.03s | Mean latent norm: 0.970042\n",
      "New best model saved at epoch 220 with loss 0.066409\n",
      "[Epoch 221/500] Loss: 0.066142 | Time: 0.03s | Mean latent norm: 0.970042\n",
      "Average difference with latent perturbation: 0.0710940808057785\n",
      "New best model saved at epoch 221 with loss 0.066142\n",
      "[Epoch 222/500] Loss: 0.065670 | Time: 0.03s | Mean latent norm: 0.970040\n",
      "New best model saved at epoch 222 with loss 0.065670\n",
      "[Epoch 223/500] Loss: 0.065868 | Time: 0.03s | Mean latent norm: 0.970047\n",
      "[Epoch 224/500] Loss: 0.065623 | Time: 0.03s | Mean latent norm: 0.970052\n",
      "New best model saved at epoch 224 with loss 0.065623\n",
      "[Epoch 225/500] Loss: 0.065776 | Time: 0.03s | Mean latent norm: 0.970049\n",
      "[Epoch 226/500] Loss: 0.065269 | Time: 0.02s | Mean latent norm: 0.970049\n",
      "New best model saved at epoch 226 with loss 0.065269\n",
      "[Epoch 227/500] Loss: 0.065693 | Time: 0.02s | Mean latent norm: 0.970056\n",
      "[Epoch 228/500] Loss: 0.065489 | Time: 0.02s | Mean latent norm: 0.970057\n",
      "[Epoch 229/500] Loss: 0.064787 | Time: 0.02s | Mean latent norm: 0.970053\n",
      "New best model saved at epoch 229 with loss 0.064787\n",
      "[Epoch 230/500] Loss: 0.064471 | Time: 0.02s | Mean latent norm: 0.970061\n",
      "New best model saved at epoch 230 with loss 0.064471\n",
      "[Epoch 231/500] Loss: 0.064038 | Time: 0.02s | Mean latent norm: 0.970066\n",
      "Average difference with latent perturbation: 0.07692383229732513\n",
      "New best model saved at epoch 231 with loss 0.064038\n",
      "[Epoch 232/500] Loss: 0.064417 | Time: 0.02s | Mean latent norm: 0.970062\n",
      "[Epoch 233/500] Loss: 0.064400 | Time: 0.03s | Mean latent norm: 0.970064\n",
      "[Epoch 234/500] Loss: 0.063448 | Time: 0.02s | Mean latent norm: 0.970072\n",
      "New best model saved at epoch 234 with loss 0.063448\n",
      "[Epoch 235/500] Loss: 0.064415 | Time: 0.02s | Mean latent norm: 0.970071\n",
      "[Epoch 236/500] Loss: 0.063295 | Time: 0.02s | Mean latent norm: 0.970067\n",
      "New best model saved at epoch 236 with loss 0.063295\n",
      "[Epoch 237/500] Loss: 0.063568 | Time: 0.03s | Mean latent norm: 0.970074\n",
      "[Epoch 238/500] Loss: 0.062980 | Time: 0.02s | Mean latent norm: 0.970076\n",
      "New best model saved at epoch 238 with loss 0.062980\n",
      "[Epoch 239/500] Loss: 0.062774 | Time: 0.02s | Mean latent norm: 0.970072\n",
      "New best model saved at epoch 239 with loss 0.062774\n",
      "[Epoch 240/500] Loss: 0.063114 | Time: 0.02s | Mean latent norm: 0.970075\n",
      "[Epoch 241/500] Loss: 0.063087 | Time: 0.03s | Mean latent norm: 0.970078\n",
      "Average difference with latent perturbation: 0.06636867672204971\n",
      "[Epoch 242/500] Loss: 0.062544 | Time: 0.02s | Mean latent norm: 0.970076\n",
      "New best model saved at epoch 242 with loss 0.062544\n",
      "[Epoch 243/500] Loss: 0.062708 | Time: 0.03s | Mean latent norm: 0.970077\n",
      "[Epoch 244/500] Loss: 0.062427 | Time: 0.02s | Mean latent norm: 0.970080\n",
      "New best model saved at epoch 244 with loss 0.062427\n",
      "[Epoch 245/500] Loss: 0.061875 | Time: 0.02s | Mean latent norm: 0.970079\n",
      "New best model saved at epoch 245 with loss 0.061875\n",
      "[Epoch 246/500] Loss: 0.062410 | Time: 0.02s | Mean latent norm: 0.970077\n",
      "[Epoch 247/500] Loss: 0.062505 | Time: 0.02s | Mean latent norm: 0.970082\n",
      "[Epoch 248/500] Loss: 0.061325 | Time: 0.02s | Mean latent norm: 0.970081\n",
      "New best model saved at epoch 248 with loss 0.061325\n",
      "[Epoch 249/500] Loss: 0.061419 | Time: 0.02s | Mean latent norm: 0.970078\n",
      "[Epoch 250/500] Loss: 0.061991 | Time: 0.02s | Mean latent norm: 0.970082\n",
      "[Epoch 251/500] Loss: 0.061402 | Time: 0.02s | Mean latent norm: 0.970083\n",
      "Average difference with latent perturbation: 0.0700865313410759\n",
      "[Epoch 252/500] Loss: 0.061296 | Time: 0.02s | Mean latent norm: 0.970078\n",
      "New best model saved at epoch 252 with loss 0.061296\n",
      "[Epoch 253/500] Loss: 0.061778 | Time: 0.02s | Mean latent norm: 0.970080\n",
      "[Epoch 254/500] Loss: 0.060653 | Time: 0.02s | Mean latent norm: 0.970087\n",
      "New best model saved at epoch 254 with loss 0.060653\n",
      "[Epoch 255/500] Loss: 0.061589 | Time: 0.02s | Mean latent norm: 0.970083\n",
      "[Epoch 256/500] Loss: 0.060647 | Time: 0.02s | Mean latent norm: 0.970083\n",
      "New best model saved at epoch 256 with loss 0.060647\n",
      "[Epoch 257/500] Loss: 0.060640 | Time: 0.02s | Mean latent norm: 0.970088\n",
      "New best model saved at epoch 257 with loss 0.060640\n",
      "[Epoch 258/500] Loss: 0.060413 | Time: 0.02s | Mean latent norm: 0.970084\n",
      "New best model saved at epoch 258 with loss 0.060413\n",
      "[Epoch 259/500] Loss: 0.060590 | Time: 0.02s | Mean latent norm: 0.970081\n",
      "[Epoch 260/500] Loss: 0.059830 | Time: 0.02s | Mean latent norm: 0.970087\n",
      "New best model saved at epoch 260 with loss 0.059830\n",
      "[Epoch 261/500] Loss: 0.060980 | Time: 0.02s | Mean latent norm: 0.970083\n",
      "Average difference with latent perturbation: 0.06844495981931686\n",
      "[Epoch 262/500] Loss: 0.060068 | Time: 0.02s | Mean latent norm: 0.970082\n",
      "[Epoch 263/500] Loss: 0.060094 | Time: 0.02s | Mean latent norm: 0.970088\n",
      "[Epoch 264/500] Loss: 0.060206 | Time: 0.02s | Mean latent norm: 0.970084\n",
      "[Epoch 265/500] Loss: 0.060180 | Time: 0.02s | Mean latent norm: 0.970081\n",
      "[Epoch 266/500] Loss: 0.059337 | Time: 0.02s | Mean latent norm: 0.970086\n",
      "New best model saved at epoch 266 with loss 0.059337\n",
      "[Epoch 267/500] Loss: 0.059375 | Time: 0.02s | Mean latent norm: 0.970083\n",
      "[Epoch 268/500] Loss: 0.059441 | Time: 0.02s | Mean latent norm: 0.970078\n",
      "[Epoch 269/500] Loss: 0.059895 | Time: 0.02s | Mean latent norm: 0.970085\n",
      "[Epoch 270/500] Loss: 0.059009 | Time: 0.02s | Mean latent norm: 0.970083\n",
      "New best model saved at epoch 270 with loss 0.059009\n",
      "[Epoch 271/500] Loss: 0.058985 | Time: 0.02s | Mean latent norm: 0.970076\n",
      "Average difference with latent perturbation: 0.06529556959867477\n",
      "New best model saved at epoch 271 with loss 0.058985\n",
      "[Epoch 272/500] Loss: 0.059410 | Time: 0.02s | Mean latent norm: 0.970082\n",
      "[Epoch 273/500] Loss: 0.058976 | Time: 0.02s | Mean latent norm: 0.970081\n",
      "New best model saved at epoch 273 with loss 0.058976\n",
      "[Epoch 274/500] Loss: 0.058240 | Time: 0.02s | Mean latent norm: 0.970073\n",
      "New best model saved at epoch 274 with loss 0.058240\n",
      "[Epoch 275/500] Loss: 0.059334 | Time: 0.02s | Mean latent norm: 0.970079\n",
      "[Epoch 276/500] Loss: 0.058495 | Time: 0.02s | Mean latent norm: 0.970080\n",
      "[Epoch 277/500] Loss: 0.058681 | Time: 0.02s | Mean latent norm: 0.970075\n",
      "[Epoch 278/500] Loss: 0.057995 | Time: 0.02s | Mean latent norm: 0.970074\n",
      "New best model saved at epoch 278 with loss 0.057995\n",
      "[Epoch 279/500] Loss: 0.058335 | Time: 0.02s | Mean latent norm: 0.970078\n",
      "[Epoch 280/500] Loss: 0.058201 | Time: 0.02s | Mean latent norm: 0.970076\n",
      "[Epoch 281/500] Loss: 0.058236 | Time: 0.02s | Mean latent norm: 0.970079\n",
      "Average difference with latent perturbation: 0.06395600736141205\n",
      "[Epoch 282/500] Loss: 0.057666 | Time: 0.02s | Mean latent norm: 0.970082\n",
      "New best model saved at epoch 282 with loss 0.057666\n",
      "[Epoch 283/500] Loss: 0.057700 | Time: 0.02s | Mean latent norm: 0.970079\n",
      "[Epoch 284/500] Loss: 0.056851 | Time: 0.02s | Mean latent norm: 0.970078\n",
      "New best model saved at epoch 284 with loss 0.056851\n",
      "[Epoch 285/500] Loss: 0.057269 | Time: 0.02s | Mean latent norm: 0.970083\n",
      "[Epoch 286/500] Loss: 0.057761 | Time: 0.02s | Mean latent norm: 0.970079\n",
      "[Epoch 287/500] Loss: 0.056521 | Time: 0.02s | Mean latent norm: 0.970079\n",
      "New best model saved at epoch 287 with loss 0.056521\n",
      "[Epoch 288/500] Loss: 0.057922 | Time: 0.02s | Mean latent norm: 0.970083\n",
      "[Epoch 289/500] Loss: 0.057036 | Time: 0.02s | Mean latent norm: 0.970083\n",
      "[Epoch 290/500] Loss: 0.057503 | Time: 0.02s | Mean latent norm: 0.970079\n",
      "[Epoch 291/500] Loss: 0.057329 | Time: 0.02s | Mean latent norm: 0.970084\n",
      "Average difference with latent perturbation: 0.06577648967504501\n",
      "[Epoch 292/500] Loss: 0.056693 | Time: 0.02s | Mean latent norm: 0.970085\n",
      "[Epoch 293/500] Loss: 0.056726 | Time: 0.02s | Mean latent norm: 0.970083\n",
      "[Epoch 294/500] Loss: 0.057054 | Time: 0.02s | Mean latent norm: 0.970085\n",
      "[Epoch 295/500] Loss: 0.056299 | Time: 0.02s | Mean latent norm: 0.970085\n",
      "New best model saved at epoch 295 with loss 0.056299\n",
      "[Epoch 296/500] Loss: 0.056539 | Time: 0.02s | Mean latent norm: 0.970082\n",
      "[Epoch 297/500] Loss: 0.056649 | Time: 0.02s | Mean latent norm: 0.970089\n",
      "[Epoch 298/500] Loss: 0.056540 | Time: 0.02s | Mean latent norm: 0.970088\n",
      "[Epoch 299/500] Loss: 0.056473 | Time: 0.02s | Mean latent norm: 0.970088\n",
      "[Epoch 300/500] Loss: 0.056751 | Time: 0.02s | Mean latent norm: 0.970095\n",
      "[Epoch 301/500] Loss: 0.056474 | Time: 0.02s | Mean latent norm: 0.970088\n",
      "Average difference with latent perturbation: 0.06038658320903778\n",
      "[Epoch 302/500] Loss: 0.056241 | Time: 0.02s | Mean latent norm: 0.970089\n",
      "New best model saved at epoch 302 with loss 0.056241\n",
      "[Epoch 303/500] Loss: 0.056534 | Time: 0.02s | Mean latent norm: 0.970096\n",
      "[Epoch 304/500] Loss: 0.056180 | Time: 0.02s | Mean latent norm: 0.970090\n",
      "New best model saved at epoch 304 with loss 0.056180\n",
      "[Epoch 305/500] Loss: 0.055974 | Time: 0.02s | Mean latent norm: 0.970092\n",
      "New best model saved at epoch 305 with loss 0.055974\n",
      "[Epoch 306/500] Loss: 0.056011 | Time: 0.02s | Mean latent norm: 0.970099\n",
      "[Epoch 307/500] Loss: 0.056202 | Time: 0.02s | Mean latent norm: 0.970091\n",
      "[Epoch 308/500] Loss: 0.056046 | Time: 0.02s | Mean latent norm: 0.970094\n",
      "[Epoch 309/500] Loss: 0.055634 | Time: 0.02s | Mean latent norm: 0.970097\n",
      "New best model saved at epoch 309 with loss 0.055634\n",
      "[Epoch 310/500] Loss: 0.055768 | Time: 0.02s | Mean latent norm: 0.970095\n",
      "[Epoch 311/500] Loss: 0.055523 | Time: 0.02s | Mean latent norm: 0.970094\n",
      "Average difference with latent perturbation: 0.06337428838014603\n",
      "New best model saved at epoch 311 with loss 0.055523\n",
      "[Epoch 312/500] Loss: 0.055084 | Time: 0.02s | Mean latent norm: 0.970102\n",
      "New best model saved at epoch 312 with loss 0.055084\n",
      "[Epoch 313/500] Loss: 0.055637 | Time: 0.02s | Mean latent norm: 0.970097\n",
      "[Epoch 314/500] Loss: 0.055386 | Time: 0.02s | Mean latent norm: 0.970094\n",
      "[Epoch 315/500] Loss: 0.055640 | Time: 0.02s | Mean latent norm: 0.970100\n",
      "[Epoch 316/500] Loss: 0.055228 | Time: 0.02s | Mean latent norm: 0.970095\n",
      "[Epoch 317/500] Loss: 0.055219 | Time: 0.02s | Mean latent norm: 0.970093\n",
      "[Epoch 318/500] Loss: 0.055646 | Time: 0.02s | Mean latent norm: 0.970101\n",
      "[Epoch 319/500] Loss: 0.055739 | Time: 0.02s | Mean latent norm: 0.970093\n",
      "[Epoch 320/500] Loss: 0.054825 | Time: 0.02s | Mean latent norm: 0.970090\n",
      "New best model saved at epoch 320 with loss 0.054825\n",
      "[Epoch 321/500] Loss: 0.054814 | Time: 0.02s | Mean latent norm: 0.970096\n",
      "Average difference with latent perturbation: 0.061842791736125946\n",
      "New best model saved at epoch 321 with loss 0.054814\n",
      "[Epoch 322/500] Loss: 0.055050 | Time: 0.02s | Mean latent norm: 0.970094\n",
      "[Epoch 323/500] Loss: 0.054933 | Time: 0.02s | Mean latent norm: 0.970090\n",
      "[Epoch 324/500] Loss: 0.054788 | Time: 0.02s | Mean latent norm: 0.970094\n",
      "New best model saved at epoch 324 with loss 0.054788\n",
      "[Epoch 325/500] Loss: 0.054116 | Time: 0.02s | Mean latent norm: 0.970093\n",
      "New best model saved at epoch 325 with loss 0.054116\n",
      "[Epoch 326/500] Loss: 0.054419 | Time: 0.02s | Mean latent norm: 0.970090\n",
      "[Epoch 327/500] Loss: 0.054303 | Time: 0.02s | Mean latent norm: 0.970093\n",
      "[Epoch 328/500] Loss: 0.054284 | Time: 0.02s | Mean latent norm: 0.970091\n",
      "[Epoch 329/500] Loss: 0.054399 | Time: 0.02s | Mean latent norm: 0.970092\n",
      "[Epoch 330/500] Loss: 0.054361 | Time: 0.02s | Mean latent norm: 0.970096\n",
      "[Epoch 331/500] Loss: 0.053910 | Time: 0.02s | Mean latent norm: 0.970092\n",
      "Average difference with latent perturbation: 0.07155629992485046\n",
      "New best model saved at epoch 331 with loss 0.053910\n",
      "[Epoch 332/500] Loss: 0.054392 | Time: 0.02s | Mean latent norm: 0.970090\n",
      "[Epoch 333/500] Loss: 0.054443 | Time: 0.02s | Mean latent norm: 0.970095\n",
      "[Epoch 334/500] Loss: 0.054584 | Time: 0.02s | Mean latent norm: 0.970087\n",
      "[Epoch 335/500] Loss: 0.053741 | Time: 0.02s | Mean latent norm: 0.970086\n",
      "New best model saved at epoch 335 with loss 0.053741\n",
      "[Epoch 336/500] Loss: 0.054013 | Time: 0.02s | Mean latent norm: 0.970095\n",
      "[Epoch 337/500] Loss: 0.054656 | Time: 0.02s | Mean latent norm: 0.970088\n",
      "[Epoch 338/500] Loss: 0.053672 | Time: 0.02s | Mean latent norm: 0.970088\n",
      "New best model saved at epoch 338 with loss 0.053672\n",
      "[Epoch 339/500] Loss: 0.053858 | Time: 0.02s | Mean latent norm: 0.970095\n",
      "[Epoch 340/500] Loss: 0.053805 | Time: 0.02s | Mean latent norm: 0.970089\n",
      "[Epoch 341/500] Loss: 0.053589 | Time: 0.02s | Mean latent norm: 0.970089\n",
      "Average difference with latent perturbation: 0.06648550927639008\n",
      "New best model saved at epoch 341 with loss 0.053589\n",
      "[Epoch 342/500] Loss: 0.053938 | Time: 0.02s | Mean latent norm: 0.970096\n",
      "[Epoch 343/500] Loss: 0.053806 | Time: 0.02s | Mean latent norm: 0.970092\n",
      "[Epoch 344/500] Loss: 0.053254 | Time: 0.02s | Mean latent norm: 0.970094\n",
      "New best model saved at epoch 344 with loss 0.053254\n",
      "[Epoch 345/500] Loss: 0.053379 | Time: 0.02s | Mean latent norm: 0.970098\n",
      "[Epoch 346/500] Loss: 0.053723 | Time: 0.02s | Mean latent norm: 0.970096\n",
      "[Epoch 347/500] Loss: 0.053714 | Time: 0.02s | Mean latent norm: 0.970099\n",
      "[Epoch 348/500] Loss: 0.053138 | Time: 0.02s | Mean latent norm: 0.970103\n",
      "New best model saved at epoch 348 with loss 0.053138\n",
      "[Epoch 349/500] Loss: 0.053685 | Time: 0.02s | Mean latent norm: 0.970102\n",
      "[Epoch 350/500] Loss: 0.053176 | Time: 0.02s | Mean latent norm: 0.970102\n",
      "[Epoch 351/500] Loss: 0.052848 | Time: 0.02s | Mean latent norm: 0.970103\n",
      "Average difference with latent perturbation: 0.06455128639936447\n",
      "New best model saved at epoch 351 with loss 0.052848\n",
      "[Epoch 352/500] Loss: 0.052973 | Time: 0.02s | Mean latent norm: 0.970102\n",
      "[Epoch 353/500] Loss: 0.052889 | Time: 0.02s | Mean latent norm: 0.970106\n",
      "[Epoch 354/500] Loss: 0.052594 | Time: 0.02s | Mean latent norm: 0.970108\n",
      "New best model saved at epoch 354 with loss 0.052594\n",
      "[Epoch 355/500] Loss: 0.052768 | Time: 0.02s | Mean latent norm: 0.970107\n",
      "[Epoch 356/500] Loss: 0.053134 | Time: 0.02s | Mean latent norm: 0.970110\n",
      "[Epoch 357/500] Loss: 0.052348 | Time: 0.02s | Mean latent norm: 0.970111\n",
      "New best model saved at epoch 357 with loss 0.052348\n",
      "[Epoch 358/500] Loss: 0.052956 | Time: 0.02s | Mean latent norm: 0.970109\n",
      "[Epoch 359/500] Loss: 0.052527 | Time: 0.02s | Mean latent norm: 0.970115\n",
      "[Epoch 360/500] Loss: 0.053025 | Time: 0.02s | Mean latent norm: 0.970114\n",
      "[Epoch 361/500] Loss: 0.052575 | Time: 0.02s | Mean latent norm: 0.970116\n",
      "Average difference with latent perturbation: 0.06355316936969757\n",
      "[Epoch 362/500] Loss: 0.052546 | Time: 0.02s | Mean latent norm: 0.970119\n",
      "[Epoch 363/500] Loss: 0.052628 | Time: 0.02s | Mean latent norm: 0.970119\n",
      "[Epoch 364/500] Loss: 0.052561 | Time: 0.02s | Mean latent norm: 0.970120\n",
      "[Epoch 365/500] Loss: 0.052796 | Time: 0.02s | Mean latent norm: 0.970122\n",
      "[Epoch 366/500] Loss: 0.052743 | Time: 0.02s | Mean latent norm: 0.970120\n",
      "[Epoch 367/500] Loss: 0.052204 | Time: 0.02s | Mean latent norm: 0.970122\n",
      "New best model saved at epoch 367 with loss 0.052204\n",
      "[Epoch 368/500] Loss: 0.052115 | Time: 0.02s | Mean latent norm: 0.970123\n",
      "New best model saved at epoch 368 with loss 0.052115\n",
      "[Epoch 369/500] Loss: 0.052188 | Time: 0.02s | Mean latent norm: 0.970122\n",
      "[Epoch 370/500] Loss: 0.051931 | Time: 0.02s | Mean latent norm: 0.970125\n",
      "New best model saved at epoch 370 with loss 0.051931\n",
      "[Epoch 371/500] Loss: 0.052472 | Time: 0.02s | Mean latent norm: 0.970123\n",
      "Average difference with latent perturbation: 0.07157965749502182\n",
      "[Epoch 372/500] Loss: 0.051964 | Time: 0.02s | Mean latent norm: 0.970127\n",
      "[Epoch 373/500] Loss: 0.052057 | Time: 0.02s | Mean latent norm: 0.970125\n",
      "[Epoch 374/500] Loss: 0.051879 | Time: 0.02s | Mean latent norm: 0.970124\n",
      "New best model saved at epoch 374 with loss 0.051879\n",
      "[Epoch 375/500] Loss: 0.051423 | Time: 0.02s | Mean latent norm: 0.970126\n",
      "New best model saved at epoch 375 with loss 0.051423\n",
      "[Epoch 376/500] Loss: 0.052076 | Time: 0.02s | Mean latent norm: 0.970126\n",
      "[Epoch 377/500] Loss: 0.051901 | Time: 0.02s | Mean latent norm: 0.970126\n",
      "[Epoch 378/500] Loss: 0.051598 | Time: 0.02s | Mean latent norm: 0.970125\n",
      "[Epoch 379/500] Loss: 0.052100 | Time: 0.02s | Mean latent norm: 0.970125\n",
      "[Epoch 380/500] Loss: 0.051681 | Time: 0.02s | Mean latent norm: 0.970125\n",
      "[Epoch 381/500] Loss: 0.051431 | Time: 0.02s | Mean latent norm: 0.970124\n",
      "Average difference with latent perturbation: 0.06194503605365753\n",
      "[Epoch 382/500] Loss: 0.051520 | Time: 0.02s | Mean latent norm: 0.970122\n",
      "[Epoch 383/500] Loss: 0.051426 | Time: 0.02s | Mean latent norm: 0.970126\n",
      "[Epoch 384/500] Loss: 0.051767 | Time: 0.02s | Mean latent norm: 0.970121\n",
      "[Epoch 385/500] Loss: 0.051641 | Time: 0.02s | Mean latent norm: 0.970125\n",
      "[Epoch 386/500] Loss: 0.051331 | Time: 0.02s | Mean latent norm: 0.970130\n",
      "New best model saved at epoch 386 with loss 0.051331\n",
      "[Epoch 387/500] Loss: 0.050917 | Time: 0.02s | Mean latent norm: 0.970123\n",
      "New best model saved at epoch 387 with loss 0.050917\n",
      "[Epoch 388/500] Loss: 0.052008 | Time: 0.02s | Mean latent norm: 0.970128\n",
      "[Epoch 389/500] Loss: 0.050687 | Time: 0.02s | Mean latent norm: 0.970129\n",
      "New best model saved at epoch 389 with loss 0.050687\n",
      "[Epoch 390/500] Loss: 0.051021 | Time: 0.02s | Mean latent norm: 0.970125\n",
      "[Epoch 391/500] Loss: 0.051728 | Time: 0.02s | Mean latent norm: 0.970127\n",
      "Average difference with latent perturbation: 0.06242143362760544\n",
      "[Epoch 392/500] Loss: 0.050406 | Time: 0.02s | Mean latent norm: 0.970129\n",
      "New best model saved at epoch 392 with loss 0.050406\n",
      "[Epoch 393/500] Loss: 0.051144 | Time: 0.02s | Mean latent norm: 0.970129\n",
      "[Epoch 394/500] Loss: 0.051015 | Time: 0.02s | Mean latent norm: 0.970130\n",
      "[Epoch 395/500] Loss: 0.050780 | Time: 0.02s | Mean latent norm: 0.970135\n",
      "[Epoch 396/500] Loss: 0.051480 | Time: 0.02s | Mean latent norm: 0.970131\n",
      "[Epoch 397/500] Loss: 0.050584 | Time: 0.02s | Mean latent norm: 0.970133\n",
      "[Epoch 398/500] Loss: 0.050810 | Time: 0.02s | Mean latent norm: 0.970134\n",
      "[Epoch 399/500] Loss: 0.050737 | Time: 0.02s | Mean latent norm: 0.970133\n",
      "[Epoch 400/500] Loss: 0.051062 | Time: 0.02s | Mean latent norm: 0.970135\n",
      "[Epoch 401/500] Loss: 0.050775 | Time: 0.02s | Mean latent norm: 0.970139\n",
      "Average difference with latent perturbation: 0.06164935231208801\n",
      "[Epoch 402/500] Loss: 0.050184 | Time: 0.02s | Mean latent norm: 0.970139\n",
      "New best model saved at epoch 402 with loss 0.050184\n",
      "[Epoch 403/500] Loss: 0.050442 | Time: 0.02s | Mean latent norm: 0.970143\n",
      "[Epoch 404/500] Loss: 0.050614 | Time: 0.02s | Mean latent norm: 0.970143\n",
      "[Epoch 405/500] Loss: 0.050543 | Time: 0.02s | Mean latent norm: 0.970143\n",
      "[Epoch 406/500] Loss: 0.050434 | Time: 0.02s | Mean latent norm: 0.970149\n",
      "[Epoch 407/500] Loss: 0.050817 | Time: 0.02s | Mean latent norm: 0.970145\n",
      "[Epoch 408/500] Loss: 0.050571 | Time: 0.02s | Mean latent norm: 0.970149\n",
      "[Epoch 409/500] Loss: 0.050483 | Time: 0.02s | Mean latent norm: 0.970152\n",
      "[Epoch 410/500] Loss: 0.050816 | Time: 0.02s | Mean latent norm: 0.970150\n",
      "[Epoch 411/500] Loss: 0.050021 | Time: 0.02s | Mean latent norm: 0.970153\n",
      "Average difference with latent perturbation: 0.06454364955425262\n",
      "New best model saved at epoch 411 with loss 0.050021\n",
      "[Epoch 412/500] Loss: 0.050016 | Time: 0.02s | Mean latent norm: 0.970155\n",
      "New best model saved at epoch 412 with loss 0.050016\n",
      "[Epoch 413/500] Loss: 0.050045 | Time: 0.02s | Mean latent norm: 0.970155\n",
      "[Epoch 414/500] Loss: 0.050227 | Time: 0.02s | Mean latent norm: 0.970155\n",
      "[Epoch 415/500] Loss: 0.049894 | Time: 0.02s | Mean latent norm: 0.970158\n",
      "New best model saved at epoch 415 with loss 0.049894\n",
      "[Epoch 416/500] Loss: 0.050313 | Time: 0.02s | Mean latent norm: 0.970158\n",
      "[Epoch 417/500] Loss: 0.049700 | Time: 0.02s | Mean latent norm: 0.970161\n",
      "New best model saved at epoch 417 with loss 0.049700\n",
      "[Epoch 418/500] Loss: 0.049901 | Time: 0.02s | Mean latent norm: 0.970159\n",
      "[Epoch 419/500] Loss: 0.050498 | Time: 0.02s | Mean latent norm: 0.970160\n",
      "[Epoch 420/500] Loss: 0.049622 | Time: 0.02s | Mean latent norm: 0.970161\n",
      "New best model saved at epoch 420 with loss 0.049622\n",
      "[Epoch 421/500] Loss: 0.050083 | Time: 0.02s | Mean latent norm: 0.970160\n",
      "Average difference with latent perturbation: 0.06560219824314117\n",
      "[Epoch 422/500] Loss: 0.049978 | Time: 0.02s | Mean latent norm: 0.970164\n",
      "[Epoch 423/500] Loss: 0.049752 | Time: 0.02s | Mean latent norm: 0.970159\n",
      "[Epoch 424/500] Loss: 0.049534 | Time: 0.02s | Mean latent norm: 0.970161\n",
      "New best model saved at epoch 424 with loss 0.049534\n",
      "[Epoch 425/500] Loss: 0.050257 | Time: 0.02s | Mean latent norm: 0.970160\n",
      "[Epoch 426/500] Loss: 0.049523 | Time: 0.02s | Mean latent norm: 0.970159\n",
      "New best model saved at epoch 426 with loss 0.049523\n",
      "[Epoch 427/500] Loss: 0.049396 | Time: 0.02s | Mean latent norm: 0.970162\n",
      "New best model saved at epoch 427 with loss 0.049396\n",
      "[Epoch 428/500] Loss: 0.050293 | Time: 0.02s | Mean latent norm: 0.970154\n",
      "[Epoch 429/500] Loss: 0.049748 | Time: 0.02s | Mean latent norm: 0.970160\n",
      "[Epoch 430/500] Loss: 0.050031 | Time: 0.02s | Mean latent norm: 0.970158\n",
      "[Epoch 431/500] Loss: 0.049525 | Time: 0.02s | Mean latent norm: 0.970154\n",
      "Average difference with latent perturbation: 0.061933983117341995\n",
      "[Epoch 432/500] Loss: 0.049753 | Time: 0.02s | Mean latent norm: 0.970163\n",
      "[Epoch 433/500] Loss: 0.049279 | Time: 0.02s | Mean latent norm: 0.970162\n",
      "New best model saved at epoch 433 with loss 0.049279\n",
      "[Epoch 434/500] Loss: 0.049105 | Time: 0.02s | Mean latent norm: 0.970160\n",
      "New best model saved at epoch 434 with loss 0.049105\n",
      "[Epoch 435/500] Loss: 0.048985 | Time: 0.02s | Mean latent norm: 0.970166\n",
      "New best model saved at epoch 435 with loss 0.048985\n",
      "[Epoch 436/500] Loss: 0.049068 | Time: 0.02s | Mean latent norm: 0.970161\n",
      "[Epoch 437/500] Loss: 0.049197 | Time: 0.02s | Mean latent norm: 0.970165\n",
      "[Epoch 438/500] Loss: 0.049125 | Time: 0.02s | Mean latent norm: 0.970167\n",
      "[Epoch 439/500] Loss: 0.049570 | Time: 0.02s | Mean latent norm: 0.970168\n",
      "[Epoch 440/500] Loss: 0.049496 | Time: 0.02s | Mean latent norm: 0.970168\n",
      "[Epoch 441/500] Loss: 0.048937 | Time: 0.02s | Mean latent norm: 0.970170\n",
      "Average difference with latent perturbation: 0.06290141493082047\n",
      "New best model saved at epoch 441 with loss 0.048937\n",
      "[Epoch 442/500] Loss: 0.049532 | Time: 0.02s | Mean latent norm: 0.970166\n",
      "[Epoch 443/500] Loss: 0.049289 | Time: 0.02s | Mean latent norm: 0.970173\n",
      "[Epoch 444/500] Loss: 0.049543 | Time: 0.02s | Mean latent norm: 0.970170\n",
      "[Epoch 445/500] Loss: 0.049201 | Time: 0.02s | Mean latent norm: 0.970171\n",
      "[Epoch 446/500] Loss: 0.048992 | Time: 0.02s | Mean latent norm: 0.970174\n",
      "[Epoch 447/500] Loss: 0.048711 | Time: 0.02s | Mean latent norm: 0.970172\n",
      "New best model saved at epoch 447 with loss 0.048711\n",
      "[Epoch 448/500] Loss: 0.048588 | Time: 0.02s | Mean latent norm: 0.970179\n",
      "New best model saved at epoch 448 with loss 0.048588\n",
      "[Epoch 449/500] Loss: 0.049000 | Time: 0.02s | Mean latent norm: 0.970176\n",
      "[Epoch 450/500] Loss: 0.048641 | Time: 0.02s | Mean latent norm: 0.970179\n",
      "[Epoch 451/500] Loss: 0.048972 | Time: 0.02s | Mean latent norm: 0.970179\n",
      "Average difference with latent perturbation: 0.059228673577308655\n",
      "[Epoch 452/500] Loss: 0.048262 | Time: 0.02s | Mean latent norm: 0.970175\n",
      "New best model saved at epoch 452 with loss 0.048262\n",
      "[Epoch 453/500] Loss: 0.048580 | Time: 0.02s | Mean latent norm: 0.970182\n",
      "[Epoch 454/500] Loss: 0.048897 | Time: 0.02s | Mean latent norm: 0.970178\n",
      "[Epoch 455/500] Loss: 0.048779 | Time: 0.02s | Mean latent norm: 0.970179\n",
      "[Epoch 456/500] Loss: 0.048654 | Time: 0.02s | Mean latent norm: 0.970183\n",
      "[Epoch 457/500] Loss: 0.048626 | Time: 0.02s | Mean latent norm: 0.970178\n",
      "[Epoch 458/500] Loss: 0.048375 | Time: 0.02s | Mean latent norm: 0.970181\n",
      "[Epoch 459/500] Loss: 0.048798 | Time: 0.02s | Mean latent norm: 0.970179\n",
      "[Epoch 460/500] Loss: 0.048485 | Time: 0.02s | Mean latent norm: 0.970183\n",
      "[Epoch 461/500] Loss: 0.048735 | Time: 0.02s | Mean latent norm: 0.970182\n",
      "Average difference with latent perturbation: 0.07585600763559341\n",
      "[Epoch 462/500] Loss: 0.048222 | Time: 0.02s | Mean latent norm: 0.970187\n",
      "New best model saved at epoch 462 with loss 0.048222\n",
      "[Epoch 463/500] Loss: 0.048619 | Time: 0.02s | Mean latent norm: 0.970186\n",
      "[Epoch 464/500] Loss: 0.047978 | Time: 0.02s | Mean latent norm: 0.970184\n",
      "New best model saved at epoch 464 with loss 0.047978\n",
      "[Epoch 465/500] Loss: 0.048842 | Time: 0.02s | Mean latent norm: 0.970189\n",
      "[Epoch 466/500] Loss: 0.048126 | Time: 0.02s | Mean latent norm: 0.970184\n",
      "[Epoch 467/500] Loss: 0.048193 | Time: 0.02s | Mean latent norm: 0.970188\n",
      "[Epoch 468/500] Loss: 0.048664 | Time: 0.02s | Mean latent norm: 0.970184\n",
      "[Epoch 469/500] Loss: 0.048257 | Time: 0.02s | Mean latent norm: 0.970193\n",
      "[Epoch 470/500] Loss: 0.048598 | Time: 0.02s | Mean latent norm: 0.970181\n",
      "[Epoch 471/500] Loss: 0.049216 | Time: 0.02s | Mean latent norm: 0.970194\n",
      "Average difference with latent perturbation: 0.08924007415771484\n",
      "[Epoch 472/500] Loss: 0.048754 | Time: 0.02s | Mean latent norm: 0.970187\n",
      "[Epoch 473/500] Loss: 0.048140 | Time: 0.02s | Mean latent norm: 0.970186\n",
      "[Epoch 474/500] Loss: 0.048092 | Time: 0.02s | Mean latent norm: 0.970194\n",
      "[Epoch 475/500] Loss: 0.048317 | Time: 0.02s | Mean latent norm: 0.970185\n",
      "[Epoch 476/500] Loss: 0.048247 | Time: 0.02s | Mean latent norm: 0.970190\n",
      "[Epoch 477/500] Loss: 0.047721 | Time: 0.02s | Mean latent norm: 0.970194\n",
      "New best model saved at epoch 477 with loss 0.047721\n",
      "[Epoch 478/500] Loss: 0.048129 | Time: 0.02s | Mean latent norm: 0.970184\n",
      "[Epoch 479/500] Loss: 0.048281 | Time: 0.02s | Mean latent norm: 0.970197\n",
      "[Epoch 480/500] Loss: 0.047803 | Time: 0.02s | Mean latent norm: 0.970188\n",
      "[Epoch 481/500] Loss: 0.048223 | Time: 0.02s | Mean latent norm: 0.970189\n",
      "Average difference with latent perturbation: 0.060048095881938934\n",
      "[Epoch 482/500] Loss: 0.047770 | Time: 0.02s | Mean latent norm: 0.970198\n",
      "[Epoch 483/500] Loss: 0.048510 | Time: 0.02s | Mean latent norm: 0.970186\n",
      "[Epoch 484/500] Loss: 0.048434 | Time: 0.02s | Mean latent norm: 0.970193\n",
      "[Epoch 485/500] Loss: 0.047927 | Time: 0.02s | Mean latent norm: 0.970195\n",
      "[Epoch 486/500] Loss: 0.047647 | Time: 0.02s | Mean latent norm: 0.970188\n",
      "New best model saved at epoch 486 with loss 0.047647\n",
      "[Epoch 487/500] Loss: 0.047839 | Time: 0.02s | Mean latent norm: 0.970196\n",
      "[Epoch 488/500] Loss: 0.047478 | Time: 0.02s | Mean latent norm: 0.970191\n",
      "New best model saved at epoch 488 with loss 0.047478\n",
      "[Epoch 489/500] Loss: 0.047955 | Time: 0.02s | Mean latent norm: 0.970195\n",
      "[Epoch 490/500] Loss: 0.047497 | Time: 0.02s | Mean latent norm: 0.970198\n",
      "[Epoch 491/500] Loss: 0.047652 | Time: 0.02s | Mean latent norm: 0.970194\n",
      "Average difference with latent perturbation: 0.0675506591796875\n",
      "[Epoch 492/500] Loss: 0.047560 | Time: 0.02s | Mean latent norm: 0.970198\n",
      "[Epoch 493/500] Loss: 0.047771 | Time: 0.02s | Mean latent norm: 0.970201\n",
      "[Epoch 494/500] Loss: 0.047148 | Time: 0.02s | Mean latent norm: 0.970194\n",
      "New best model saved at epoch 494 with loss 0.047148\n",
      "[Epoch 495/500] Loss: 0.047601 | Time: 0.02s | Mean latent norm: 0.970203\n",
      "[Epoch 496/500] Loss: 0.047687 | Time: 0.02s | Mean latent norm: 0.970200\n",
      "[Epoch 497/500] Loss: 0.047223 | Time: 0.02s | Mean latent norm: 0.970197\n",
      "[Epoch 498/500] Loss: 0.047151 | Time: 0.02s | Mean latent norm: 0.970208\n",
      "[Epoch 499/500] Loss: 0.047728 | Time: 0.02s | Mean latent norm: 0.970198\n",
      "[Epoch 500/500] Loss: 0.047223 | Time: 0.02s | Mean latent norm: 0.970204\n",
      "Loss curve saved to training_loss_curve.png\n"
     ]
    }
   ],
   "source": [
    "# Testing on sphere:\n",
    "!python /home/lucasp/thesis/main.py --npz_folder /home/lucasp/thesis/sphere_test --num_epochs 500 --do_code_regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc6ce8",
   "metadata": {},
   "source": [
    "Changing GT image for reconstruction/visualization, it only seems to be moving the reconstructed shape around without actually changing its shape which means that it doesn't seem to be using the latent vectors appropriately or there is an issue with how they're being integrated in the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57b1ed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lucasp/thesis/main.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"auto_decoder_model.pth\", map_location=\"cuda\")\n",
      "/home/lucasp/miniconda3/envs/nnunet/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "Loaded trained model for visualization.\n",
      "Selected slice_idx 104 with maximum voxels (count: 786.0)\n",
      "SDF slice visualization saved to sdf_slice.png\n",
      "Visualization saved as sdf_slice.png\n"
     ]
    }
   ],
   "source": [
    "# Visualization:\n",
    "!python main.py --visualize_only --segmentation_file /home/lucasp/thesis/TopCoW/gt_train/topcow_ct_001.nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9dd114f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lucasp/thesis/geometric_model/reconstruct_mesh.py:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(checkpoint_path, map_location=device)\n",
      "/home/lucasp/miniconda3/envs/nnunet/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "sampling takes: 41.838190\n",
      "Mesh saved as gt_001_2500_epochs_15k_samples.ply\n"
     ]
    }
   ],
   "source": [
    "# To reconstruct mesh:\n",
    "!python geometric_model/reconstruct_mesh.py --checkpoint auto_decoder.pth --shape_idx 5 --output gt_001_2500_epochs_15k_samples.ply --resolution 512\n",
    "# !python geometric_model/reconstruct_mesh.py --checkpoint auto_decoder.pth --shape_idx 5 --output gt_001_2500_epochs_25k_samples_384.ply --resolution 512 --hidden_dims \"384,384,384,384,384,384,384,384\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8235b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lucasp/thesis/geometric_model/reconstruct_mesh.py:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(checkpoint_path, map_location=device)\n",
      "/home/lucasp/miniconda3/envs/nnunet/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "Histogram saved to surface_noisy_histogram.png\n",
      "Iter    0: data_loss=0.6925, prior=0.0000\n",
      "Iter  100: data_loss=0.4673, prior=0.0005\n",
      "Iter  200: data_loss=0.4574, prior=0.0009\n",
      "Iter  300: data_loss=0.4545, prior=0.0011\n",
      "Iter  400: data_loss=0.4552, prior=0.0012\n",
      "Iter  500: data_loss=0.4521, prior=0.0013\n",
      "Iter  600: data_loss=0.4518, prior=0.0014\n",
      "Iter  700: data_loss=0.4516, prior=0.0015\n",
      "Iter  800: data_loss=0.4515, prior=0.0015\n",
      "Iter  900: data_loss=0.4512, prior=0.0016\n",
      "Iter 1000: data_loss=0.4525, prior=0.0016\n",
      "Iter 1100: data_loss=0.4508, prior=0.0017\n",
      "Iter 1200: data_loss=0.4509, prior=0.0018\n",
      "Iter 1300: data_loss=0.4504, prior=0.0019\n",
      "Iter 1400: data_loss=0.4492, prior=0.0020\n",
      "Iter 1500: data_loss=0.4496, prior=0.0021\n",
      "Iter 1600: data_loss=0.4489, prior=0.0021\n",
      "Iter 1700: data_loss=0.4493, prior=0.0022\n",
      "Iter 1800: data_loss=0.4494, prior=0.0022\n",
      "Iter 1900: data_loss=0.4491, prior=0.0022\n",
      "Iter 2000: data_loss=0.4486, prior=0.0022\n",
      "Iter 2100: data_loss=0.4486, prior=0.0022\n",
      "Iter 2200: data_loss=0.4488, prior=0.0022\n",
      "Iter 2300: data_loss=0.4488, prior=0.0022\n",
      "Iter 2400: data_loss=0.4492, prior=0.0022\n",
      "Iter 2500: data_loss=0.4486, prior=0.0022\n",
      "Iter 2600: data_loss=0.4486, prior=0.0022\n",
      "Iter 2700: data_loss=0.4486, prior=0.0022\n",
      "Iter 2800: data_loss=0.4486, prior=0.0022\n",
      "Iter 2900: data_loss=0.4485, prior=0.0022\n",
      "Iter 3000: data_loss=0.4489, prior=0.0022\n",
      "Iter 3100: data_loss=0.4491, prior=0.0022\n",
      "Iter 3200: data_loss=0.4485, prior=0.0022\n",
      "Iter 3300: data_loss=0.4487, prior=0.0022\n",
      "Iter 3400: data_loss=0.4486, prior=0.0022\n",
      "Iter 3500: data_loss=0.4487, prior=0.0022\n",
      "Iter 3600: data_loss=0.4486, prior=0.0022\n",
      "Iter 3700: data_loss=0.4493, prior=0.0023\n",
      "Iter 3800: data_loss=0.4491, prior=0.0023\n",
      "Iter 3900: data_loss=0.4484, prior=0.0023\n",
      "Iter 4000: data_loss=0.4482, prior=0.0023\n",
      "Iter 4100: data_loss=0.4484, prior=0.0024\n",
      "Iter 4200: data_loss=0.4500, prior=0.0025\n",
      "Iter 4300: data_loss=0.4487, prior=0.0025\n",
      "Iter 4400: data_loss=0.4477, prior=0.0026\n",
      "Iter 4500: data_loss=0.4456, prior=0.0027\n",
      "Iter 4600: data_loss=0.4456, prior=0.0027\n",
      "Iter 4700: data_loss=0.4456, prior=0.0027\n",
      "Iter 4800: data_loss=0.4451, prior=0.0027\n",
      "Iter 4900: data_loss=0.4451, prior=0.0028\n",
      "Optimized latent is closest to pre-trained latent #2 (distance=2.6526)\n",
      "sampling takes: 43.452077\n",
      "Mesh saved as topcow_ct_001_recon.ply\n"
     ]
    }
   ],
   "source": [
    "# TODO: To run inference on trained model:\n",
    "!python geometric_model/reconstruct_mesh.py --checkpoint auto_decoder_2500_epochs.pth --segmentation TopCoW/2d_base_without_mirroring/topcow_ct_001.nii.gz --output topcow_ct_001_recon.ply --resolution 512 --opt_num_iters 5000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnunet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
